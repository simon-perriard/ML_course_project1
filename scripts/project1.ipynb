{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data according to the value of column 24 (PRI_jet_num) \n",
    "\n",
    "def separate(y, tX, ids):\n",
    "    \n",
    "    split_x = []\n",
    "    split_y = []\n",
    "    split_ids = []\n",
    "    \n",
    "    jet_column_nbr = 22\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        split_x.append(tX[np.where(tX[:,jet_column_nbr] == i)])\n",
    "        split_y.append(y[np.where(tX[:,jet_column_nbr] == i)])\n",
    "        split_ids.append(ids[np.where(tX[:,jet_column_nbr] == i)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return split_x, split_y, split_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_x, split_y, split_ids = separate(y, tX, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns from each set of data given a boolean array\n",
    "\n",
    "def removeNone(data, selection):\n",
    "   \n",
    "    cleaned=[]\n",
    "    \n",
    "    for i in range(4):\n",
    "        curr_data = data[i]\n",
    "        \n",
    "        cleaned.append(curr_data[:,selection[i]])\n",
    "      \n",
    "    return cleaned\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print statistics about the None values (-999) for each columns\n",
    "#returns a boolean array that can be used to filter the columns that have 100% of undefined values (-999)\n",
    "def dataStatistics(data):\n",
    "    \n",
    "    stats=[]\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        print(\"Statistics \")\n",
    "        print(\"Type :\")\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "        nones = (data[i] == -999)\n",
    "    \n",
    "        mean = np.sum(nones, axis=0)/nones.shape[0]\n",
    "        print(mean) \n",
    "        stats.append(mean != 1)\n",
    "    \n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.26145747 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 1.         1.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09751883 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05859584 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.0666396 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "selection = dataStatistics(split_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = removeNone(split_x, selection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_momentum_vector(data):\n",
    "#### colums with tau info ####\n",
    "    tau_transverse_momentum_0_1_col = 9\n",
    "    tau_pseudo_rapidity_0_1_col = 10\n",
    "    tau_azimuth_angle_0_1_col = 11\n",
    "    \n",
    "    tau_transverse_momentum_2_3_col = 13\n",
    "    tau_pseudo_rapidity_2_3_col = 14\n",
    "    tau_azimuth_angle_2_3_col = 15\n",
    "    \n",
    "#### colums with lep info ####    \n",
    "    lep_transverse_momentum_0_1_col = 12\n",
    "    lep_pseudo_rapidity_0_1_col = 13\n",
    "    lep_azimuth_angle_0_1_col = 14\n",
    "     \n",
    "    lep_transverse_momentum_2_3_col = 16\n",
    "    lep_pseudo_rapidity_2_3_col = 17\n",
    "    lep_azimuth_angle_2_3_col = 18\n",
    "\n",
    "#### colums with leading_jet info ####\n",
    "    leading_jet_transverse_momentum_1_col = 19\n",
    "    leading_jet_pseudo_rapidity_1_col = 20\n",
    "    leading_jet_azimuth_angle_1_col = 21\n",
    "\n",
    "    leading_jet_transverse_momentum_2_3_col = 23\n",
    "    leading_jet_pseudo_rapidity_2_3_col = 24\n",
    "    leading_jet_azimuth_angle_2_3_col = 25\n",
    "    \n",
    "#### colums with subleading_jet info ####\n",
    "    subleading_jet_transverse_momentum_2_3_col = 26\n",
    "    subleading_jet_pseudo_rapidity_2_3_col = 27\n",
    "    subleading_jet_azimuth_angle_2_3_col = 28\n",
    "\n",
    "#### columns with missing transverse energy info ####\n",
    "    mte_0_1_col = 15\n",
    "    mte_angle_0_1_col = 16\n",
    "    \n",
    "    mte_2_3_col = 19\n",
    "    mte_angle_2_3_col = 20\n",
    "    \n",
    "    data_with_momentum_vector = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        current = data[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            tau_transverse_momentum_0 = current[:,tau_transverse_momentum_0_1_col]\n",
    "            tau_pseudo_rapidity_0 = current[:, tau_pseudo_rapidity_0_1_col]\n",
    "            tau_azimuth_angle_0 = current[:, tau_azimuth_angle_0_1_col]\n",
    "            \n",
    "            lep_transverse_momentum_0 = current[:,lep_transverse_momentum_0_1_col]\n",
    "            lep_pseudo_rapidity_0 = current[:, lep_pseudo_rapidity_0_1_col]\n",
    "            lep_azimuth_angle_0 = current[:, lep_azimuth_angle_0_1_col]\n",
    "            \n",
    "            mte_0 = current[:, mte_0_1_col]\n",
    "            mte_angle_0 = current[:, mte_angle_0_1_col]\n",
    "            \n",
    "            current = np.c_[current, tau_transverse_momentum_0*np.cos(tau_azimuth_angle_0)]\n",
    "            current = np.c_[current, tau_transverse_momentum_0*np.sin(tau_azimuth_angle_0)]\n",
    "            current = np.c_[current, tau_transverse_momentum_0*np.sinh(tau_pseudo_rapidity_0)]\n",
    "            current = np.c_[current, tau_transverse_momentum_0*np.cosh(tau_pseudo_rapidity_0)]\n",
    "            \n",
    "            current = np.c_[current, lep_transverse_momentum_0*np.cos(lep_azimuth_angle_0)]\n",
    "            current = np.c_[current, lep_transverse_momentum_0*np.sin(lep_azimuth_angle_0)]\n",
    "            current = np.c_[current, lep_transverse_momentum_0*np.sinh(lep_pseudo_rapidity_0)]\n",
    "            current = np.c_[current, lep_transverse_momentum_0*np.cosh(lep_pseudo_rapidity_0)]\n",
    "            \n",
    "            current = np.c_[current, mte_0*np.cos(mte_angle_0)]\n",
    "            current = np.c_[current, mte_0*np.sin(mte_angle_0)]\n",
    "            \n",
    "        elif i == 1:\n",
    "            tau_transverse_momentum_1 = current[:,tau_transverse_momentum_0_1_col]\n",
    "            tau_pseudo_rapidity_1 = current[:, tau_pseudo_rapidity_0_1_col]\n",
    "            tau_azimuth_angle_1 = current[:, tau_azimuth_angle_0_1_col]\n",
    "            \n",
    "            lep_transverse_momentum_1 = current[:,lep_transverse_momentum_0_1_col]\n",
    "            lep_pseudo_rapidity_1 = current[:, lep_pseudo_rapidity_0_1_col]\n",
    "            lep_azimuth_angle_1 = current[:, lep_azimuth_angle_0_1_col]\n",
    "            \n",
    "            leading_jet_transverse_momentum_1 = current[:,lep_transverse_momentum_0_1_col]\n",
    "            leading_jet_pseudo_rapidity_1 = current[:, lep_pseudo_rapidity_0_1_col]\n",
    "            leading_jet_azimuth_angle_1 = current[:, lep_azimuth_angle_0_1_col]\n",
    "            \n",
    "            mte_1 = current[:, mte_0_1_col]\n",
    "            mte_angle_1 = current[:, mte_angle_0_1_col]\n",
    "            \n",
    "            current = np.c_[current, tau_transverse_momentum_1*np.cos(tau_azimuth_angle_1)]\n",
    "            current = np.c_[current, tau_transverse_momentum_1*np.sin(tau_azimuth_angle_1)]\n",
    "            current = np.c_[current, tau_transverse_momentum_1*np.sinh(tau_pseudo_rapidity_1)]\n",
    "            current = np.c_[current, tau_transverse_momentum_1*np.cosh(tau_pseudo_rapidity_1)]\n",
    "            \n",
    "            current = np.c_[current, lep_transverse_momentum_1*np.cos(lep_azimuth_angle_1)]\n",
    "            current = np.c_[current, lep_transverse_momentum_1*np.sin(lep_azimuth_angle_1)]\n",
    "            current = np.c_[current, lep_transverse_momentum_1*np.sinh(lep_pseudo_rapidity_1)]\n",
    "            current = np.c_[current, lep_transverse_momentum_1*np.cosh(lep_pseudo_rapidity_1)]\n",
    "            \n",
    "            current = np.c_[current, leading_jet_transverse_momentum_1*np.cos(leading_jet_azimuth_angle_1)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_1*np.sin(leading_jet_azimuth_angle_1)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_1*np.sinh(leading_jet_pseudo_rapidity_1)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_1*np.cosh(leading_jet_pseudo_rapidity_1)]\n",
    "            \n",
    "            current = np.c_[current, mte_1*np.cos(mte_angle_1)]\n",
    "            current = np.c_[current, mte_1*np.sin(mte_angle_1)]\n",
    "            \n",
    "        else:\n",
    "            tau_transverse_momentum_2_3 = current[:,tau_transverse_momentum_2_3_col]\n",
    "            tau_pseudo_rapidity_2_3 = current[:, tau_pseudo_rapidity_2_3_col]\n",
    "            tau_azimuth_angle_2_3 = current[:, tau_azimuth_angle_2_3_col]\n",
    "            \n",
    "            lep_transverse_momentum_2_3 = current[:,lep_transverse_momentum_2_3_col]\n",
    "            lep_pseudo_rapidity_2_3 = current[:, lep_pseudo_rapidity_2_3_col]\n",
    "            lep_azimuth_angle_2_3 = current[:, lep_azimuth_angle_2_3_col]\n",
    "            \n",
    "            leading_jet_transverse_momentum_2_3 = current[:,leading_jet_transverse_momentum_2_3_col]\n",
    "            leading_jet_pseudo_rapidity_2_3 = current[:, leading_jet_pseudo_rapidity_2_3_col]\n",
    "            leading_jet_azimuth_angle_2_3 = current[:, leading_jet_azimuth_angle_2_3_col]\n",
    "            \n",
    "            subleading_jet_transverse_momentum_2_3 = current[:,subleading_jet_transverse_momentum_2_3_col]\n",
    "            subleading_jet_pseudo_rapidity_2_3 = current[:, subleading_jet_pseudo_rapidity_2_3_col]\n",
    "            subleading_jet_azimuth_angle_2_3 = current[:, subleading_jet_azimuth_angle_2_3_col]\n",
    "            \n",
    "            mte_2_3 = current[:, mte_2_3_col]\n",
    "            mte_angle_2_3 = current[:, mte_angle_2_3_col]\n",
    "            \n",
    "            current = np.c_[current, tau_transverse_momentum_2_3*np.cos(tau_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, tau_transverse_momentum_2_3*np.sin(tau_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, tau_transverse_momentum_2_3*np.sinh(tau_pseudo_rapidity_2_3)]\n",
    "            current = np.c_[current, tau_transverse_momentum_2_3*np.cosh(tau_pseudo_rapidity_2_3)]\n",
    "            \n",
    "            current = np.c_[current, lep_transverse_momentum_2_3*np.cos(lep_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, lep_transverse_momentum_2_3*np.sin(lep_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, lep_transverse_momentum_2_3*np.sinh(lep_pseudo_rapidity_2_3)]\n",
    "            current = np.c_[current, lep_transverse_momentum_2_3*np.cosh(lep_pseudo_rapidity_2_3)]\n",
    "            \n",
    "            current = np.c_[current, leading_jet_transverse_momentum_2_3*np.cos(leading_jet_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_2_3*np.sin(leading_jet_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_2_3*np.sinh(leading_jet_pseudo_rapidity_2_3)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_2_3*np.cosh(leading_jet_pseudo_rapidity_2_3)]\n",
    "        \n",
    "            current = np.c_[current, subleading_jet_transverse_momentum_2_3*np.cos(subleading_jet_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, subleading_jet_transverse_momentum_2_3*np.sin(subleading_jet_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, subleading_jet_transverse_momentum_2_3*np.sinh(subleading_jet_pseudo_rapidity_2_3)]\n",
    "            current = np.c_[current, subleading_jet_transverse_momentum_2_3*np.cosh(subleading_jet_pseudo_rapidity_2_3)]\n",
    "        \n",
    "            current = np.c_[current, mte_2_3*np.cos(mte_angle_2_3)]\n",
    "            current = np.c_[current, mte_2_3*np.sin(mte_angle_2_3)]\n",
    "                    \n",
    "        data_with_momentum_vector.append(current)\n",
    "    \n",
    "    return data_with_momentum_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can either drop the lines with residual Nones or replace the Nones by the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the value of column 0 (can be None sometimes) by the median value of this column\n",
    "\n",
    "def putMedianInsteadOfNone(cleaned):\n",
    "    \n",
    "    completed_data = []\n",
    "    \n",
    "    for i in range(len(cleaned)):\n",
    "        #current PRI_jet_num\n",
    "        current = cleaned[i]\n",
    "        \n",
    "        median = np.median(current[np.where(current[:,0] != -999)], axis = 0)\n",
    "        \n",
    "        #replace -999 by median value\n",
    "        current[np.where(current[:,0] == -999)] = median\n",
    "        \n",
    "        completed_data.append(current)\n",
    "    \n",
    "    \n",
    "    return completed_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_with_median = putMedianInsteadOfNone(cleaned)\n",
    "\n",
    "cleaned_with_median_with_momentum = add_momentum_vector(cleaned_with_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of putting the median we can simply drop the data where columns 0 == -999\n",
    "def dropLineIfNone(cleaned, split_y, split_ids):\n",
    "    \n",
    "    res_x=[]\n",
    "    res_y=[]\n",
    "    res_ids=[]\n",
    "    \n",
    "    for i in range(len(cleaned)):\n",
    "        \n",
    "        current = cleaned[i]\n",
    "        \n",
    "        drop_indexes = np.where(current[:,0] != -999)\n",
    "        \n",
    "        res_x.append(current[drop_indexes])\n",
    "        res_y.append(current[drop_indexes])\n",
    "        res_ids.append(current[drop_indexes])\n",
    "        \n",
    "    return res_x, res_y, res_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropped_x, dropped_y, dropped_ids = dropLineIfNone(cleaned, split_y, split_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, the first values in each of the split data has a PRI_jet_num = 0, then 1 and so on. The data is clean and we can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Expension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to perform polynomial feature expension\n",
    "\n",
    "def build_poly(x, degree):\n",
    "   \n",
    "    x_extended = x\n",
    "\n",
    "    for d in range (2, degree +1):\n",
    "        x_extended = np.c_[x_extended, x**d]\n",
    "        \n",
    "\n",
    "    return x_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to split the training set into a (new) training set and a test set (same as in lab03)\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    " \n",
    "    # split the data based on the given ratio\n",
    "\n",
    "    training_nbr = int(x.shape[0] * ratio)\n",
    "    indexes = np.random.choice(x.shape[0],training_nbr, replace=False)\n",
    "    \n",
    "    x_train = x[indexes]\n",
    "    y_train = y[indexes]\n",
    "    x_test = np.delete(x, indexes, axis = 0)\n",
    "    y_test = np.delete(y, indexes, axis = 0)\n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross-validation \n",
    "\n",
    "def crossValidation(x, y, splitRatio, degrees, seed =1):\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, splitRatio, seed)\n",
    "    \n",
    "    a_training = []\n",
    "    a_testing = []\n",
    "    weights = []\n",
    "    degr = []\n",
    "    \n",
    "    # define parameter (just add more for loops if there are more parameters for the model)\n",
    "    lambdas = np.arange(0.000001,0.00001,0.000001)\n",
    "    lambdas = [0]\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        for ind_d, d in enumerate(degrees):\n",
    "            \n",
    "            \n",
    "            #perform polynomial feature expension\n",
    "            x_test_poly = build_poly(x_test,d)\n",
    "            x_train_poly = build_poly(x_train, d)\n",
    "           \n",
    "            \n",
    "            #normalize data (DANGER: the test set must be normalized with the training set's mean and std)\n",
    "            mean = np.mean(x_train_poly, axis =0)\n",
    "            std = np.std(x_train_poly, axis = 0)\n",
    "            \n",
    "              \n",
    "            #put 1 if std = 0\n",
    "            std = std + (std == 0)\n",
    "\n",
    "            \n",
    "            x_train_ready = (x_train_poly - mean) / std\n",
    "            x_test_ready = (x_test_poly - mean) / std\n",
    "            \n",
    "            \n",
    "            #add bias term\n",
    "            bias_tr = np.ones(shape=x_train.shape)\n",
    "            bias_te = np.ones(shape=x_test.shape)\n",
    "            \n",
    "            x_train_ready = np.c_[bias_tr, x_train_ready]\n",
    "            x_test_ready = np.c_[bias_te, x_test_ready]\n",
    "            \n",
    "            \n",
    "            #Models\n",
    "        \n",
    "            #ideal : lambdas = np.arange(0.01,0.2,0.04)\n",
    "            #w_star, e_tr = ridge_regression(y_train,x_train_ready, lambda_)\n",
    "        \n",
    "            #ideal : lambdas = np.arange(0,0.3,0.1)\n",
    "            #w_star, e_tr = logistic_regression(y_train, x_train_ready,np.ones(x_train_ready.shape[1])  ,400, lambda_)\n",
    "        \n",
    "            #don't usel least squares with lambda bigger than 0.35 ideal: lambdas = np.arange(0.001,0.13,0.01)\n",
    "            #w_star, e_tr = least_squares_GD(y_train, x_train_ready,np.ones(x_train_ready.shape[1])  ,400, lambda_)    \n",
    "            #w_star, e_tr = least_squares_SGD(y_train, x_train,np.ones(x_train.shape[1])  ,400, lambda_)\n",
    "        \n",
    "            #DON'T REALLY NEED TO DO CROSS VALIDATION FOR THIS ONE ;) BUT PRACTICAL TO RUN IT HERE\n",
    "            w_star, e_tr = least_squares(y_train, x_train_ready)  \n",
    "        \n",
    "            degr.append(d)\n",
    "        \n",
    "            #compare the prediction with the reality\n",
    "            accuracy_training = np.count_nonzero(predict_labels(w_star, x_train_ready) + y_train)/len(y_train)\n",
    "            accuracy_testing = np.count_nonzero(predict_labels(w_star, x_test_ready) + y_test)/len(y_test)\n",
    "        \n",
    "            a_training.append(accuracy_training)\n",
    "            a_testing.append(accuracy_testing)\n",
    "            weights.append(w_star)\n",
    "            print(\"lambda={l:.5f},degree={deg}, Training Accuracy={tr}, Testing Accuracy={te}\".format(\n",
    "                   l=lambda_, tr=a_training[ind*len(degrees)+ind_d], te=a_testing[ind*len(degrees)+ind_d], deg=d))\n",
    "        \n",
    "            #plt.plot(lambdas, a_training,'r--' , lambdas, a_testing, 'g--')\n",
    "            #plt.show\n",
    "    \n",
    "    return weights[np.argmax(a_testing)], degr[np.argmax(a_testing)], a_testing[np.argmax(a_testing)], x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross-validation \n",
    "\n",
    "def crossValidationForLogistic_reg(x, y, splitRatio, degrees, seed =1):\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, splitRatio, seed)\n",
    "    \n",
    "    a_training = []\n",
    "    a_testing = []\n",
    "    weights = []\n",
    "    degr = []\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    # define parameter (just add more for loops if there are more parameters for the model)\n",
    "    lambdas = np.arange(0.0001,0.3,0.1)\n",
    "    gammas = np.arange(0.01,1,0.3)\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        for ind_d, d in enumerate(degrees):\n",
    "            \n",
    "            for ind_g, gamma in enumerate(gammas):\n",
    "            \n",
    "                #perform polynomial feature expension\n",
    "                x_test_poly = build_poly(x_test,d)\n",
    "                x_train_poly = build_poly(x_train, d)\n",
    "            \n",
    "                #normalize data (DANGER: the test set must be normalized with the training set's mean and std)\n",
    "                mean = np.mean(x_train_poly, axis =0)\n",
    "                std = np.std(x_train_poly, axis = 0)\n",
    "            \n",
    "                #put 1 if std = 0\n",
    "                std = std + (std == 0)\n",
    "            \n",
    "                x_train_ready = (x_train_poly - mean) / std\n",
    "                x_test_ready = (x_test_poly - mean) / std\n",
    "                \n",
    "               \n",
    "                #add bias term\n",
    "                \n",
    "                bias_tr = np.ones(shape=x_train.shape)\n",
    "                bias_te = np.ones(shape=x_test.shape)\n",
    "            \n",
    "                x_train_ready = np.c_[bias_tr, x_train_ready]\n",
    "                x_test_ready = np.c_[bias_te, x_test_ready]\n",
    "                \n",
    "           \n",
    "\n",
    "                #Model\n",
    "        \n",
    "                #ideal :lambdas = np.arange(0,0.3,0.01)\n",
    "                #       gammas = np.arange(0,3,0.5)\n",
    "                w_star, e_tr = reg_logistic_regression(y_train, x_train_ready, lambda_, np.ones(x_test_ready.shape[1]), 30, gamma)\n",
    "        \n",
    "           \n",
    "                degr.append(d)\n",
    "        \n",
    "                #compare the prediction with the reality\n",
    "                accuracy_training = np.count_nonzero(predict_labels(w_star, x_train_ready) + y_train)/len(y_train)\n",
    "                accuracy_testing = np.count_nonzero(predict_labels(w_star, x_test_ready) + y_test)/len(y_test)\n",
    "        \n",
    "                a_training.append(accuracy_training)\n",
    "                a_testing.append(accuracy_testing)\n",
    "                weights.append(w_star)\n",
    "                print(\"lambda={l:.5f},degree={deg}, gamma={ga:.5f}, Training Accuracy={tr}, Testing Accuracy={te}\".format(\n",
    "                       l=lambda_, tr=a_training[index], te=a_testing[index], deg=d, ga=gamma))\n",
    "        \n",
    "                #increment index\n",
    "                index = index + 1\n",
    "    \n",
    "    return weights[np.argmax(a_testing)], degr[np.argmax(a_testing)], a_testing[np.argmax(a_testing)], x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we separated the data according to PRI_jet_num\n",
    "# we have to make separate prediction and then put them together for the submission\n",
    "\n",
    "def put_together(labels, indices):\n",
    "    \n",
    "    #First build first chunk\n",
    "    ids_0 = np.matrix(indices[0]).T\n",
    "    lab_0 = np.matrix(labels[0]).T\n",
    "    \n",
    "    unsorted_res = np.concatenate((ids_0, lab_0), axis=1)\n",
    "    \n",
    "    for i in range(1,len(labels)):\n",
    "        ids = np.matrix(indices[i]).T\n",
    "        lab = np.matrix(labels[i]).T\n",
    "        by_jet_num = np.concatenate((ids, lab), axis=1)\n",
    "        unsorted_res = np.concatenate((unsorted_res, by_jet_num), axis=0)\n",
    "    \n",
    "    sorted_res = unsorted_res[np.lexsort(np.fliplr(unsorted_res).T)]\n",
    "    \n",
    "    return sorted_res[0,:,:][:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_donotUse, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.2605448 0.        0.        0.        1.        1.        1.\n",
      " 0.        0.        0.        0.        0.        1.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.        1.        1.        1.        1.\n",
      " 1.        0.       ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09834149 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05881481 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.06376737 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8271033764323794, Testing Accuracy=0.8294147073536768\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8292379026492637, Testing Accuracy=0.8329164582291145\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.8342525073023266, Testing Accuracy=0.8359179589794897\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8385521988683947, Testing Accuracy=0.8374187093546773\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8388177380149927, Testing Accuracy=0.8379189594797398\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8399207467777846, Testing Accuracy=0.8384192096048024\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8413301468635741, Testing Accuracy=0.840920460230115\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.843117429581061, Testing Accuracy=0.8394197098549274\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8436995730947566, Testing Accuracy=0.8414207103551776\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8435055252568581, Testing Accuracy=0.83991995997999\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8435157383009579, Testing Accuracy=0.8404202101050525\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8434953122127581, Testing Accuracy=0.840920460230115\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8435872296096575, Testing Accuracy=0.840920460230115\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8435361643891578, Testing Accuracy=0.8404202101050525\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8436280817860572, Testing Accuracy=0.840920460230115\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8435565904773576, Testing Accuracy=0.8384192096048024\n",
      "lambda=0.00000,degree=17, Training Accuracy=0.8435974426537574, Testing Accuracy=0.8404202101050525\n",
      "lambda=0.00000,degree=18, Training Accuracy=0.8435872296096575, Testing Accuracy=0.840920460230115\n",
      "lambda=0.00000,degree=19, Training Accuracy=0.8437914904916559, Testing Accuracy=0.83991995997999\n",
      "lambda=0.00000,degree=20, Training Accuracy=0.8438119165798558, Testing Accuracy=0.8404202101050525\n",
      "lambda=0.00000,degree=21, Training Accuracy=0.8437914904916559, Testing Accuracy=0.8384192096048024\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7184477517666101, Testing Accuracy=0.7130883301096067\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7614122353374653, Testing Accuracy=0.7562862669245648\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7717289750371745, Testing Accuracy=0.7640232108317214\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7802297580040267, Testing Accuracy=0.7736943907156673\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7839406261102997, Testing Accuracy=0.7711154094132817\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7874935849354546, Testing Accuracy=0.7756286266924565\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7954022080981143, Testing Accuracy=0.781431334622824\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8018106930901531, Testing Accuracy=0.7911025145067698\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8052715381679891, Testing Accuracy=0.793036750483559\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8044425144421197, Testing Accuracy=0.7917472598323663\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8052978563815089, Testing Accuracy=0.7923920051579626\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8052846972747489, Testing Accuracy=0.7923920051579626\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8057979024383825, Testing Accuracy=0.7949709864603481\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.805837379758662, Testing Accuracy=0.793036750483559\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.805521561196426, Testing Accuracy=0.7949709864603481\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8055741976234653, Testing Accuracy=0.7943262411347518\n",
      "lambda=0.00000,degree=17, Training Accuracy=0.80520574263419, Testing Accuracy=0.7917472598323663\n",
      "lambda=0.00000,degree=18, Training Accuracy=0.805521561196426, Testing Accuracy=0.7923920051579626\n",
      "lambda=0.00000,degree=19, Training Accuracy=0.8056136749437448, Testing Accuracy=0.793036750483559\n",
      "lambda=0.00000,degree=20, Training Accuracy=0.8057847433316226, Testing Accuracy=0.793036750483559\n",
      "lambda=0.00000,degree=21, Training Accuracy=0.8058242206519022, Testing Accuracy=0.7923920051579626\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7411638411213061, Testing Accuracy=0.7301587301587301\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7783111543213628, Testing Accuracy=0.7797619047619048\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.797289907030443, Testing Accuracy=0.8015873015873016\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8075793482003605, Testing Accuracy=0.8045634920634921\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8092402422474733, Testing Accuracy=0.8055555555555556\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8127645783962245, Testing Accuracy=0.8115079365079365\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.822608413846185, Testing Accuracy=0.8184523809523809\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8302647303072654, Testing Accuracy=0.8273809523809523\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8319458791598306, Testing Accuracy=0.8273809523809523\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8326142877397663, Testing Accuracy=0.8303571428571429\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8328776002106499, Testing Accuracy=0.8273809523809523\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8333839703469648, Testing Accuracy=0.8303571428571429\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8337283020396589, Testing Accuracy=0.8293650793650794\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8341131433432581, Testing Accuracy=0.8273809523809523\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8328573454051974, Testing Accuracy=0.8263888888888888\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8335865184014908, Testing Accuracy=0.8293650793650794\n",
      "lambda=0.00000,degree=17, Training Accuracy=0.8341131433432581, Testing Accuracy=0.8293650793650794\n",
      "lambda=0.00000,degree=18, Training Accuracy=0.8342144173705212, Testing Accuracy=0.8293650793650794\n",
      "lambda=0.00000,degree=19, Training Accuracy=0.8339916145105426, Testing Accuracy=0.8293650793650794\n",
      "lambda=0.00000,degree=20, Training Accuracy=0.833870085677827, Testing Accuracy=0.8303571428571429\n",
      "lambda=0.00000,degree=21, Training Accuracy=0.8335460087905856, Testing Accuracy=0.8303571428571429\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7308931860036832, Testing Accuracy=0.7612612612612613\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7636279926335175, Testing Accuracy=0.7635135135135135\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7942449355432781, Testing Accuracy=0.795045045045045\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8046500920810313, Testing Accuracy=0.8243243243243243\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8078729281767956, Testing Accuracy=0.8265765765765766\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8095303867403315, Testing Accuracy=0.8265765765765766\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8196593001841621, Testing Accuracy=0.8333333333333334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=8, Training Accuracy=0.830939226519337, Testing Accuracy=0.8423423423423423\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8361878453038674, Testing Accuracy=0.8558558558558559\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8378453038674033, Testing Accuracy=0.8468468468468469\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8378913443830571, Testing Accuracy=0.8536036036036037\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8375690607734807, Testing Accuracy=0.8490990990990991\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.838121546961326, Testing Accuracy=0.8490990990990991\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8377992633517496, Testing Accuracy=0.8513513513513513\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8382596685082873, Testing Accuracy=0.8423423423423423\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8392725598526704, Testing Accuracy=0.8468468468468469\n",
      "lambda=0.00000,degree=17, Training Accuracy=0.8397790055248618, Testing Accuracy=0.8423423423423423\n",
      "lambda=0.00000,degree=18, Training Accuracy=0.839134438305709, Testing Accuracy=0.8423423423423423\n",
      "lambda=0.00000,degree=19, Training Accuracy=0.8392725598526704, Testing Accuracy=0.8423423423423423\n",
      "lambda=0.00000,degree=20, Training Accuracy=0.8398710865561694, Testing Accuracy=0.8400900900900901\n",
      "lambda=0.00000,degree=21, Training Accuracy=0.8397790055248618, Testing Accuracy=0.8378378378378378\n",
      "Accuracy per jet nbr: \n",
      "\n",
      "[0.8414207103551776, 0.7949709864603481, 0.8303571428571429, 0.8558558558558559]\n"
     ]
    }
   ],
   "source": [
    "#separate data with respect to column 24 and remove None\n",
    "split_x_test, _, split_ids_test =  separate(y_donotUse, tX_test, ids_test)\n",
    "\n",
    "\n",
    "split_x_cleaned_test = removeNone(split_x_test, dataStatistics(split_x_test))\n",
    "\n",
    "#median instead of None\n",
    "split_x_with_median = putMedianInsteadOfNone(split_x_cleaned_test)\n",
    "\n",
    "split_x_with_median_with_momentum = add_momentum_vector(split_x_with_median)\n",
    "\n",
    "#line dropped when None\n",
    "#split_x_drop_lines, split_y_dropped_split_indexes_dropped = dropLineIfNone(split_x_cleaned_test, _, split_ids_test)\n",
    "\n",
    "#degrees for polynomial feature expension\n",
    "degrees = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n",
    "\n",
    "y_res = []\n",
    "\n",
    "acc = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(cleaned_with_median)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #training: chose either cross calidation or cross validation for logistic regression with regularization\n",
    "    w_star, d, accuracy, training_set = crossValidation(cleaned_with_median_with_momentum[i], split_y[i], 0.98, degrees ,6)\n",
    "    #w_star, d, accuracy, training_set = crossValidationForLogistic_reg(cleaned_with_median[i], split_y[i], 0.9, degrees ,6)\n",
    "    \n",
    "    \n",
    "    #polynomial feature expension and normalization using the training data\n",
    "    mean = np.mean(build_poly(training_set,d), axis = 0)\n",
    "    std = np.std(build_poly(training_set,d), axis = 0)\n",
    "    \n",
    "      \n",
    "    #put 1 if std = 0\n",
    "    std = std + (std == 0)\n",
    "    \n",
    "    extended_and_normalized = (build_poly(split_x_with_median_with_momentum[i], d) - mean) / std\n",
    "    \n",
    "    #adding bias term\n",
    "    bias = np.ones(shape=split_x_with_median_with_momentum[i].shape)          \n",
    "    x_test_ready = np.c_[bias, extended_and_normalized]\n",
    "    \n",
    "    #prediction\n",
    "    y_res.append(predict_labels(w_star, x_test_ready))\n",
    "\n",
    "\n",
    "    acc.append(accuracy)\n",
    "\n",
    "print(\"Accuracy per jet nbr: \\n\")\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "#reassemble the data for the submission\n",
    "y_pred = put_together(y_res, split_ids_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
