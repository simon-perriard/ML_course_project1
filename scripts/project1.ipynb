{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data according to the value of column 24 (PRI_jet_num) \n",
    "\n",
    "def separate(y, tX, ids):\n",
    "    \n",
    "    split_x = []\n",
    "    split_y = []\n",
    "    split_ids = []\n",
    "    \n",
    "    jet_column_nbr = 22\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        split_x.append(tX[np.where(tX[:,jet_column_nbr] == i)])\n",
    "        split_y.append(y[np.where(tX[:,jet_column_nbr] == i)])\n",
    "        split_ids.append(ids[np.where(tX[:,jet_column_nbr] == i)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return split_x, split_y, split_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_x, split_y, split_ids = separate(y, tX, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns from each set of data given a boolean array\n",
    "\n",
    "def removeNone(data, selection):\n",
    "   \n",
    "    cleaned=[]\n",
    "    \n",
    "    for i in range(4):\n",
    "        curr_data = data[i]\n",
    "        \n",
    "        cleaned.append(curr_data[:,selection[i]])\n",
    "      \n",
    "    return cleaned\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print statistics about the None values (-999) for each columns\n",
    "#returns a boolean array that can be used to filter the columns that have 100% of undefined values (-999)\n",
    "def dataStatistics(data):\n",
    "    \n",
    "    stats=[]\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        print(\"Statistics \")\n",
    "        print(\"Type :\")\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "        nones = (data[i] == -999)\n",
    "    \n",
    "        mean = np.sum(nones, axis=0)/nones.shape[0]\n",
    "        print(mean) \n",
    "        stats.append(mean != 1)\n",
    "    \n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.26145747 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 1.         1.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09751883 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05859584 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.0666396 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "selection = dataStatistics(split_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = removeNone(split_x, selection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can either drop the lines with residual Nones or replace the Nones by the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the value of column 0 (can be None sometimes) by the median value of this column\n",
    "\n",
    "def putMedianInsteadOfNone(cleaned):\n",
    "    \n",
    "    completed_data = []\n",
    "    \n",
    "    for i in range(len(cleaned)):\n",
    "        #current PRI_jet_num\n",
    "        current = cleaned[i]\n",
    "        \n",
    "        median = np.median(current[np.where(current[:,0] != -999)], axis = 0)\n",
    "        \n",
    "        #replace -999 by median value\n",
    "        current[np.where(current[:,0] == -999)] = median\n",
    "        \n",
    "        completed_data.append(current)\n",
    "    \n",
    "    \n",
    "    return completed_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_with_median = putMedianInsteadOfNone(cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of putting the median we can simply drop the data where columns 0 == -999\n",
    "def dropLineIfNone(cleaned, split_y, split_ids):\n",
    "    \n",
    "    res_x=[]\n",
    "    res_y=[]\n",
    "    res_ids=[]\n",
    "    \n",
    "    for i in range(len(cleaned)):\n",
    "        \n",
    "        current = cleaned[i]\n",
    "        \n",
    "        drop_indexes = np.where(current[:,0] != -999)\n",
    "        \n",
    "        res_x.append(current[drop_indexes])\n",
    "        res_y.append(current[drop_indexes])\n",
    "        res_ids.append(current[drop_indexes])\n",
    "        \n",
    "    return res_x, res_y, res_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropped_x, dropped_y, dropped_ids = dropLineIfNone(cleaned, split_y, split_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, the first values in each of the split data has a PRI_jet_num = 0, then 1 and so on. The data is clean and we can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Expension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to perform polynomial feature expension\n",
    "\n",
    "def build_poly(x, degree):\n",
    "   \n",
    "    x_extended = x\n",
    "\n",
    "    for d in range (2, degree +1):\n",
    "        x_extended = np.c_[x_extended, x**d]\n",
    "        \n",
    "\n",
    "    return x_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to split the training set into a (new) training set and a test set (same as in lab03)\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    " \n",
    "    # split the data based on the given ratio\n",
    "\n",
    "    training_nbr = int(x.shape[0] * ratio)\n",
    "    indexes = np.random.choice(x.shape[0],training_nbr, replace=False)\n",
    "    \n",
    "    x_train = x[indexes]\n",
    "    y_train = y[indexes]\n",
    "    x_test = np.delete(x, indexes, axis = 0)\n",
    "    y_test = np.delete(y, indexes, axis = 0)\n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross-validation \n",
    "\n",
    "def crossValidation(x, y, splitRatio, degrees, seed =1):\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, splitRatio, seed)\n",
    "    \n",
    "    a_training = []\n",
    "    a_testing = []\n",
    "    weights = []\n",
    "    degr = []\n",
    "    \n",
    "    # define parameter (just add more for loops if there are more parameters for the model)\n",
    "    lambdas = np.arange(0.001,0.01,0.001)\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        for ind_d, d in enumerate(degrees):\n",
    "            \n",
    "            \n",
    "            #perform polynomial feature expension\n",
    "            x_test_poly = build_poly(x_test,d)\n",
    "            x_train_poly = build_poly(x_train, d)\n",
    "           \n",
    "            \n",
    "            #normalize data (DANGER: the test set must be normalized with the training set's mean and std)\n",
    "            mean = np.mean(x_train_poly, axis =0)\n",
    "            std = np.std(x_train_poly, axis = 0)\n",
    "            \n",
    "              \n",
    "            #put 1 if std = 0\n",
    "            std = std + (std == 0)\n",
    "\n",
    "            \n",
    "            x_train_ready = (x_train_poly - mean) / std\n",
    "            x_test_ready = (x_test_poly - mean) / std\n",
    "            \n",
    "            \n",
    "            #add bias term\n",
    "            bias_tr = np.ones(shape=x_train.shape)\n",
    "            bias_te = np.ones(shape=x_test.shape)\n",
    "            \n",
    "            x_train_ready = np.c_[bias_tr, x_train_ready]\n",
    "            x_test_ready = np.c_[bias_te, x_test_ready]\n",
    "            \n",
    "            \n",
    "            #Models\n",
    "        \n",
    "            #ideal : lambdas = np.arange(0.01,0.2,0.04)\n",
    "            w_star, e_tr = ridge_regression(y_train,x_train_ready, lambda_)\n",
    "        \n",
    "            #ideal : lambdas = np.arange(0,0.3,0.1)\n",
    "            #w_star, e_tr = logistic_regression(y_train, x_train_ready,np.ones(x_train_ready.shape[1])  ,400, lambda_)\n",
    "        \n",
    "            #don't usel least squares with lambda bigger than 0.35 ideal: lambdas = np.arange(0.001,0.13,0.01)\n",
    "            #w_star, e_tr = least_squares_GD(y_train, x_train_ready,np.ones(x_train_ready.shape[1])  ,400, lambda_)    \n",
    "            #w_star, e_tr = least_squares_SGD(y_train, x_train,np.ones(x_train.shape[1])  ,400, lambda_)\n",
    "        \n",
    "            #DON'T REALLY NEED TO DO CROSS VALIDATION FOR THIS ONE ;) BUT PRACTICAL TO RUN IT HERE\n",
    "            #w_star, e_tr = least_squares(y_train, x_train_ready)  \n",
    "        \n",
    "            degr.append(d)\n",
    "        \n",
    "            #compare the prediction with the reality\n",
    "            accuracy_training = np.count_nonzero(predict_labels(w_star, x_train_ready) + y_train)/len(y_train)\n",
    "            accuracy_testing = np.count_nonzero(predict_labels(w_star, x_test_ready) + y_test)/len(y_test)\n",
    "        \n",
    "            a_training.append(accuracy_training)\n",
    "            a_testing.append(accuracy_testing)\n",
    "            weights.append(w_star)\n",
    "            print(\"lambda={l:.5f},degree={deg}, Training Accuracy={tr}, Testing Accuracy={te}\".format(\n",
    "                   l=lambda_, tr=a_training[ind*len(degrees)+ind_d], te=a_testing[ind*len(degrees)+ind_d], deg=d))\n",
    "        \n",
    "            #plt.plot(lambdas, a_training,'r--' , lambdas, a_testing, 'g--')\n",
    "            #plt.show\n",
    "    \n",
    "    return weights[np.argmax(a_testing)], degr[np.argmax(a_testing)], a_testing[np.argmax(a_testing)], x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross-validation \n",
    "\n",
    "def crossValidationForLogistic_reg(x, y, splitRatio, degrees, seed =1):\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, splitRatio, seed)\n",
    "    \n",
    "    a_training = []\n",
    "    a_testing = []\n",
    "    weights = []\n",
    "    degr = []\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    # define parameter (just add more for loops if there are more parameters for the model)\n",
    "    lambdas = np.arange(0.0001,0.3,0.1)\n",
    "    gammas = np.arange(0.01,1,0.3)\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        for ind_d, d in enumerate(degrees):\n",
    "            \n",
    "            for ind_g, gamma in enumerate(gammas):\n",
    "            \n",
    "                #perform polynomial feature expension\n",
    "                x_test_poly = build_poly(x_test,d)\n",
    "                x_train_poly = build_poly(x_train, d)\n",
    "            \n",
    "                #normalize data (DANGER: the test set must be normalized with the training set's mean and std)\n",
    "                mean = np.mean(x_train_poly, axis =0)\n",
    "                std = np.std(x_train_poly, axis = 0)\n",
    "            \n",
    "                #put 1 if std = 0\n",
    "                std = std + (std == 0)\n",
    "            \n",
    "                x_train_ready = (x_train_poly - mean) / std\n",
    "                x_test_ready = (x_test_poly - mean) / std\n",
    "                \n",
    "               \n",
    "                #add bias term\n",
    "                \n",
    "                bias_tr = np.ones(shape=x_train.shape)\n",
    "                bias_te = np.ones(shape=x_test.shape)\n",
    "            \n",
    "                x_train_ready = np.c_[bias_tr, x_train_ready]\n",
    "                x_test_ready = np.c_[bias_te, x_test_ready]\n",
    "                \n",
    "           \n",
    "\n",
    "                #Model\n",
    "        \n",
    "                #ideal :lambdas = np.arange(0,0.3,0.01)\n",
    "                #       gammas = np.arange(0,3,0.5)\n",
    "                w_star, e_tr = reg_logistic_regression(y_train, x_train_ready, lambda_, np.ones(x_test_ready.shape[1]), 30, gamma)\n",
    "        \n",
    "           \n",
    "                degr.append(d)\n",
    "        \n",
    "                #compare the prediction with the reality\n",
    "                accuracy_training = np.count_nonzero(predict_labels(w_star, x_train_ready) + y_train)/len(y_train)\n",
    "                accuracy_testing = np.count_nonzero(predict_labels(w_star, x_test_ready) + y_test)/len(y_test)\n",
    "        \n",
    "                a_training.append(accuracy_training)\n",
    "                a_testing.append(accuracy_testing)\n",
    "                weights.append(w_star)\n",
    "                print(\"lambda={l:.5f},degree={deg}, gamma={ga:.5f}, Training Accuracy={tr}, Testing Accuracy={te}\".format(\n",
    "                       l=lambda_, tr=a_training[index], te=a_testing[index], deg=d, ga=gamma))\n",
    "        \n",
    "                #increment index\n",
    "                index = index + 1\n",
    "    \n",
    "    return weights[np.argmax(a_testing)], degr[np.argmax(a_testing)], a_testing[np.argmax(a_testing)], x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we separated the data according to PRI_jet_num\n",
    "# we have to make separate prediction and then put them together for the submission\n",
    "\n",
    "def put_together(labels, indices):\n",
    "    \n",
    "    #First build first chunk\n",
    "    ids_0 = np.matrix(indices[0]).T\n",
    "    lab_0 = np.matrix(labels[0]).T\n",
    "    \n",
    "    unsorted_res = np.concatenate((ids_0, lab_0), axis=1)\n",
    "    \n",
    "    for i in range(1,len(labels)):\n",
    "        ids = np.matrix(indices[i]).T\n",
    "        lab = np.matrix(labels[i]).T\n",
    "        by_jet_num = np.concatenate((ids, lab), axis=1)\n",
    "        unsorted_res = np.concatenate((unsorted_res, by_jet_num), axis=0)\n",
    "    \n",
    "    sorted_res = unsorted_res[np.lexsort(np.fliplr(unsorted_res).T)]\n",
    "    \n",
    "    return sorted_res[0,:,:][:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_donotUse, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.2605448 0.        0.        0.        1.        1.        1.\n",
      " 0.        0.        0.        0.        0.        1.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.        1.        1.        1.        1.\n",
      " 1.        0.       ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09834149 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05881481 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.06376737 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "lambda=0.00100,degree=1, Training Accuracy=0.8198752238075644, Testing Accuracy=0.8227582065652522\n",
      "lambda=0.00100,degree=2, Training Accuracy=0.8249352209161375, Testing Accuracy=0.8306645316253003\n",
      "lambda=0.00100,degree=3, Training Accuracy=0.8277599225987256, Testing Accuracy=0.8349679743795037\n",
      "lambda=0.00100,degree=4, Training Accuracy=0.8301509102434359, Testing Accuracy=0.8376701361088871\n",
      "lambda=0.00100,degree=5, Training Accuracy=0.8307959208638694, Testing Accuracy=0.8387710168134508\n",
      "lambda=0.00100,degree=6, Training Accuracy=0.8310850635557878, Testing Accuracy=0.8390712570056045\n",
      "lambda=0.00100,degree=7, Training Accuracy=0.8311740305379166, Testing Accuracy=0.838871096877502\n",
      "lambda=0.00100,degree=8, Training Accuracy=0.8315410193391978, Testing Accuracy=0.8387710168134508\n",
      "lambda=0.00100,degree=9, Training Accuracy=0.8317967994128179, Testing Accuracy=0.839271417133707\n",
      "lambda=0.00200,degree=1, Training Accuracy=0.8169837968883799, Testing Accuracy=0.8217574059247398\n",
      "lambda=0.00200,degree=2, Training Accuracy=0.824290210295704, Testing Accuracy=0.8302642113690952\n",
      "lambda=0.00200,degree=3, Training Accuracy=0.8268257692863736, Testing Accuracy=0.835568454763811\n",
      "lambda=0.00200,degree=4, Training Accuracy=0.8292723612949144, Testing Accuracy=0.8367694155324259\n",
      "lambda=0.00200,degree=5, Training Accuracy=0.8299062510425819, Testing Accuracy=0.8366693354683747\n",
      "lambda=0.00200,degree=6, Training Accuracy=0.8302398772255647, Testing Accuracy=0.8377702161729383\n",
      "lambda=0.00200,degree=7, Training Accuracy=0.8303177233349274, Testing Accuracy=0.8377702161729383\n",
      "lambda=0.00200,degree=8, Training Accuracy=0.8303288442076934, Testing Accuracy=0.8377702161729383\n",
      "lambda=0.00200,degree=9, Training Accuracy=0.8304845364264187, Testing Accuracy=0.8379703763010409\n",
      "lambda=0.00300,degree=1, Training Accuracy=0.8155714460470858, Testing Accuracy=0.8203562850280224\n",
      "lambda=0.00300,degree=2, Training Accuracy=0.823812012766762, Testing Accuracy=0.8306645316253003\n",
      "lambda=0.00300,degree=3, Training Accuracy=0.8261807586659401, Testing Accuracy=0.8337670136108887\n",
      "lambda=0.00300,degree=4, Training Accuracy=0.8287941637659724, Testing Accuracy=0.8352682145716573\n",
      "lambda=0.00300,degree=5, Training Accuracy=0.8292056360583179, Testing Accuracy=0.8358686949559647\n",
      "lambda=0.00300,degree=6, Training Accuracy=0.8298172840604531, Testing Accuracy=0.8365692554043235\n",
      "lambda=0.00300,degree=7, Training Accuracy=0.829928492788114, Testing Accuracy=0.8370696557245797\n",
      "lambda=0.00300,degree=8, Training Accuracy=0.8298728884242835, Testing Accuracy=0.8374699759807847\n",
      "lambda=0.00300,degree=9, Training Accuracy=0.8300953058796054, Testing Accuracy=0.8374699759807847\n",
      "lambda=0.00400,degree=1, Training Accuracy=0.8144816005160085, Testing Accuracy=0.8198558847077662\n",
      "lambda=0.00400,degree=2, Training Accuracy=0.8234005404744165, Testing Accuracy=0.8300640512409928\n",
      "lambda=0.00400,degree=3, Training Accuracy=0.8258582533557234, Testing Accuracy=0.8333666933546837\n",
      "lambda=0.00400,degree=4, Training Accuracy=0.8283382079825625, Testing Accuracy=0.8354683746997598\n",
      "lambda=0.00400,degree=5, Training Accuracy=0.8287274385293758, Testing Accuracy=0.8353682946357086\n",
      "lambda=0.00400,degree=6, Training Accuracy=0.8292056360583179, Testing Accuracy=0.8360688550840673\n",
      "lambda=0.00400,degree=7, Training Accuracy=0.8293946908953415, Testing Accuracy=0.8363690952762209\n",
      "lambda=0.00400,degree=8, Training Accuracy=0.8295170204957685, Testing Accuracy=0.8366693354683747\n",
      "lambda=0.00400,degree=9, Training Accuracy=0.8295948666051312, Testing Accuracy=0.8364691753402722\n",
      "lambda=0.00500,degree=1, Training Accuracy=0.8138143481500428, Testing Accuracy=0.8195556445156125\n",
      "lambda=0.00500,degree=2, Training Accuracy=0.8232559691284572, Testing Accuracy=0.830564451561249\n",
      "lambda=0.00500,degree=3, Training Accuracy=0.8255579897910388, Testing Accuracy=0.83306645316253\n",
      "lambda=0.00500,degree=4, Training Accuracy=0.8282047575093693, Testing Accuracy=0.835568454763811\n",
      "lambda=0.00500,degree=5, Training Accuracy=0.8284494167102234, Testing Accuracy=0.835568454763811\n",
      "lambda=0.00500,degree=6, Training Accuracy=0.828760801147674, Testing Accuracy=0.835968775020016\n",
      "lambda=0.00500,degree=7, Training Accuracy=0.8287274385293758, Testing Accuracy=0.8360688550840673\n",
      "lambda=0.00500,degree=8, Training Accuracy=0.8290610647123586, Testing Accuracy=0.8364691753402722\n",
      "lambda=0.00500,degree=9, Training Accuracy=0.8290499438395925, Testing Accuracy=0.8361689351481185\n",
      "lambda=0.00600,degree=1, Training Accuracy=0.8130692496747145, Testing Accuracy=0.8191553242594075\n",
      "lambda=0.00600,degree=2, Training Accuracy=0.8232559691284572, Testing Accuracy=0.8306645316253003\n",
      "lambda=0.00600,degree=3, Training Accuracy=0.8252799679718864, Testing Accuracy=0.8328662930344275\n",
      "lambda=0.00600,degree=4, Training Accuracy=0.8278488895808543, Testing Accuracy=0.8356685348278623\n",
      "lambda=0.00600,degree=5, Training Accuracy=0.8281491531455388, Testing Accuracy=0.8356685348278623\n",
      "lambda=0.00600,degree=6, Training Accuracy=0.8285717463106504, Testing Accuracy=0.8356685348278623\n",
      "lambda=0.00600,degree=7, Training Accuracy=0.8285717463106504, Testing Accuracy=0.8353682946357086\n",
      "lambda=0.00600,degree=8, Training Accuracy=0.8288831307481012, Testing Accuracy=0.8357686148919136\n",
      "lambda=0.00600,degree=9, Training Accuracy=0.828872009875335, Testing Accuracy=0.8358686949559647\n",
      "lambda=0.00700,degree=1, Training Accuracy=0.8124798434181114, Testing Accuracy=0.8183546837469976\n",
      "lambda=0.00700,degree=2, Training Accuracy=0.8231670021463284, Testing Accuracy=0.8302642113690952\n",
      "lambda=0.00700,degree=3, Training Accuracy=0.825113154880395, Testing Accuracy=0.8328662930344275\n",
      "lambda=0.00700,degree=4, Training Accuracy=0.82734845030638, Testing Accuracy=0.8351681345076061\n",
      "lambda=0.00700,degree=5, Training Accuracy=0.8277710434714917, Testing Accuracy=0.8353682946357086\n",
      "lambda=0.00700,degree=6, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8356685348278623\n",
      "lambda=0.00700,degree=7, Training Accuracy=0.8284271749646912, Testing Accuracy=0.8357686148919136\n",
      "lambda=0.00700,degree=8, Training Accuracy=0.8286940759110775, Testing Accuracy=0.8360688550840673\n",
      "lambda=0.00700,degree=9, Training Accuracy=0.8285828671834166, Testing Accuracy=0.8361689351481185\n",
      "lambda=0.00800,degree=1, Training Accuracy=0.8119126789070407, Testing Accuracy=0.817654123298639\n",
      "lambda=0.00800,degree=2, Training Accuracy=0.8230446725459014, Testing Accuracy=0.8304643714971978\n",
      "lambda=0.00800,degree=3, Training Accuracy=0.8249241000433714, Testing Accuracy=0.8328662930344275\n",
      "lambda=0.00800,degree=4, Training Accuracy=0.8269480988868007, Testing Accuracy=0.8343674939951962\n",
      "lambda=0.00800,degree=5, Training Accuracy=0.827226120705953, Testing Accuracy=0.8348678943154524\n",
      "lambda=0.00800,degree=6, Training Accuracy=0.8281602740183049, Testing Accuracy=0.8350680544435548\n",
      "lambda=0.00800,degree=7, Training Accuracy=0.828282603618732, Testing Accuracy=0.8354683746997598\n",
      "lambda=0.00800,degree=8, Training Accuracy=0.8284382958374573, Testing Accuracy=0.8361689351481185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00800,degree=9, Training Accuracy=0.8284049332191591, Testing Accuracy=0.8360688550840673\n",
      "lambda=0.00900,degree=1, Training Accuracy=0.8115012066146952, Testing Accuracy=0.8174539631705364\n",
      "lambda=0.00900,degree=2, Training Accuracy=0.8228778594544099, Testing Accuracy=0.8307646116893515\n",
      "lambda=0.00900,degree=3, Training Accuracy=0.8245459903693242, Testing Accuracy=0.8325660528422738\n",
      "lambda=0.00900,degree=4, Training Accuracy=0.8267256814314787, Testing Accuracy=0.8336669335468375\n",
      "lambda=0.00900,degree=5, Training Accuracy=0.8268591319046719, Testing Accuracy=0.8343674939951962\n",
      "lambda=0.00900,degree=6, Training Accuracy=0.8279044939446848, Testing Accuracy=0.8340672538030425\n",
      "lambda=0.00900,degree=7, Training Accuracy=0.8281269114000067, Testing Accuracy=0.8353682946357086\n",
      "lambda=0.00900,degree=8, Training Accuracy=0.8282381201276676, Testing Accuracy=0.835568454763811\n",
      "lambda=0.00900,degree=9, Training Accuracy=0.8281269114000067, Testing Accuracy=0.8356685348278623\n",
      "lambda=0.00100,degree=1, Training Accuracy=0.7122612446087492, Testing Accuracy=0.7161831076724694\n",
      "lambda=0.00100,degree=2, Training Accuracy=0.7529696657066299, Testing Accuracy=0.7534493874919407\n",
      "lambda=0.00100,degree=3, Training Accuracy=0.7625986903380189, Testing Accuracy=0.7629916183107672\n",
      "lambda=0.00100,degree=4, Training Accuracy=0.7675135049936236, Testing Accuracy=0.767504835589942\n",
      "lambda=0.00100,degree=5, Training Accuracy=0.7695625385089341, Testing Accuracy=0.767504835589942\n",
      "lambda=0.00100,degree=6, Training Accuracy=0.7703936150396195, Testing Accuracy=0.768923275306254\n",
      "lambda=0.00100,degree=7, Training Accuracy=0.7704222728510224, Testing Accuracy=0.770728562217924\n",
      "lambda=0.00100,degree=8, Training Accuracy=0.7708807978334694, Testing Accuracy=0.7709864603481624\n",
      "lambda=0.00100,degree=9, Training Accuracy=0.7710527447018871, Testing Accuracy=0.770728562217924\n",
      "lambda=0.00200,degree=1, Training Accuracy=0.7118170485320036, Testing Accuracy=0.7133462282398453\n",
      "lambda=0.00200,degree=2, Training Accuracy=0.7510352634369313, Testing Accuracy=0.7526756931012251\n",
      "lambda=0.00200,degree=3, Training Accuracy=0.7599048560661422, Testing Accuracy=0.7614442295293359\n",
      "lambda=0.00200,degree=4, Training Accuracy=0.7644184613621058, Testing Accuracy=0.7640232108317214\n",
      "lambda=0.00200,degree=5, Training Accuracy=0.7668973620484604, Testing Accuracy=0.7655705996131528\n",
      "lambda=0.00200,degree=6, Training Accuracy=0.7677427674848472, Testing Accuracy=0.7676337846550613\n",
      "lambda=0.00200,degree=7, Training Accuracy=0.767556491710728, Testing Accuracy=0.7664732430689878\n",
      "lambda=0.00200,degree=8, Training Accuracy=0.7680866612216825, Testing Accuracy=0.7671179883945841\n",
      "lambda=0.00200,degree=9, Training Accuracy=0.7680580034102795, Testing Accuracy=0.7672469374597034\n",
      "lambda=0.00300,degree=1, Training Accuracy=0.7113012079267507, Testing Accuracy=0.7127014829142488\n",
      "lambda=0.00300,degree=2, Training Accuracy=0.7500465689435298, Testing Accuracy=0.7520309477756286\n",
      "lambda=0.00300,degree=3, Training Accuracy=0.7581997162876671, Testing Accuracy=0.760541586073501\n",
      "lambda=0.00300,degree=4, Training Accuracy=0.7626846637722278, Testing Accuracy=0.7618310767246937\n",
      "lambda=0.00300,degree=5, Training Accuracy=0.7647623550989411, Testing Accuracy=0.7646679561573179\n",
      "lambda=0.00300,degree=6, Training Accuracy=0.7657367206866411, Testing Accuracy=0.7646679561573179\n",
      "lambda=0.00300,degree=7, Training Accuracy=0.7658656808379544, Testing Accuracy=0.7654416505480335\n",
      "lambda=0.00300,degree=8, Training Accuracy=0.7665104815945206, Testing Accuracy=0.7653127014829143\n",
      "lambda=0.00300,degree=9, Training Accuracy=0.7667254151800427, Testing Accuracy=0.7653127014829143\n",
      "lambda=0.00400,degree=1, Training Accuracy=0.7106707360758859, Testing Accuracy=0.7115409413281754\n",
      "lambda=0.00400,degree=2, Training Accuracy=0.7491868346014415, Testing Accuracy=0.7516441005802708\n",
      "lambda=0.00400,degree=3, Training Accuracy=0.7573113241341759, Testing Accuracy=0.7593810444874275\n",
      "lambda=0.00400,degree=4, Training Accuracy=0.7613950622590953, Testing Accuracy=0.7602836879432624\n",
      "lambda=0.00400,degree=5, Training Accuracy=0.7632578200002865, Testing Accuracy=0.7624758220502902\n",
      "lambda=0.00400,degree=6, Training Accuracy=0.7646477238533294, Testing Accuracy=0.7624758220502902\n",
      "lambda=0.00400,degree=7, Training Accuracy=0.7648913152502543, Testing Accuracy=0.7632495164410058\n",
      "lambda=0.00400,degree=8, Training Accuracy=0.7651778933642838, Testing Accuracy=0.7640232108317214\n",
      "lambda=0.00400,degree=9, Training Accuracy=0.7651922222699853, Testing Accuracy=0.763765312701483\n",
      "lambda=0.00500,degree=1, Training Accuracy=0.7103268423390505, Testing Accuracy=0.7106382978723405\n",
      "lambda=0.00500,degree=2, Training Accuracy=0.7483271002593532, Testing Accuracy=0.751257253384913\n",
      "lambda=0.00500,degree=3, Training Accuracy=0.7562796429236699, Testing Accuracy=0.7588652482269503\n",
      "lambda=0.00500,degree=4, Training Accuracy=0.7604636833884996, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00500,degree=5, Training Accuracy=0.7622261387897806, Testing Accuracy=0.7618310767246937\n",
      "lambda=0.00500,degree=6, Training Accuracy=0.7636303715485249, Testing Accuracy=0.7614442295293359\n",
      "lambda=0.00500,degree=7, Training Accuracy=0.7637593316998381, Testing Accuracy=0.7622179239200516\n",
      "lambda=0.00500,degree=8, Training Accuracy=0.7641462121537779, Testing Accuracy=0.7632495164410058\n",
      "lambda=0.00500,degree=9, Training Accuracy=0.7644901058906132, Testing Accuracy=0.7629916183107672\n",
      "lambda=0.00600,degree=1, Training Accuracy=0.7100689220364241, Testing Accuracy=0.7102514506769826\n",
      "lambda=0.00600,degree=2, Training Accuracy=0.7478255885598016, Testing Accuracy=0.7504835589941973\n",
      "lambda=0.00600,degree=3, Training Accuracy=0.7553339351473728, Testing Accuracy=0.758736299161831\n",
      "lambda=0.00600,degree=4, Training Accuracy=0.7596755935749187, Testing Accuracy=0.7600257898130238\n",
      "lambda=0.00600,degree=5, Training Accuracy=0.7611371419564688, Testing Accuracy=0.7611863313990973\n",
      "lambda=0.00600,degree=6, Training Accuracy=0.7626273481494218, Testing Accuracy=0.7606705351386203\n",
      "lambda=0.00600,degree=7, Training Accuracy=0.7630858731318689, Testing Accuracy=0.7614442295293359\n",
      "lambda=0.00600,degree=8, Training Accuracy=0.7633437934344954, Testing Accuracy=0.7626047711154094\n",
      "lambda=0.00600,degree=9, Training Accuracy=0.7634154379630028, Testing Accuracy=0.7627337201805287\n",
      "lambda=0.00700,degree=1, Training Accuracy=0.7099113040737078, Testing Accuracy=0.7096067053513863\n",
      "lambda=0.00700,degree=2, Training Accuracy=0.7470518276519222, Testing Accuracy=0.7502256608639587\n",
      "lambda=0.00700,degree=3, Training Accuracy=0.7546461476737022, Testing Accuracy=0.7577047066408769\n",
      "lambda=0.00700,degree=4, Training Accuracy=0.7589304904784422, Testing Accuracy=0.7579626047711154\n",
      "lambda=0.00700,degree=5, Training Accuracy=0.7602774076143805, Testing Accuracy=0.7602836879432624\n",
      "lambda=0.00700,degree=6, Training Accuracy=0.7618822450529453, Testing Accuracy=0.7598968407479045\n",
      "lambda=0.00700,degree=7, Training Accuracy=0.7623837567524968, Testing Accuracy=0.7613152804642166\n",
      "lambda=0.00700,degree=8, Training Accuracy=0.762842281734944, Testing Accuracy=0.7617021276595745\n",
      "lambda=0.00700,degree=9, Training Accuracy=0.7629425840748542, Testing Accuracy=0.7623468729851709\n",
      "lambda=0.00800,degree=1, Training Accuracy=0.7095817392425741, Testing Accuracy=0.709993552546744\n",
      "lambda=0.00800,degree=2, Training Accuracy=0.7464930003295648, Testing Accuracy=0.7500967117988394\n",
      "lambda=0.00800,degree=3, Training Accuracy=0.753729097708808, Testing Accuracy=0.757317859445519\n",
      "lambda=0.00800,degree=4, Training Accuracy=0.7586152545530098, Testing Accuracy=0.7575757575757576\n",
      "lambda=0.00800,degree=5, Training Accuracy=0.7596469357635157, Testing Accuracy=0.7589941972920696\n",
      "lambda=0.00800,degree=6, Training Accuracy=0.76105116852226, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00800,degree=7, Training Accuracy=0.7616529825617218, Testing Accuracy=0.760541586073501\n",
      "lambda=0.00800,degree=8, Training Accuracy=0.7620828497327659, Testing Accuracy=0.7606705351386203\n",
      "lambda=0.00800,degree=9, Training Accuracy=0.7623980856581983, Testing Accuracy=0.7610573823339781\n",
      "lambda=0.00900,degree=1, Training Accuracy=0.7094671079969622, Testing Accuracy=0.7096067053513863\n",
      "lambda=0.00900,degree=2, Training Accuracy=0.7458481995729986, Testing Accuracy=0.7499677627337202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00900,degree=3, Training Accuracy=0.7533995328776741, Testing Accuracy=0.7569310122501612\n",
      "lambda=0.00900,degree=4, Training Accuracy=0.7578271647394289, Testing Accuracy=0.7571889103803997\n",
      "lambda=0.00900,degree=5, Training Accuracy=0.7591884107810687, Testing Accuracy=0.7578336557059961\n",
      "lambda=0.00900,degree=6, Training Accuracy=0.7608219060310364, Testing Accuracy=0.7591231463571889\n",
      "lambda=0.00900,degree=7, Training Accuracy=0.7611944575792746, Testing Accuracy=0.7591231463571889\n",
      "lambda=0.00900,degree=8, Training Accuracy=0.7618106005244379, Testing Accuracy=0.7607994842037396\n",
      "lambda=0.00900,degree=9, Training Accuracy=0.7616386536560202, Testing Accuracy=0.7607994842037396\n",
      "lambda=0.00100,degree=1, Training Accuracy=0.7378752122802761, Testing Accuracy=0.723302897975387\n",
      "lambda=0.00100,degree=2, Training Accuracy=0.7772876645861362, Testing Accuracy=0.760619293370385\n",
      "lambda=0.00100,degree=3, Training Accuracy=0.7901898943561015, Testing Accuracy=0.777689559348948\n",
      "lambda=0.00100,degree=4, Training Accuracy=0.7942039213956463, Testing Accuracy=0.7790789996030171\n",
      "lambda=0.00100,degree=5, Training Accuracy=0.797026973379502, Testing Accuracy=0.7850337435490274\n",
      "lambda=0.00100,degree=6, Training Accuracy=0.7982179484351911, Testing Accuracy=0.7852322350138944\n",
      "lambda=0.00100,degree=7, Training Accuracy=0.7981958933415673, Testing Accuracy=0.7852322350138944\n",
      "lambda=0.00100,degree=8, Training Accuracy=0.7985267197459254, Testing Accuracy=0.7842397776895593\n",
      "lambda=0.00100,degree=9, Training Accuracy=0.7992104276482653, Testing Accuracy=0.7844382691544264\n",
      "lambda=0.00200,degree=1, Training Accuracy=0.7381398734037626, Testing Accuracy=0.723699880905121\n",
      "lambda=0.00200,degree=2, Training Accuracy=0.7738029597935643, Testing Accuracy=0.7570464470027789\n",
      "lambda=0.00200,degree=3, Training Accuracy=0.7862861427846761, Testing Accuracy=0.7713378324732036\n",
      "lambda=0.00200,degree=4, Training Accuracy=0.7892635804238989, Testing Accuracy=0.7719333068678047\n",
      "lambda=0.00200,degree=5, Training Accuracy=0.7927482852164708, Testing Accuracy=0.777491067884081\n",
      "lambda=0.00200,degree=6, Training Accuracy=0.7943362519573895, Testing Accuracy=0.7792774910678841\n",
      "lambda=0.00200,degree=7, Training Accuracy=0.7940054255530314, Testing Accuracy=0.7802699483922191\n",
      "lambda=0.00200,degree=8, Training Accuracy=0.7942039213956463, Testing Accuracy=0.77888050813815\n",
      "lambda=0.00200,degree=9, Training Accuracy=0.7945126927063806, Testing Accuracy=0.7802699483922191\n",
      "lambda=0.00300,degree=1, Training Accuracy=0.7380295979356433, Testing Accuracy=0.723699880905121\n",
      "lambda=0.00300,degree=2, Training Accuracy=0.7717297809929203, Testing Accuracy=0.7558554982135768\n",
      "lambda=0.00300,degree=3, Training Accuracy=0.7824706115877462, Testing Accuracy=0.7693529178245335\n",
      "lambda=0.00300,degree=4, Training Accuracy=0.7863743631591716, Testing Accuracy=0.7673680031758634\n",
      "lambda=0.00300,degree=5, Training Accuracy=0.7896605721091287, Testing Accuracy=0.7739182215164748\n",
      "lambda=0.00300,degree=6, Training Accuracy=0.7915131999735339, Testing Accuracy=0.775903136165145\n",
      "lambda=0.00300,degree=7, Training Accuracy=0.7919543018460113, Testing Accuracy=0.7757046447002779\n",
      "lambda=0.00300,degree=8, Training Accuracy=0.791844026377892, Testing Accuracy=0.776300119094879\n",
      "lambda=0.00300,degree=9, Training Accuracy=0.7917116958161488, Testing Accuracy=0.7761016276300119\n",
      "lambda=0.00400,degree=1, Training Accuracy=0.7379193224675239, Testing Accuracy=0.722905915045653\n",
      "lambda=0.00400,degree=2, Training Accuracy=0.7696566021922763, Testing Accuracy=0.7542675664946408\n",
      "lambda=0.00400,degree=3, Training Accuracy=0.7810370305021945, Testing Accuracy=0.7663755458515283\n",
      "lambda=0.00400,degree=4, Training Accuracy=0.7847202311373812, Testing Accuracy=0.7663755458515283\n",
      "lambda=0.00400,degree=5, Training Accuracy=0.7867272446571536, Testing Accuracy=0.7697499007542675\n",
      "lambda=0.00400,degree=6, Training Accuracy=0.7891974151430273, Testing Accuracy=0.7719333068678047\n",
      "lambda=0.00400,degree=7, Training Accuracy=0.7901678392624777, Testing Accuracy=0.7731242556570067\n",
      "lambda=0.00400,degree=8, Training Accuracy=0.7897708475772479, Testing Accuracy=0.7739182215164748\n",
      "lambda=0.00400,degree=9, Training Accuracy=0.789947288326239, Testing Accuracy=0.7733227471218738\n",
      "lambda=0.00500,degree=1, Training Accuracy=0.7377869919057807, Testing Accuracy=0.722905915045653\n",
      "lambda=0.00500,degree=2, Training Accuracy=0.7680906905449814, Testing Accuracy=0.7544660579595077\n",
      "lambda=0.00500,degree=3, Training Accuracy=0.7795372841357712, Testing Accuracy=0.7633981738785233\n",
      "lambda=0.00500,degree=4, Training Accuracy=0.7829337685538476, Testing Accuracy=0.7631996824136562\n",
      "lambda=0.00500,degree=5, Training Accuracy=0.7846540658565095, Testing Accuracy=0.7673680031758634\n",
      "lambda=0.00500,degree=6, Training Accuracy=0.787344787278622, Testing Accuracy=0.7701468836840016\n",
      "lambda=0.00500,degree=7, Training Accuracy=0.7883593215853202, Testing Accuracy=0.7709408495434696\n",
      "lambda=0.00500,degree=8, Training Accuracy=0.788381376678944, Testing Accuracy=0.7709408495434696\n",
      "lambda=0.00500,degree=9, Training Accuracy=0.7886460378024305, Testing Accuracy=0.7719333068678047\n",
      "lambda=0.00600,degree=1, Training Accuracy=0.7379193224675239, Testing Accuracy=0.722508932115919\n",
      "lambda=0.00600,degree=2, Training Accuracy=0.7665688890849341, Testing Accuracy=0.7528781262405717\n",
      "lambda=0.00600,degree=3, Training Accuracy=0.7777067113649897, Testing Accuracy=0.7616117506947201\n",
      "lambda=0.00600,degree=4, Training Accuracy=0.7813237467193048, Testing Accuracy=0.7630011909487892\n",
      "lambda=0.00600,degree=5, Training Accuracy=0.782845548179352, Testing Accuracy=0.7645891226677253\n",
      "lambda=0.00600,degree=6, Training Accuracy=0.7853157186652258, Testing Accuracy=0.7675664946407305\n",
      "lambda=0.00600,degree=7, Training Accuracy=0.7867492997507775, Testing Accuracy=0.7691544263596666\n",
      "lambda=0.00600,degree=8, Training Accuracy=0.7870360159678877, Testing Accuracy=0.7689559348947995\n",
      "lambda=0.00600,degree=9, Training Accuracy=0.7871021812487594, Testing Accuracy=0.7699483922191346\n",
      "lambda=0.00700,degree=1, Training Accuracy=0.7378972673739, Testing Accuracy=0.722310440651052\n",
      "lambda=0.00700,degree=2, Training Accuracy=0.7657969608080987, Testing Accuracy=0.7520841603811036\n",
      "lambda=0.00700,degree=3, Training Accuracy=0.7761628548113186, Testing Accuracy=0.7610162763001191\n",
      "lambda=0.00700,degree=4, Training Accuracy=0.779559339229395, Testing Accuracy=0.7624057165541882\n",
      "lambda=0.00700,degree=5, Training Accuracy=0.7813237467193048, Testing Accuracy=0.7633981738785233\n",
      "lambda=0.00700,degree=6, Training Accuracy=0.7840806334222888, Testing Accuracy=0.7661770543866614\n",
      "lambda=0.00700,degree=7, Training Accuracy=0.7853377737588496, Testing Accuracy=0.7675664946407305\n",
      "lambda=0.00700,degree=8, Training Accuracy=0.7858229858185748, Testing Accuracy=0.7681619690353315\n",
      "lambda=0.00700,degree=9, Training Accuracy=0.7861538122229329, Testing Accuracy=0.7679634775704645\n",
      "lambda=0.00800,degree=1, Training Accuracy=0.7376326062504135, Testing Accuracy=0.722508932115919\n",
      "lambda=0.00800,degree=2, Training Accuracy=0.7649147570631437, Testing Accuracy=0.7512901945216356\n",
      "lambda=0.00800,degree=3, Training Accuracy=0.7749498246620057, Testing Accuracy=0.759428344581183\n",
      "lambda=0.00800,degree=4, Training Accuracy=0.7779934275821001, Testing Accuracy=0.7610162763001191\n",
      "lambda=0.00800,degree=5, Training Accuracy=0.7797357799783861, Testing Accuracy=0.7628026994839222\n",
      "lambda=0.00800,degree=6, Training Accuracy=0.7825808870558656, Testing Accuracy=0.7649861055974593\n",
      "lambda=0.00800,degree=7, Training Accuracy=0.7839924130477934, Testing Accuracy=0.7649861055974593\n",
      "lambda=0.00800,degree=8, Training Accuracy=0.7846761209501334, Testing Accuracy=0.7659785629217943\n",
      "lambda=0.00800,degree=9, Training Accuracy=0.784918726979996, Testing Accuracy=0.7659785629217943\n",
      "lambda=0.00900,degree=1, Training Accuracy=0.7374341104077987, Testing Accuracy=0.722111949186185\n",
      "lambda=0.00900,degree=2, Training Accuracy=0.7635914514457114, Testing Accuracy=0.7516871774513696\n",
      "lambda=0.00900,degree=3, Training Accuracy=0.7737147394190689, Testing Accuracy=0.759428344581183\n",
      "lambda=0.00900,degree=4, Training Accuracy=0.7766701219646678, Testing Accuracy=0.759428344581183\n",
      "lambda=0.00900,degree=5, Training Accuracy=0.7783242539864582, Testing Accuracy=0.7614132592298531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00900,degree=6, Training Accuracy=0.780772369378708, Testing Accuracy=0.7624057165541882\n",
      "lambda=0.00900,degree=7, Training Accuracy=0.7828676032729759, Testing Accuracy=0.7643906312028583\n",
      "lambda=0.00900,degree=8, Training Accuracy=0.7835072009880681, Testing Accuracy=0.7647876141325923\n",
      "lambda=0.00900,degree=9, Training Accuracy=0.7839041926732979, Testing Accuracy=0.7643906312028583\n",
      "lambda=0.00100,degree=1, Training Accuracy=0.7231162580839224, Testing Accuracy=0.7374830852503383\n",
      "lambda=0.00100,degree=2, Training Accuracy=0.7598135057903445, Testing Accuracy=0.7645466847090663\n",
      "lambda=0.00100,degree=3, Training Accuracy=0.7771093397503384, Testing Accuracy=0.7938655841226884\n",
      "lambda=0.00100,degree=4, Training Accuracy=0.7831252819972928, Testing Accuracy=0.7916102841677943\n",
      "lambda=0.00100,degree=5, Training Accuracy=0.7852308617837269, Testing Accuracy=0.7938655841226884\n",
      "lambda=0.00100,degree=6, Training Accuracy=0.7866847144934075, Testing Accuracy=0.7965719440685611\n",
      "lambda=0.00100,degree=7, Training Accuracy=0.7876372386825087, Testing Accuracy=0.7965719440685611\n",
      "lambda=0.00100,degree=8, Training Accuracy=0.789391888504537, Testing Accuracy=0.7965719440685611\n",
      "lambda=0.00100,degree=9, Training Accuracy=0.789391888504537, Testing Accuracy=0.7961208840775823\n",
      "lambda=0.00200,degree=1, Training Accuracy=0.7228655938236326, Testing Accuracy=0.7356788452864231\n",
      "lambda=0.00200,degree=2, Training Accuracy=0.758058855968316, Testing Accuracy=0.7627424447451511\n",
      "lambda=0.00200,degree=3, Training Accuracy=0.7716448588760214, Testing Accuracy=0.7866486242670275\n",
      "lambda=0.00200,degree=4, Training Accuracy=0.776457612673585, Testing Accuracy=0.7884528642309427\n",
      "lambda=0.00200,degree=5, Training Accuracy=0.7799669123176417, Testing Accuracy=0.7898060442038791\n",
      "lambda=0.00200,degree=6, Training Accuracy=0.7814207650273224, Testing Accuracy=0.7907081641858367\n",
      "lambda=0.00200,degree=7, Training Accuracy=0.7816212964355542, Testing Accuracy=0.790257104194858\n",
      "lambda=0.00200,degree=8, Training Accuracy=0.7826239534767133, Testing Accuracy=0.7911592241768155\n",
      "lambda=0.00200,degree=9, Training Accuracy=0.7835764776658144, Testing Accuracy=0.7898060442038791\n",
      "lambda=0.00300,degree=1, Training Accuracy=0.7231663909359803, Testing Accuracy=0.7356788452864231\n",
      "lambda=0.00300,degree=2, Training Accuracy=0.7563543389983456, Testing Accuracy=0.7627424447451511\n",
      "lambda=0.00300,degree=3, Training Accuracy=0.7678848949716749, Testing Accuracy=0.7798827244023455\n",
      "lambda=0.00300,degree=4, Training Accuracy=0.7729483130295283, Testing Accuracy=0.7834912043301759\n",
      "lambda=0.00300,degree=5, Training Accuracy=0.7755050884844839, Testing Accuracy=0.7861975642760487\n",
      "lambda=0.00300,degree=6, Training Accuracy=0.7782623953476713, Testing Accuracy=0.7898060442038791\n",
      "lambda=0.00300,degree=7, Training Accuracy=0.778613325312077, Testing Accuracy=0.7884528642309427\n",
      "lambda=0.00300,degree=8, Training Accuracy=0.779666115205294, Testing Accuracy=0.7880018042399639\n",
      "lambda=0.00300,degree=9, Training Accuracy=0.7795157166491201, Testing Accuracy=0.7880018042399639\n",
      "lambda=0.00400,degree=1, Training Accuracy=0.7228154609715747, Testing Accuracy=0.7365809652683807\n",
      "lambda=0.00400,degree=2, Training Accuracy=0.7546498220283752, Testing Accuracy=0.7604871447902571\n",
      "lambda=0.00400,degree=3, Training Accuracy=0.7643755953276182, Testing Accuracy=0.7758231844835363\n",
      "lambda=0.00400,degree=4, Training Accuracy=0.7696395447937033, Testing Accuracy=0.7789806044203879\n",
      "lambda=0.00400,degree=5, Training Accuracy=0.7724971173610067, Testing Accuracy=0.7821380243572396\n",
      "lambda=0.00400,degree=6, Training Accuracy=0.774402165739209, Testing Accuracy=0.7848443843031123\n",
      "lambda=0.00400,degree=7, Training Accuracy=0.7756554870406578, Testing Accuracy=0.7866486242670275\n",
      "lambda=0.00400,degree=8, Training Accuracy=0.776407479821527, Testing Accuracy=0.7852954442940911\n",
      "lambda=0.00400,degree=9, Training Accuracy=0.7769589411941645, Testing Accuracy=0.7861975642760487\n",
      "lambda=0.00500,degree=1, Training Accuracy=0.7225146638592269, Testing Accuracy=0.7370320252593595\n",
      "lambda=0.00500,degree=2, Training Accuracy=0.7535468992831003, Testing Accuracy=0.7604871447902571\n",
      "lambda=0.00500,degree=3, Training Accuracy=0.7621196169850103, Testing Accuracy=0.7735678845286423\n",
      "lambda=0.00500,degree=4, Training Accuracy=0.7675339650072692, Testing Accuracy=0.7771763644564728\n",
      "lambda=0.00500,degree=5, Training Accuracy=0.7704918032786885, Testing Accuracy=0.7785295444294091\n",
      "lambda=0.00500,degree=6, Training Accuracy=0.7726475159171805, Testing Accuracy=0.7830401443391971\n",
      "lambda=0.00500,degree=7, Training Accuracy=0.7733995086980499, Testing Accuracy=0.7839422643211547\n",
      "lambda=0.00500,degree=8, Training Accuracy=0.7737504386624555, Testing Accuracy=0.7852954442940911\n",
      "lambda=0.00500,degree=9, Training Accuracy=0.7743520328871509, Testing Accuracy=0.7843933243121335\n",
      "lambda=0.00600,degree=1, Training Accuracy=0.7223642653030531, Testing Accuracy=0.7370320252593595\n",
      "lambda=0.00600,degree=2, Training Accuracy=0.7528450393542888, Testing Accuracy=0.7595850248082995\n",
      "lambda=0.00600,degree=3, Training Accuracy=0.7611169599438512, Testing Accuracy=0.7695083446098331\n",
      "lambda=0.00600,degree=4, Training Accuracy=0.7667819722264, Testing Accuracy=0.7758231844835363\n",
      "lambda=0.00600,degree=5, Training Accuracy=0.7687371534566602, Testing Accuracy=0.7771763644564728\n",
      "lambda=0.00600,degree=6, Training Accuracy=0.7712939289116157, Testing Accuracy=0.7798827244023455\n",
      "lambda=0.00600,degree=7, Training Accuracy=0.7714944603198476, Testing Accuracy=0.7798827244023455\n",
      "lambda=0.00600,degree=8, Training Accuracy=0.7718955231363113, Testing Accuracy=0.7816869643662607\n",
      "lambda=0.00600,degree=9, Training Accuracy=0.7721963202486589, Testing Accuracy=0.7816869643662607\n",
      "lambda=0.00700,degree=1, Training Accuracy=0.722464531007169, Testing Accuracy=0.7374830852503383\n",
      "lambda=0.00700,degree=2, Training Accuracy=0.7523437108337093, Testing Accuracy=0.7604871447902571\n",
      "lambda=0.00700,degree=3, Training Accuracy=0.7597633729382864, Testing Accuracy=0.7686062246278755\n",
      "lambda=0.00700,degree=4, Training Accuracy=0.7652278538126034, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00700,degree=5, Training Accuracy=0.7670326364866897, Testing Accuracy=0.7749210645015787\n",
      "lambda=0.00700,degree=6, Training Accuracy=0.7690880834210658, Testing Accuracy=0.7780784844384303\n",
      "lambda=0.00700,degree=7, Training Accuracy=0.769990474758109, Testing Accuracy=0.7789806044203879\n",
      "lambda=0.00700,degree=8, Training Accuracy=0.7705920689828044, Testing Accuracy=0.7789806044203879\n",
      "lambda=0.00700,degree=9, Training Accuracy=0.7707926003910363, Testing Accuracy=0.7794316644113667\n",
      "lambda=0.00800,degree=1, Training Accuracy=0.7220133353386474, Testing Accuracy=0.7383852052322959\n",
      "lambda=0.00800,degree=2, Training Accuracy=0.7517922494610718, Testing Accuracy=0.7613892647722147\n",
      "lambda=0.00800,degree=3, Training Accuracy=0.7581591216724319, Testing Accuracy=0.7677041046459179\n"
     ]
    }
   ],
   "source": [
    "#separate data with respect to column 24 and remove None\n",
    "split_x_test, _, split_ids_test =  separate(y_donotUse, tX_test, ids_test)\n",
    "\n",
    "\n",
    "split_x_cleaned_test = removeNone(split_x_test, dataStatistics(split_x_test))\n",
    "\n",
    "#median instead of None\n",
    "split_x_with_median = putMedianInsteadOfNone(split_x_cleaned_test)\n",
    "\n",
    "\n",
    "\n",
    "#line dropped when None\n",
    "#split_x_drop_lines, split_y_dropped_split_indexes_dropped = dropLineIfNone(split_x_cleaned_test, _, split_ids_test)\n",
    "\n",
    "#degrees for polynomial feature expension\n",
    "degrees = [1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "y_res = []\n",
    "\n",
    "acc = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(cleaned_with_median)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #training: chose either cross calidation or cross validation for logistic regression with regularization\n",
    "    w_star, d, accuracy, training_set = crossValidation(cleaned_with_median[i], split_y[i], 0.9, degrees ,6)\n",
    "    #w_star, d, accuracy, training_set = crossValidationForLogistic_reg(cleaned_with_median[i], split_y[i], 0.9, degrees ,6)\n",
    "    \n",
    "    \n",
    "    #polynomial feature expension and normalization using the training data\n",
    "    mean = np.mean(build_poly(training_set,d), axis = 0)\n",
    "    std = np.std(build_poly(training_set,d), axis = 0)\n",
    "    \n",
    "      \n",
    "    #put 1 if std = 0\n",
    "    std = std + (std == 0)\n",
    "    \n",
    "    extended_and_normalized = build_poly(split_x_with_median[i], d) - mean / std\n",
    "    \n",
    "    #adding bias term\n",
    "    bias = np.ones(shape=split_x_with_median[i].shape)          \n",
    "    x_test_ready = np.c_[bias, extended_and_normalized]\n",
    "    \n",
    "    #prediction\n",
    "    y_res.append(predict_labels(w_star, x_test_ready))\n",
    "\n",
    "\n",
    "    acc.append(accuracy)\n",
    "\n",
    "print(\"Accuracy per jet nbr: \\n\")\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "\n",
    "#reassemble the data for the submission\n",
    "y_pred = put_together(y_res, split_ids_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
