{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from validation_helpers import *\n",
    "from plots import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_x, split_y, split_ids = separate(y, tX, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.26145747 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 1.         1.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09751883 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05859584 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.0666396 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "selection = dataStatistics(split_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = removeNone(split_x, selection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can either drop the lines with residual Nones or replace the Nones by the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_with_median = putMedianInsteadOfNone(cleaned)\n",
    "\n",
    "cleaned_with_median_with_momentum = add_momentum_vector(cleaned_with_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropped_x, dropped_y, dropped_ids = dropLineIfNone(cleaned, split_y, split_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, the first values in each of the split data has a PRI_jet_num = 0, then 1 and so on. The data is clean and we can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_donotUse, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.2605448 0.        0.        0.        1.        1.        1.\n",
      " 0.        0.        0.        0.        0.        1.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.        1.        1.        1.        1.\n",
      " 1.        0.       ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09834149 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05881481 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.06376737 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "lambda=0.00000,degree=1, gamma=0.01000, Training Loss=801007.58701413, Testing Loss=87844.8645882947\n",
      "lambda=0.00000,degree=1, gamma=0.21000, Training Loss=714396.5753544314, Testing Loss=77843.50804091315\n",
      "lambda=0.00000,degree=1, gamma=0.41000, Training Loss=805228.0045053678, Testing Loss=98599.17424389426\n",
      "lambda=0.00000,degree=1, gamma=0.61000, Training Loss=789972.7309027285, Testing Loss=103601.5058069002\n",
      "lambda=0.00000,degree=1, gamma=0.81000, Training Loss=838855.5102782527, Testing Loss=132226.68001677908\n",
      "lambda=0.00000,degree=2, gamma=0.01000, Training Loss=1153544.1348617943, Testing Loss=126532.56636577901\n",
      "lambda=0.00000,degree=2, gamma=0.21000, Training Loss=1089221.5007903425, Testing Loss=123100.26687727543\n",
      "lambda=0.00000,degree=2, gamma=0.41000, Training Loss=839265.36172629, Testing Loss=99715.98326761427\n",
      "lambda=0.00000,degree=2, gamma=0.61000, Training Loss=1218174.8668747356, Testing Loss=152002.12021843978\n",
      "lambda=0.00000,degree=2, gamma=0.81000, Training Loss=863137.2496693494, Testing Loss=129214.99486225689\n",
      "lambda=0.00000,degree=3, gamma=0.01000, Training Loss=1819678.9035687377, Testing Loss=194104.84402523417\n",
      "lambda=0.00000,degree=3, gamma=0.21000, Training Loss=928346.9830481466, Testing Loss=99138.8567188348\n",
      "lambda=0.00000,degree=3, gamma=0.41000, Training Loss=926243.0727610849, Testing Loss=105748.94476506163\n",
      "lambda=0.00000,degree=3, gamma=0.61000, Training Loss=933682.6470685027, Testing Loss=116293.43246840285\n",
      "lambda=0.00000,degree=3, gamma=0.81000, Training Loss=951949.131812764, Testing Loss=130876.14004295773\n",
      "lambda=0.00000,degree=4, gamma=0.01000, Training Loss=2355692.230676187, Testing Loss=265677.4434396095\n",
      "lambda=0.00000,degree=4, gamma=0.21000, Training Loss=1724416.0084181153, Testing Loss=196003.16123018187\n",
      "lambda=0.00000,degree=4, gamma=0.41000, Training Loss=1724969.1517226442, Testing Loss=200615.73983077283\n",
      "lambda=0.00000,degree=4, gamma=0.61000, Training Loss=637716.8557755635, Testing Loss=93113.3446409999\n",
      "lambda=0.00000,degree=4, gamma=0.81000, Training Loss=1740773.1789687148, Testing Loss=226954.71818649737\n",
      "lambda=0.00000,degree=5, gamma=0.01000, Training Loss=1208326.0589243737, Testing Loss=128451.50555705909\n",
      "lambda=0.00000,degree=5, gamma=0.21000, Training Loss=759329.6041510506, Testing Loss=81809.238072542\n",
      "lambda=0.00000,degree=5, gamma=0.41000, Training Loss=753650.9525632211, Testing Loss=87846.87048206119\n",
      "lambda=0.00000,degree=5, gamma=0.61000, Training Loss=761406.1279734186, Testing Loss=99306.7238749234\n",
      "lambda=0.00000,degree=5, gamma=0.81000, Training Loss=775046.804020387, Testing Loss=114573.59044836505\n",
      "lambda=0.00000,degree=6, gamma=0.01000, Training Loss=1161240.9060500234, Testing Loss=121909.9385562305\n",
      "lambda=0.00000,degree=6, gamma=0.21000, Training Loss=647423.4681463813, Testing Loss=75906.86864615943\n",
      "lambda=0.00000,degree=6, gamma=0.41000, Training Loss=634374.261668248, Testing Loss=80437.40027846847\n",
      "lambda=0.00000,degree=6, gamma=0.61000, Training Loss=642440.6613331563, Testing Loss=94777.37338462778\n",
      "lambda=0.00000,degree=6, gamma=0.81000, Training Loss=690111.651579396, Testing Loss=108224.30839986258\n",
      "lambda=0.00000,degree=7, gamma=0.01000, Training Loss=1700132.1937292248, Testing Loss=180179.01115364654\n",
      "lambda=0.00000,degree=7, gamma=0.21000, Training Loss=691597.4944800994, Testing Loss=77109.12751063732\n",
      "lambda=0.00000,degree=7, gamma=0.41000, Training Loss=680823.9499291133, Testing Loss=84659.5875625164\n",
      "lambda=0.00000,degree=7, gamma=0.61000, Training Loss=694552.2174225766, Testing Loss=102302.98568379167\n",
      "lambda=0.00000,degree=7, gamma=0.81000, Training Loss=711619.7347777545, Testing Loss=123030.21394977835\n",
      "lambda=0.00000,degree=8, gamma=0.01000, Training Loss=1902040.724830444, Testing Loss=203979.2936481973\n",
      "lambda=0.00000,degree=8, gamma=0.21000, Training Loss=701735.981653109, Testing Loss=79144.50393438747\n",
      "lambda=0.00000,degree=8, gamma=0.41000, Training Loss=683309.1992043188, Testing Loss=83732.61383515976\n",
      "lambda=0.00000,degree=8, gamma=0.61000, Training Loss=690699.5992275055, Testing Loss=94911.58927033034\n",
      "lambda=0.00000,degree=8, gamma=0.81000, Training Loss=702550.4817346495, Testing Loss=110383.67805989261\n",
      "lambda=0.00000,degree=9, gamma=0.01000, Training Loss=1180167.8981441786, Testing Loss=129136.1520778004\n",
      "lambda=0.00000,degree=9, gamma=0.21000, Training Loss=680134.9739707691, Testing Loss=73505.74450668473\n",
      "lambda=0.00000,degree=9, gamma=0.41000, Training Loss=672203.2475217175, Testing Loss=81711.14988102228\n",
      "lambda=0.00000,degree=9, gamma=0.61000, Training Loss=678242.1936049159, Testing Loss=94887.27134815483\n",
      "lambda=0.00000,degree=9, gamma=0.81000, Training Loss=693693.4248838687, Testing Loss=112949.54875226674\n",
      "lambda=0.00000,degree=10, gamma=0.01000, Training Loss=796481.9090203857, Testing Loss=87586.66672565445\n",
      "lambda=0.00000,degree=10, gamma=0.21000, Training Loss=715117.6696028255, Testing Loss=77803.86532665584\n",
      "lambda=0.00000,degree=10, gamma=0.41000, Training Loss=698295.7074862751, Testing Loss=82757.78067512633\n",
      "lambda=0.00000,degree=10, gamma=0.61000, Training Loss=829199.5098242696, Testing Loss=122605.20874402154\n",
      "lambda=0.00000,degree=10, gamma=0.81000, Training Loss=856428.007349622, Testing Loss=149764.62871658418\n",
      "lambda=0.00000,degree=11, gamma=0.01000, Training Loss=1451256.8767238227, Testing Loss=163750.37596155744\n",
      "lambda=0.00000,degree=11, gamma=0.21000, Training Loss=702013.9593717436, Testing Loss=79403.36070902801\n",
      "lambda=0.00000,degree=11, gamma=0.41000, Training Loss=691376.8361744832, Testing Loss=85878.02191004681\n",
      "lambda=0.00000,degree=11, gamma=0.61000, Training Loss=700766.033037827, Testing Loss=101899.58887315137\n",
      "lambda=0.00000,degree=11, gamma=0.81000, Training Loss=725256.7871705646, Testing Loss=121069.81168191742\n",
      "lambda=0.00000,degree=12, gamma=0.01000, Training Loss=1841277.584450333, Testing Loss=197539.53619690312\n",
      "lambda=0.00000,degree=12, gamma=0.21000, Training Loss=718551.1049125494, Testing Loss=79742.58413514943\n",
      "lambda=0.00000,degree=12, gamma=0.41000, Training Loss=696266.1943947267, Testing Loss=85134.05448090864\n",
      "lambda=0.00000,degree=12, gamma=0.61000, Training Loss=704960.6646032289, Testing Loss=94428.62767824181\n",
      "lambda=0.00000,degree=12, gamma=0.81000, Training Loss=714995.8988385169, Testing Loss=110111.93865979792\n",
      "lambda=0.00000,degree=13, gamma=0.01000, Training Loss=1849932.5553153902, Testing Loss=197158.4714319914\n",
      "lambda=0.00000,degree=13, gamma=0.21000, Training Loss=715444.3134105226, Testing Loss=79317.64201825992\n",
      "lambda=0.00000,degree=13, gamma=0.41000, Training Loss=702791.8348825083, Testing Loss=84658.861032827\n",
      "lambda=0.00000,degree=13, gamma=0.61000, Training Loss=703516.4728874984, Testing Loss=95611.06380919035\n",
      "lambda=0.00000,degree=13, gamma=0.81000, Training Loss=715295.6921835942, Testing Loss=108918.7115347619\n",
      "lambda=0.00000,degree=14, gamma=0.01000, Training Loss=1905307.6883455478, Testing Loss=200532.37916083552\n",
      "lambda=0.00000,degree=14, gamma=0.21000, Training Loss=707366.0284810588, Testing Loss=80497.67122983896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=14, gamma=0.41000, Training Loss=688708.3644440247, Testing Loss=83287.05060262501\n",
      "lambda=0.00000,degree=14, gamma=0.61000, Training Loss=688879.7290670391, Testing Loss=93418.04398808022\n",
      "lambda=0.00000,degree=14, gamma=0.81000, Training Loss=699695.2975332655, Testing Loss=107997.59158628443\n",
      "lambda=0.00000,degree=15, gamma=0.01000, Training Loss=1852733.2065816168, Testing Loss=206943.8746222811\n",
      "lambda=0.00000,degree=15, gamma=0.21000, Training Loss=1744738.868331446, Testing Loss=195601.49380683905\n",
      "lambda=0.00000,degree=15, gamma=0.41000, Training Loss=1746642.3887609988, Testing Loss=202125.78029112\n",
      "lambda=0.00000,degree=15, gamma=0.61000, Training Loss=1752029.6577556692, Testing Loss=212406.87123562352\n",
      "lambda=0.00000,degree=15, gamma=0.81000, Training Loss=1767280.6213269376, Testing Loss=227669.0069228728\n",
      "lambda=0.00000,degree=16, gamma=0.01000, Training Loss=1482087.1342017197, Testing Loss=157704.12152207876\n",
      "lambda=0.00000,degree=16, gamma=0.21000, Training Loss=749599.0704305907, Testing Loss=81958.64843043263\n",
      "lambda=0.00000,degree=16, gamma=0.41000, Training Loss=750948.0094543922, Testing Loss=87527.0138828158\n",
      "lambda=0.00000,degree=16, gamma=0.61000, Training Loss=754937.3578968456, Testing Loss=98619.24456976632\n",
      "lambda=0.00000,degree=16, gamma=0.81000, Training Loss=767782.8641204595, Testing Loss=113118.25350647152\n",
      "lambda=0.10000,degree=1, gamma=0.01000, Training Loss=1383921.5897469474, Testing Loss=676962.7428045626\n",
      "lambda=0.10000,degree=1, gamma=0.21000, Training Loss=73183840.19042908, Testing Loss=71831815.18538445\n",
      "lambda=0.10000,degree=1, gamma=0.41000, Training Loss=207883322.24668008, Testing Loss=207208518.3286791\n",
      "lambda=0.10000,degree=1, gamma=0.61000, Training Loss=675110096.7268413, Testing Loss=674463857.2157886\n",
      "lambda=0.10000,degree=1, gamma=0.81000, Training Loss=2841215970.658642, Testing Loss=2840509169.128901\n",
      "lambda=0.10000,degree=2, gamma=0.01000, Training Loss=2188862.7097034105, Testing Loss=796931.3151766687\n",
      "lambda=0.10000,degree=2, gamma=0.21000, Training Loss=131402507.65731266, Testing Loss=130736244.48762827\n",
      "lambda=0.10000,degree=2, gamma=0.41000, Training Loss=480815768.387506, Testing Loss=480222940.4580099\n",
      "lambda=0.10000,degree=2, gamma=0.61000, Training Loss=2103204571.0782065, Testing Loss=2102497562.3110101\n",
      "lambda=0.10000,degree=2, gamma=0.81000, Training Loss=1239762372.0693967, Testing Loss=1238983719.847154\n",
      "lambda=0.10000,degree=3, gamma=0.01000, Training Loss=2537976.594876937, Testing Loss=710211.2028457557\n",
      "lambda=0.10000,degree=3, gamma=0.21000, Training Loss=102356297.39715818, Testing Loss=101675941.20927504\n",
      "lambda=0.10000,degree=3, gamma=0.41000, Training Loss=468897416.3288011, Testing Loss=468203629.76660347\n",
      "lambda=0.10000,degree=3, gamma=0.61000, Training Loss=710753040.1747805, Testing Loss=710099948.0747479\n",
      "lambda=0.10000,degree=3, gamma=0.81000, Training Loss=1005130540.1011428, Testing Loss=1004453039.0665603\n",
      "lambda=0.10000,degree=4, gamma=0.01000, Training Loss=2090328.1206208644, Testing Loss=736304.9946955667\n",
      "lambda=0.10000,degree=4, gamma=0.21000, Training Loss=174657696.75308898, Testing Loss=174029627.06291935\n",
      "lambda=0.10000,degree=4, gamma=0.41000, Training Loss=954723618.0263097, Testing Loss=954016471.0976108\n",
      "lambda=0.10000,degree=4, gamma=0.61000, Training Loss=699860887.2130826, Testing Loss=699210867.0300694\n",
      "lambda=0.10000,degree=4, gamma=0.81000, Training Loss=1001177422.1741059, Testing Loss=1000505642.4650295\n",
      "lambda=0.10000,degree=5, gamma=0.01000, Training Loss=1842854.5560903891, Testing Loss=769423.0169469722\n",
      "lambda=0.10000,degree=5, gamma=0.21000, Training Loss=114653968.14772163, Testing Loss=112937865.61337501\n",
      "lambda=0.10000,degree=5, gamma=0.41000, Training Loss=1116502717.0830903, Testing Loss=1115795708.3110962\n",
      "lambda=0.10000,degree=5, gamma=0.61000, Training Loss=674977917.481993, Testing Loss=674324528.3226639\n",
      "lambda=0.10000,degree=5, gamma=0.81000, Training Loss=1262871322.319492, Testing Loss=1262156453.1723404\n",
      "lambda=0.10000,degree=6, gamma=0.01000, Training Loss=2288645.0140009676, Testing Loss=815515.5698800989\n",
      "lambda=0.10000,degree=6, gamma=0.21000, Training Loss=132141052.59008639, Testing Loss=131497978.02498433\n",
      "lambda=0.10000,degree=6, gamma=0.41000, Training Loss=1095742385.9803803, Testing Loss=1095034997.2706513\n",
      "lambda=0.10000,degree=6, gamma=0.61000, Training Loss=843487851.4204128, Testing Loss=842799086.5662242\n",
      "lambda=0.10000,degree=6, gamma=0.81000, Training Loss=943822092.7317545, Testing Loss=942135804.3090563\n",
      "lambda=0.10000,degree=7, gamma=0.01000, Training Loss=2494170.5360525316, Testing Loss=762895.0381066025\n",
      "lambda=0.10000,degree=7, gamma=0.21000, Training Loss=208793275.3068121, Testing Loss=208119434.79902762\n",
      "lambda=0.10000,degree=7, gamma=0.41000, Training Loss=375669352.0287859, Testing Loss=374962205.0848946\n",
      "lambda=0.10000,degree=7, gamma=0.61000, Training Loss=621535152.7798398, Testing Loss=619894490.0653094\n",
      "lambda=0.10000,degree=7, gamma=0.81000, Training Loss=928515510.1361896, Testing Loss=926817619.0140443\n",
      "lambda=0.10000,degree=8, gamma=0.01000, Training Loss=1793447.557973153, Testing Loss=812654.1332905411\n",
      "lambda=0.10000,degree=8, gamma=0.21000, Training Loss=185323743.29454714, Testing Loss=184705926.93045214\n",
      "lambda=0.10000,degree=8, gamma=0.41000, Training Loss=452816419.95481557, Testing Loss=452149297.88453853\n",
      "lambda=0.10000,degree=8, gamma=0.61000, Training Loss=1159375429.0339432, Testing Loss=1158722985.1289632\n",
      "lambda=0.10000,degree=8, gamma=0.81000, Training Loss=1708037807.056775, Testing Loss=1707388105.2403214\n",
      "lambda=0.10000,degree=9, gamma=0.01000, Training Loss=1778681.3971246288, Testing Loss=737208.8381578102\n",
      "lambda=0.10000,degree=9, gamma=0.21000, Training Loss=184568330.8877353, Testing Loss=183958442.6701262\n",
      "lambda=0.10000,degree=9, gamma=0.41000, Training Loss=454076203.8094512, Testing Loss=453448648.2690971\n",
      "lambda=0.10000,degree=9, gamma=0.61000, Training Loss=1194907262.27613, Testing Loss=1194260230.42016\n",
      "lambda=0.10000,degree=9, gamma=0.81000, Training Loss=1719990988.9165626, Testing Loss=1719337626.134407\n",
      "lambda=0.10000,degree=10, gamma=0.01000, Training Loss=1763064.4203473777, Testing Loss=715598.7769127588\n",
      "lambda=0.10000,degree=10, gamma=0.21000, Training Loss=336441800.86517775, Testing Loss=335734895.7087132\n",
      "lambda=0.10000,degree=10, gamma=0.41000, Training Loss=890317280.5653325, Testing Loss=889608303.0295109\n",
      "lambda=0.10000,degree=10, gamma=0.61000, Training Loss=1198341350.7535205, Testing Loss=1197688155.755782\n",
      "lambda=0.10000,degree=10, gamma=0.81000, Training Loss=1198398887.3261538, Testing Loss=1197728068.2792277\n",
      "lambda=0.10000,degree=11, gamma=0.01000, Training Loss=2166404.1559696165, Testing Loss=873876.3510836768\n",
      "lambda=0.10000,degree=11, gamma=0.21000, Training Loss=137019210.6687614, Testing Loss=136383258.63533503\n",
      "lambda=0.10000,degree=11, gamma=0.41000, Training Loss=341244804.48864216, Testing Loss=339732621.92276996\n",
      "lambda=0.10000,degree=11, gamma=0.61000, Training Loss=611059705.5952551, Testing Loss=610380771.0271957\n",
      "lambda=0.10000,degree=11, gamma=0.81000, Training Loss=882267387.2168884, Testing Loss=881609396.2499343\n",
      "lambda=0.10000,degree=12, gamma=0.01000, Training Loss=1770779.364371744, Testing Loss=705843.0899917729\n",
      "lambda=0.10000,degree=12, gamma=0.21000, Training Loss=139334152.99672237, Testing Loss=138696174.51541868\n",
      "lambda=0.10000,degree=12, gamma=0.41000, Training Loss=345289603.27993023, Testing Loss=344402424.5628346\n",
      "lambda=0.10000,degree=12, gamma=0.61000, Training Loss=625697110.4553984, Testing Loss=625035856.2271752\n",
      "lambda=0.10000,degree=12, gamma=0.81000, Training Loss=882559766.5326295, Testing Loss=881903019.2824519\n",
      "lambda=0.10000,degree=13, gamma=0.01000, Training Loss=1764776.0302790971, Testing Loss=695831.9842169083\n",
      "lambda=0.10000,degree=13, gamma=0.21000, Training Loss=338325441.3902603, Testing Loss=337618501.6942197\n",
      "lambda=0.10000,degree=13, gamma=0.41000, Training Loss=468728207.57573, Testing Loss=468097417.0966473\n",
      "lambda=0.10000,degree=13, gamma=0.61000, Training Loss=595290042.4074883, Testing Loss=594581799.0142142\n",
      "lambda=0.10000,degree=13, gamma=0.81000, Training Loss=1754978035.8785365, Testing Loss=1754322753.7762876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.10000,degree=14, gamma=0.01000, Training Loss=1759211.1869425513, Testing Loss=693689.3629881621\n",
      "lambda=0.10000,degree=14, gamma=0.21000, Training Loss=337193138.18600845, Testing Loss=336486233.0279447\n",
      "lambda=0.10000,degree=14, gamma=0.41000, Training Loss=366839338.7465525, Testing Loss=366132416.12539124\n",
      "lambda=0.10000,degree=14, gamma=0.61000, Training Loss=1199911015.72632, Testing Loss=1199255259.1547303\n",
      "lambda=0.10000,degree=14, gamma=0.81000, Training Loss=1769019538.660829, Testing Loss=1768358586.5271337\n",
      "lambda=0.10000,degree=15, gamma=0.01000, Training Loss=2511147.642759745, Testing Loss=763601.5475296862\n",
      "lambda=0.10000,degree=15, gamma=0.21000, Training Loss=132047661.59118079, Testing Loss=130341869.48579088\n",
      "lambda=0.10000,degree=15, gamma=0.41000, Training Loss=339664770.44214135, Testing Loss=338284271.9836188\n",
      "lambda=0.10000,degree=15, gamma=0.61000, Training Loss=788258740.1733695, Testing Loss=787615217.4275476\n",
      "lambda=0.10000,degree=15, gamma=0.81000, Training Loss=1175894586.9520397, Testing Loss=1175251241.954599\n",
      "lambda=0.10000,degree=16, gamma=0.01000, Training Loss=2377394.661662523, Testing Loss=713614.973749216\n",
      "lambda=0.10000,degree=16, gamma=0.21000, Training Loss=134093195.05017361, Testing Loss=132402920.81161718\n",
      "lambda=0.10000,degree=16, gamma=0.41000, Training Loss=341657004.5033787, Testing Loss=341065886.16277343\n",
      "lambda=0.10000,degree=16, gamma=0.61000, Training Loss=2203195246.809375, Testing Loss=2202486718.288041\n",
      "lambda=0.10000,degree=16, gamma=0.81000, Training Loss=3777590322.3922877, Testing Loss=3776882381.045345\n",
      "lambda=0.20000,degree=1, gamma=0.01000, Training Loss=2614639.6638794118, Testing Loss=1095903.6624023828\n",
      "lambda=0.20000,degree=1, gamma=0.21000, Training Loss=168704472.25765064, Testing Loss=167312373.9095251\n",
      "lambda=0.20000,degree=1, gamma=0.41000, Training Loss=1460772864.934405, Testing Loss=1460066063.4046643\n",
      "lambda=0.20000,degree=1, gamma=0.61000, Training Loss=955858966.3947992, Testing Loss=954235194.7026718\n",
      "lambda=0.20000,degree=1, gamma=0.81000, Training Loss=1518337527.9568756, Testing Loss=1516628644.5770595\n",
      "lambda=0.20000,degree=2, gamma=0.01000, Training Loss=2844847.858413608, Testing Loss=1468187.3015649908\n",
      "lambda=0.20000,degree=2, gamma=0.21000, Training Loss=258407092.3347295, Testing Loss=257774386.5706079\n",
      "lambda=0.20000,degree=2, gamma=0.41000, Training Loss=630903692.8292509, Testing Loss=630122332.3578091\n",
      "lambda=0.20000,degree=2, gamma=0.61000, Training Loss=3461861842.094071, Testing Loss=3461154764.2461233\n",
      "lambda=0.20000,degree=2, gamma=0.81000, Training Loss=1653851438.4530723, Testing Loss=1652189623.4907172\n",
      "lambda=0.20000,degree=3, gamma=0.01000, Training Loss=2791259.694655497, Testing Loss=1177466.9997461787\n",
      "lambda=0.20000,degree=3, gamma=0.21000, Training Loss=621697240.3520337, Testing Loss=620990646.0605487\n",
      "lambda=0.20000,degree=3, gamma=0.41000, Training Loss=512209099.10953623, Testing Loss=511524113.16398644\n",
      "lambda=0.20000,degree=3, gamma=0.61000, Training Loss=806135333.3814609, Testing Loss=804562915.8132486\n",
      "lambda=0.20000,degree=3, gamma=0.81000, Training Loss=1337772220.1934333, Testing Loss=1336799805.7966392\n",
      "lambda=0.20000,degree=4, gamma=0.01000, Training Loss=2828810.397414742, Testing Loss=1265014.8137568994\n",
      "lambda=0.20000,degree=4, gamma=0.21000, Training Loss=174768842.52766648, Testing Loss=173988724.78598028\n",
      "lambda=0.20000,degree=4, gamma=0.41000, Training Loss=509478732.7151138, Testing Loss=508797292.3074987\n",
      "lambda=0.20000,degree=4, gamma=0.61000, Training Loss=793010656.4048811, Testing Loss=791390438.2634248\n",
      "lambda=0.20000,degree=4, gamma=0.81000, Training Loss=2099925713.0022206, Testing Loss=2099219187.790687\n",
      "lambda=0.20000,degree=5, gamma=0.01000, Training Loss=2968589.901270357, Testing Loss=1262269.664434785\n",
      "lambda=0.20000,degree=5, gamma=0.21000, Training Loss=451895572.3762078, Testing Loss=451187700.1100163\n",
      "lambda=0.20000,degree=5, gamma=0.41000, Training Loss=643282013.2060635, Testing Loss=642564182.0885483\n",
      "lambda=0.20000,degree=5, gamma=0.61000, Training Loss=863193529.7757105, Testing Loss=861456541.9102974\n",
      "lambda=0.20000,degree=5, gamma=0.81000, Training Loss=1584920435.0664122, Testing Loss=1584250220.1343687\n",
      "lambda=0.20000,degree=6, gamma=0.01000, Training Loss=3132641.3118609455, Testing Loss=1362540.3980040404\n",
      "lambda=0.20000,degree=6, gamma=0.21000, Training Loss=564242673.2605557, Testing Loss=563535250.0104512\n",
      "lambda=0.20000,degree=6, gamma=0.41000, Training Loss=482530142.9253402, Testing Loss=480841021.6557958\n",
      "lambda=0.20000,degree=6, gamma=0.61000, Training Loss=853905851.0823362, Testing Loss=852177272.2114223\n",
      "lambda=0.20000,degree=6, gamma=0.81000, Training Loss=1017301029.1178935, Testing Loss=1015940328.9376854\n",
      "lambda=0.20000,degree=7, gamma=0.01000, Training Loss=2908363.398385948, Testing Loss=1238950.833179211\n",
      "lambda=0.20000,degree=7, gamma=0.21000, Training Loss=173021492.27846158, Testing Loss=172371138.1967254\n",
      "lambda=0.20000,degree=7, gamma=0.41000, Training Loss=472969358.6082291, Testing Loss=471268722.6526057\n",
      "lambda=0.20000,degree=7, gamma=0.61000, Training Loss=867529016.062908, Testing Loss=865764486.4725925\n",
      "lambda=0.20000,degree=7, gamma=0.81000, Training Loss=1092924856.1793284, Testing Loss=1091966558.8508873\n",
      "lambda=0.20000,degree=8, gamma=0.01000, Training Loss=3182306.6351034963, Testing Loss=1460777.197552409\n",
      "lambda=0.20000,degree=8, gamma=0.21000, Training Loss=173841090.27025637, Testing Loss=173016748.20405456\n",
      "lambda=0.20000,degree=8, gamma=0.41000, Training Loss=870562157.4322939, Testing Loss=869908884.6954969\n",
      "lambda=0.20000,degree=8, gamma=0.61000, Training Loss=1578201844.3274508, Testing Loss=1577542454.7663994\n",
      "lambda=0.20000,degree=8, gamma=0.81000, Training Loss=1782203164.954846, Testing Loss=1781496363.421907\n",
      "lambda=0.20000,degree=9, gamma=0.01000, Training Loss=2297529.8410806395, Testing Loss=1250635.7422316035\n",
      "lambda=0.20000,degree=9, gamma=0.21000, Training Loss=237141794.2264873, Testing Loss=236478794.02741745\n",
      "lambda=0.20000,degree=9, gamma=0.41000, Training Loss=877617781.2238271, Testing Loss=876960010.6398895\n",
      "lambda=0.20000,degree=9, gamma=0.61000, Training Loss=1611582413.9844847, Testing Loss=1610924900.7226827\n",
      "lambda=0.20000,degree=9, gamma=0.81000, Training Loss=1734054532.2925127, Testing Loss=1733347799.8387256\n",
      "lambda=0.20000,degree=10, gamma=0.01000, Training Loss=2993712.756841464, Testing Loss=1392049.9203594422\n",
      "lambda=0.20000,degree=10, gamma=0.21000, Training Loss=240872416.5850125, Testing Loss=240215666.91072428\n",
      "lambda=0.20000,degree=10, gamma=0.41000, Training Loss=608496886.5337672, Testing Loss=607820636.7154112\n",
      "lambda=0.20000,degree=10, gamma=0.61000, Training Loss=757570692.6577783, Testing Loss=756864268.9073914\n",
      "lambda=0.20000,degree=10, gamma=0.81000, Training Loss=6452964947.483765, Testing Loss=6452256798.899365\n",
      "lambda=0.20000,degree=11, gamma=0.01000, Training Loss=3035323.2745762775, Testing Loss=1251264.7127506016\n",
      "lambda=0.20000,degree=11, gamma=0.21000, Training Loss=171449573.08665034, Testing Loss=169779461.94085416\n",
      "lambda=0.20000,degree=11, gamma=0.41000, Training Loss=447139341.4734246, Testing Loss=446467252.5557186\n",
      "lambda=0.20000,degree=11, gamma=0.61000, Training Loss=734244600.2171212, Testing Loss=733540157.819527\n",
      "lambda=0.20000,degree=11, gamma=0.81000, Training Loss=1726153967.6632764, Testing Loss=1725447131.5907614\n",
      "lambda=0.20000,degree=12, gamma=0.01000, Training Loss=2241482.466289974, Testing Loss=1199204.5726996628\n",
      "lambda=0.20000,degree=12, gamma=0.21000, Training Loss=178079054.23659396, Testing Loss=177175124.1449152\n",
      "lambda=0.20000,degree=12, gamma=0.41000, Training Loss=448424392.483735, Testing Loss=447752371.43151426\n",
      "lambda=0.20000,degree=12, gamma=0.61000, Training Loss=734571533.507077, Testing Loss=733869347.8697101\n",
      "lambda=0.20000,degree=12, gamma=0.81000, Training Loss=1433733121.0144236, Testing Loss=1433026146.7836046\n",
      "lambda=0.20000,degree=13, gamma=0.01000, Training Loss=2232036.524569801, Testing Loss=1182963.1536965552\n",
      "lambda=0.20000,degree=13, gamma=0.21000, Training Loss=244470938.41441256, Testing Loss=243832310.7481999\n",
      "lambda=0.20000,degree=13, gamma=0.41000, Training Loss=895947955.7097561, Testing Loss=895286114.0003036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.20000,degree=13, gamma=0.61000, Training Loss=719630973.7066249, Testing Loss=718926654.8589602\n",
      "lambda=0.20000,degree=13, gamma=0.81000, Training Loss=1281162779.011339, Testing Loss=1280455908.399248\n",
      "lambda=0.20000,degree=14, gamma=0.01000, Training Loss=2241423.8809092315, Testing Loss=1185421.311684204\n",
      "lambda=0.20000,degree=14, gamma=0.21000, Training Loss=234427005.2677524, Testing Loss=233768332.83891314\n",
      "lambda=0.20000,degree=14, gamma=0.41000, Training Loss=901174570.2397978, Testing Loss=900507956.6489261\n",
      "lambda=0.20000,degree=14, gamma=0.61000, Training Loss=722926713.6635984, Testing Loss=722220704.6556474\n",
      "lambda=0.20000,degree=14, gamma=0.81000, Training Loss=1251853543.1759608, Testing Loss=1251146300.9889796\n",
      "lambda=0.20000,degree=15, gamma=0.01000, Training Loss=2720897.353093419, Testing Loss=1486756.4593180998\n",
      "lambda=0.20000,degree=15, gamma=0.21000, Training Loss=262354102.99438986, Testing Loss=261697484.48247203\n",
      "lambda=0.20000,degree=15, gamma=0.41000, Training Loss=598375952.1463926, Testing Loss=597723630.4839396\n",
      "lambda=0.20000,degree=15, gamma=0.61000, Training Loss=675330577.7173376, Testing Loss=674707551.5052825\n",
      "lambda=0.20000,degree=15, gamma=0.81000, Training Loss=990127027.2460306, Testing Loss=989448869.5544288\n",
      "lambda=0.20000,degree=16, gamma=0.01000, Training Loss=2642961.726270834, Testing Loss=1243482.5945315626\n",
      "lambda=0.20000,degree=16, gamma=0.21000, Training Loss=204782088.24290782, Testing Loss=204168607.40674758\n",
      "lambda=0.20000,degree=16, gamma=0.41000, Training Loss=1932640464.5943308, Testing Loss=1931932187.6576636\n",
      "lambda=0.20000,degree=16, gamma=0.61000, Training Loss=4206528897.3773875, Testing Loss=4205820852.414915\n",
      "lambda=0.20000,degree=16, gamma=0.81000, Training Loss=4811055834.174243, Testing Loss=4810340955.105491\n",
      "lambda=0.00000,degree=1, gamma=0.01000, Training Loss=3173267.7971103615, Testing Loss=352113.24286714307\n",
      "lambda=0.00000,degree=1, gamma=0.21000, Training Loss=1042692.3578747441, Testing Loss=115487.79917431969\n",
      "lambda=0.00000,degree=1, gamma=0.41000, Training Loss=868304.4632987271, Testing Loss=100714.62153606296\n",
      "lambda=0.00000,degree=1, gamma=0.61000, Training Loss=877987.3120320763, Testing Loss=111433.60317655669\n",
      "lambda=0.00000,degree=1, gamma=0.81000, Training Loss=1100575.8222274522, Testing Loss=143916.22970326713\n",
      "lambda=0.00000,degree=2, gamma=0.01000, Training Loss=1410957.4485575436, Testing Loss=157519.98408454127\n",
      "lambda=0.00000,degree=2, gamma=0.21000, Training Loss=764815.4799206591, Testing Loss=80884.20233083787\n",
      "lambda=0.00000,degree=2, gamma=0.41000, Training Loss=1156333.8048531872, Testing Loss=135073.59309174074\n",
      "lambda=0.00000,degree=2, gamma=0.61000, Training Loss=657995.9568430417, Testing Loss=81513.72642769615\n",
      "lambda=0.00000,degree=2, gamma=0.81000, Training Loss=689122.4162735622, Testing Loss=105163.14239734378\n",
      "lambda=0.00000,degree=3, gamma=0.01000, Training Loss=2156414.454465332, Testing Loss=233694.24996791146\n",
      "lambda=0.00000,degree=3, gamma=0.21000, Training Loss=1331495.4530857618, Testing Loss=150753.8196546964\n",
      "lambda=0.00000,degree=3, gamma=0.41000, Training Loss=871622.7751931868, Testing Loss=101256.22495206223\n",
      "lambda=0.00000,degree=3, gamma=0.61000, Training Loss=883932.7484602054, Testing Loss=114758.92688262317\n",
      "lambda=0.00000,degree=3, gamma=0.81000, Training Loss=663728.7215870974, Testing Loss=100517.86683169485\n",
      "lambda=0.00000,degree=4, gamma=0.01000, Training Loss=1293286.2251639438, Testing Loss=142863.26448907994\n",
      "lambda=0.00000,degree=4, gamma=0.21000, Training Loss=675758.6404741484, Testing Loss=76859.83606998697\n",
      "lambda=0.00000,degree=4, gamma=0.41000, Training Loss=739106.0701402423, Testing Loss=85465.05775660234\n",
      "lambda=0.00000,degree=4, gamma=0.61000, Training Loss=644819.2324173389, Testing Loss=87062.38084417736\n",
      "lambda=0.00000,degree=4, gamma=0.81000, Training Loss=860749.5254546999, Testing Loss=118901.72164205369\n",
      "lambda=0.00000,degree=5, gamma=0.01000, Training Loss=912995.1810560211, Testing Loss=97023.82347415933\n",
      "lambda=0.00000,degree=5, gamma=0.21000, Training Loss=775700.0841945658, Testing Loss=86321.95465195511\n",
      "lambda=0.00000,degree=5, gamma=0.41000, Training Loss=872113.0728547071, Testing Loss=104113.83128267746\n",
      "lambda=0.00000,degree=5, gamma=0.61000, Training Loss=880937.2612242813, Testing Loss=111931.40239279316\n",
      "lambda=0.00000,degree=5, gamma=0.81000, Training Loss=635789.1340182038, Testing Loss=97759.1797618463\n",
      "lambda=0.00000,degree=6, gamma=0.01000, Training Loss=913653.1059730892, Testing Loss=95266.72886326739\n",
      "lambda=0.00000,degree=6, gamma=0.21000, Training Loss=863173.4555873837, Testing Loss=96417.0886660762\n",
      "lambda=0.00000,degree=6, gamma=0.41000, Training Loss=870737.525441468, Testing Loss=103562.19137420741\n",
      "lambda=0.00000,degree=6, gamma=0.61000, Training Loss=1232201.195069601, Testing Loss=152906.5580688741\n",
      "lambda=0.00000,degree=6, gamma=0.81000, Training Loss=975056.5560827163, Testing Loss=142725.3397427826\n",
      "lambda=0.00000,degree=7, gamma=0.01000, Training Loss=973957.54604034, Testing Loss=101613.26130536415\n",
      "lambda=0.00000,degree=7, gamma=0.21000, Training Loss=665842.3362135226, Testing Loss=71779.93619560149\n",
      "lambda=0.00000,degree=7, gamma=0.41000, Training Loss=657990.0321023281, Testing Loss=82204.28762367286\n",
      "lambda=0.00000,degree=7, gamma=0.61000, Training Loss=756491.4222864576, Testing Loss=101064.3324061663\n",
      "lambda=0.00000,degree=7, gamma=0.81000, Training Loss=634627.2766622899, Testing Loss=97117.08737337333\n",
      "lambda=0.00000,degree=8, gamma=0.01000, Training Loss=1707055.1097839475, Testing Loss=187261.23805502724\n",
      "lambda=0.00000,degree=8, gamma=0.21000, Training Loss=745434.5046430968, Testing Loss=86838.9631462642\n",
      "lambda=0.00000,degree=8, gamma=0.41000, Training Loss=859652.2001429001, Testing Loss=103781.80101181976\n",
      "lambda=0.00000,degree=8, gamma=0.61000, Training Loss=649744.592569944, Testing Loss=88488.16551493006\n",
      "lambda=0.00000,degree=8, gamma=0.81000, Training Loss=937961.2583540003, Testing Loss=138711.15659827887\n",
      "lambda=0.00000,degree=9, gamma=0.01000, Training Loss=1504699.6436954713, Testing Loss=166898.06331623407\n",
      "lambda=0.00000,degree=9, gamma=0.21000, Training Loss=747866.1222077622, Testing Loss=85036.38225851022\n",
      "lambda=0.00000,degree=9, gamma=0.41000, Training Loss=740438.1251397263, Testing Loss=93278.33377198414\n",
      "lambda=0.00000,degree=9, gamma=0.61000, Training Loss=885533.5931971674, Testing Loss=118410.92908504557\n",
      "lambda=0.00000,degree=9, gamma=0.81000, Training Loss=757442.3228347942, Testing Loss=120320.21896258093\n",
      "lambda=0.00000,degree=10, gamma=0.01000, Training Loss=1171406.2234390327, Testing Loss=130315.04243219378\n",
      "lambda=0.00000,degree=10, gamma=0.21000, Training Loss=750808.6330319233, Testing Loss=83816.61396862064\n",
      "lambda=0.00000,degree=10, gamma=0.41000, Training Loss=916369.0994855951, Testing Loss=110955.1039996077\n",
      "lambda=0.00000,degree=10, gamma=0.61000, Training Loss=872835.1337206304, Testing Loss=119988.00789101032\n",
      "lambda=0.00000,degree=10, gamma=0.81000, Training Loss=892635.684731757, Testing Loss=141592.88594971198\n",
      "lambda=0.00000,degree=11, gamma=0.01000, Training Loss=1366878.901708953, Testing Loss=146173.92858132147\n",
      "lambda=0.00000,degree=11, gamma=0.21000, Training Loss=855672.9615893745, Testing Loss=96107.61822736853\n",
      "lambda=0.00000,degree=11, gamma=0.41000, Training Loss=1111086.2315246875, Testing Loss=134203.64427906126\n",
      "lambda=0.00000,degree=11, gamma=0.61000, Training Loss=872478.1988956599, Testing Loss=120417.6365998055\n",
      "lambda=0.00000,degree=11, gamma=0.81000, Training Loss=891723.5846595938, Testing Loss=140502.4154017541\n",
      "lambda=0.00000,degree=12, gamma=0.01000, Training Loss=1402185.4350121722, Testing Loss=148096.18421609586\n",
      "lambda=0.00000,degree=12, gamma=0.21000, Training Loss=848572.5335252503, Testing Loss=94211.23012219097\n",
      "lambda=0.00000,degree=12, gamma=0.41000, Training Loss=831750.520841636, Testing Loss=101486.92436386948\n",
      "lambda=0.00000,degree=12, gamma=0.61000, Training Loss=842850.6184785268, Testing Loss=115772.40049637001\n",
      "lambda=0.00000,degree=12, gamma=0.81000, Training Loss=862427.0862389423, Testing Loss=135510.58976681498\n",
      "lambda=0.00000,degree=13, gamma=0.01000, Training Loss=1258493.2813929268, Testing Loss=141442.5184986714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=13, gamma=0.21000, Training Loss=838255.1089025338, Testing Loss=93318.26945084347\n",
      "lambda=0.00000,degree=13, gamma=0.41000, Training Loss=834067.3232317022, Testing Loss=101990.02761767106\n",
      "lambda=0.00000,degree=13, gamma=0.61000, Training Loss=851691.2566394066, Testing Loss=115678.31397015214\n",
      "lambda=0.00000,degree=13, gamma=0.81000, Training Loss=868364.7805339226, Testing Loss=136089.86651494555\n",
      "lambda=0.00000,degree=14, gamma=0.01000, Training Loss=1256349.5081165098, Testing Loss=141795.0398679439\n",
      "lambda=0.00000,degree=14, gamma=0.21000, Training Loss=1099824.474475602, Testing Loss=123888.40413484418\n",
      "lambda=0.00000,degree=14, gamma=0.41000, Training Loss=1100438.3331032132, Testing Loss=132889.73846222888\n",
      "lambda=0.00000,degree=14, gamma=0.61000, Training Loss=1112751.2424350996, Testing Loss=147187.68815245316\n",
      "lambda=0.00000,degree=14, gamma=0.81000, Training Loss=1132224.5101857139, Testing Loss=166903.69696180025\n",
      "lambda=0.00000,degree=15, gamma=0.01000, Training Loss=1536037.9675340648, Testing Loss=164286.73352815947\n",
      "lambda=0.00000,degree=15, gamma=0.21000, Training Loss=835884.7903467776, Testing Loss=95081.92534376323\n",
      "lambda=0.00000,degree=15, gamma=0.41000, Training Loss=857063.9224040683, Testing Loss=104136.95381664843\n",
      "lambda=0.00000,degree=15, gamma=0.61000, Training Loss=849561.8687943835, Testing Loss=116057.45653144174\n",
      "lambda=0.00000,degree=15, gamma=0.81000, Training Loss=916494.5733166, Testing Loss=141757.28766806642\n",
      "lambda=0.00000,degree=16, gamma=0.01000, Training Loss=1257217.3136606533, Testing Loss=140725.65277437534\n",
      "lambda=0.00000,degree=16, gamma=0.21000, Training Loss=1096942.5247930803, Testing Loss=124036.35994979694\n",
      "lambda=0.00000,degree=16, gamma=0.41000, Training Loss=1100391.4260997355, Testing Loss=132210.0009888995\n",
      "lambda=0.00000,degree=16, gamma=0.61000, Training Loss=1115368.495876378, Testing Loss=146483.44240060312\n",
      "lambda=0.00000,degree=16, gamma=0.81000, Training Loss=1133722.0011526386, Testing Loss=166990.80550229157\n",
      "lambda=0.10000,degree=1, gamma=0.01000, Training Loss=2949050.968064079, Testing Loss=594921.2458803654\n",
      "lambda=0.10000,degree=1, gamma=0.21000, Training Loss=63936360.644979924, Testing Loss=63166319.88835297\n",
      "lambda=0.10000,degree=1, gamma=0.41000, Training Loss=562536556.9625791, Testing Loss=561766514.9412555\n",
      "lambda=0.10000,degree=1, gamma=0.61000, Training Loss=1271183585.9138265, Testing Loss=1270413923.8142455\n",
      "lambda=0.10000,degree=1, gamma=0.81000, Training Loss=2233091886.7865343, Testing Loss=2232322121.072223\n",
      "lambda=0.10000,degree=2, gamma=0.01000, Training Loss=2553330.834261332, Testing Loss=684680.060360706\n",
      "lambda=0.10000,degree=2, gamma=0.21000, Training Loss=192856668.08046946, Testing Loss=192089226.7197651\n",
      "lambda=0.10000,degree=2, gamma=0.41000, Training Loss=269082355.6008824, Testing Loss=268324662.58811715\n",
      "lambda=0.10000,degree=2, gamma=0.61000, Training Loss=537848452.9286209, Testing Loss=537244880.9745075\n",
      "lambda=0.10000,degree=2, gamma=0.81000, Training Loss=672847192.9565746, Testing Loss=672240990.5709531\n",
      "lambda=0.10000,degree=3, gamma=0.01000, Training Loss=1531126.1056357485, Testing Loss=723894.3090399489\n",
      "lambda=0.10000,degree=3, gamma=0.21000, Training Loss=104341537.79395778, Testing Loss=103073335.32372113\n",
      "lambda=0.10000,degree=3, gamma=0.41000, Training Loss=357018500.39586484, Testing Loss=356419083.4767887\n",
      "lambda=0.10000,degree=3, gamma=0.61000, Training Loss=1346567980.7561903, Testing Loss=1345802816.6199336\n",
      "lambda=0.10000,degree=3, gamma=0.81000, Training Loss=680747180.2811909, Testing Loss=680038541.2634447\n",
      "lambda=0.10000,degree=4, gamma=0.01000, Training Loss=1934791.6913078486, Testing Loss=637867.0277326112\n",
      "lambda=0.10000,degree=4, gamma=0.21000, Training Loss=92375292.99054901, Testing Loss=91679323.29325269\n",
      "lambda=0.10000,degree=4, gamma=0.41000, Training Loss=248355193.88017195, Testing Loss=247687066.90365916\n",
      "lambda=0.10000,degree=4, gamma=0.61000, Training Loss=562466780.3968037, Testing Loss=561862102.7074775\n",
      "lambda=0.10000,degree=4, gamma=0.81000, Training Loss=580904549.8812509, Testing Loss=580156217.0758342\n",
      "lambda=0.10000,degree=5, gamma=0.01000, Training Loss=2927513.484086917, Testing Loss=698666.1055491336\n",
      "lambda=0.10000,degree=5, gamma=0.21000, Training Loss=91244765.74835964, Testing Loss=89888811.08364634\n",
      "lambda=0.10000,degree=5, gamma=0.41000, Training Loss=323495395.5099374, Testing Loss=322892458.7687029\n",
      "lambda=0.10000,degree=5, gamma=0.61000, Training Loss=1251167932.2428815, Testing Loss=1250403485.364204\n",
      "lambda=0.10000,degree=5, gamma=0.81000, Training Loss=629085481.9928243, Testing Loss=628421121.9694239\n",
      "lambda=0.10000,degree=6, gamma=0.01000, Training Loss=1505351.0267828878, Testing Loss=559669.8508636354\n",
      "lambda=0.10000,degree=6, gamma=0.21000, Training Loss=82340577.26145667, Testing Loss=81139093.85360883\n",
      "lambda=0.10000,degree=6, gamma=0.41000, Training Loss=272077892.38593256, Testing Loss=270797830.2146481\n",
      "lambda=0.10000,degree=6, gamma=0.61000, Training Loss=362994833.73237485, Testing Loss=362013875.3561138\n",
      "lambda=0.10000,degree=6, gamma=0.81000, Training Loss=758316746.5912344, Testing Loss=756957854.9248389\n",
      "lambda=0.10000,degree=7, gamma=0.01000, Training Loss=2241605.5536291944, Testing Loss=660604.1057984681\n",
      "lambda=0.10000,degree=7, gamma=0.21000, Training Loss=61952762.92070385, Testing Loss=61248117.24752254\n",
      "lambda=0.10000,degree=7, gamma=0.41000, Training Loss=175620718.55535394, Testing Loss=174930823.2762907\n",
      "lambda=0.10000,degree=7, gamma=0.61000, Training Loss=1205330750.5051422, Testing Loss=1204564564.6193728\n",
      "lambda=0.10000,degree=7, gamma=0.81000, Training Loss=1658554354.1804817, Testing Loss=1657786971.6113572\n",
      "lambda=0.10000,degree=8, gamma=0.01000, Training Loss=2588616.7904051836, Testing Loss=777871.3819783747\n",
      "lambda=0.10000,degree=8, gamma=0.21000, Training Loss=114660826.7704916, Testing Loss=114042663.1965732\n",
      "lambda=0.10000,degree=8, gamma=0.41000, Training Loss=249582885.14966822, Testing Loss=248813682.1944984\n",
      "lambda=0.10000,degree=8, gamma=0.61000, Training Loss=258494887.36092794, Testing Loss=257867765.58381018\n",
      "lambda=0.10000,degree=8, gamma=0.81000, Training Loss=646808315.3679153, Testing Loss=646181876.0887218\n",
      "lambda=0.10000,degree=9, gamma=0.01000, Training Loss=1887125.546083559, Testing Loss=766004.4460546222\n",
      "lambda=0.10000,degree=9, gamma=0.21000, Training Loss=62048420.7636978, Testing Loss=61284584.75431934\n",
      "lambda=0.10000,degree=9, gamma=0.41000, Training Loss=323582870.0685289, Testing Loss=322950458.71377295\n",
      "lambda=0.10000,degree=9, gamma=0.61000, Training Loss=478792307.90210754, Testing Loss=477445609.8529038\n",
      "lambda=0.10000,degree=9, gamma=0.81000, Training Loss=536916605.7628583, Testing Loss=536233163.1419603\n",
      "lambda=0.10000,degree=10, gamma=0.01000, Training Loss=1918896.261248557, Testing Loss=795267.7996179343\n",
      "lambda=0.10000,degree=10, gamma=0.21000, Training Loss=80487714.7075937, Testing Loss=79856021.40592352\n",
      "lambda=0.10000,degree=10, gamma=0.41000, Training Loss=184951520.01197138, Testing Loss=184322986.10380995\n",
      "lambda=0.10000,degree=10, gamma=0.61000, Training Loss=163292547.27687508, Testing Loss=162401023.2591837\n",
      "lambda=0.10000,degree=10, gamma=0.81000, Training Loss=289907169.0224824, Testing Loss=289306023.33828735\n",
      "lambda=0.10000,degree=11, gamma=0.01000, Training Loss=2103278.096087058, Testing Loss=787942.2872834585\n",
      "lambda=0.10000,degree=11, gamma=0.21000, Training Loss=185252203.09601092, Testing Loss=184484971.02947482\n",
      "lambda=0.10000,degree=11, gamma=0.41000, Training Loss=284057760.19727534, Testing Loss=282717606.41822827\n",
      "lambda=0.10000,degree=11, gamma=0.61000, Training Loss=247165512.5140208, Testing Loss=246548422.02210638\n",
      "lambda=0.10000,degree=11, gamma=0.81000, Training Loss=856008783.2514931, Testing Loss=855377978.4756508\n",
      "lambda=0.10000,degree=12, gamma=0.01000, Training Loss=2126386.3008073014, Testing Loss=806843.6540292457\n",
      "lambda=0.10000,degree=12, gamma=0.21000, Training Loss=101388437.23204736, Testing Loss=100781548.32232815\n",
      "lambda=0.10000,degree=12, gamma=0.41000, Training Loss=263940605.82062736, Testing Loss=262602455.07228243\n",
      "lambda=0.10000,degree=12, gamma=0.61000, Training Loss=230093405.96961203, Testing Loss=228944391.732758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.10000,degree=12, gamma=0.81000, Training Loss=484823088.94548786, Testing Loss=484137820.1192405\n",
      "lambda=0.10000,degree=13, gamma=0.01000, Training Loss=2194787.3647091957, Testing Loss=841054.232386107\n",
      "lambda=0.10000,degree=13, gamma=0.21000, Training Loss=97255763.81670053, Testing Loss=96579823.14574938\n",
      "lambda=0.10000,degree=13, gamma=0.41000, Training Loss=184061833.72552857, Testing Loss=183419708.88087955\n",
      "lambda=0.10000,degree=13, gamma=0.61000, Training Loss=333698602.4995538, Testing Loss=332450578.7867525\n",
      "lambda=0.10000,degree=13, gamma=0.81000, Training Loss=2187109279.232638, Testing Loss=2186343899.819791\n",
      "lambda=0.10000,degree=14, gamma=0.01000, Training Loss=1929734.2001327975, Testing Loss=812023.1308982414\n",
      "lambda=0.10000,degree=14, gamma=0.21000, Training Loss=172704429.53829345, Testing Loss=171935891.44545868\n",
      "lambda=0.10000,degree=14, gamma=0.41000, Training Loss=189613506.56887937, Testing Loss=188969903.8391512\n",
      "lambda=0.10000,degree=14, gamma=0.61000, Training Loss=425347363.99221677, Testing Loss=423988556.5510305\n",
      "lambda=0.10000,degree=14, gamma=0.81000, Training Loss=418111183.4394637, Testing Loss=416808744.21867496\n",
      "lambda=0.10000,degree=15, gamma=0.01000, Training Loss=1936449.1223043837, Testing Loss=818470.254123708\n",
      "lambda=0.10000,degree=15, gamma=0.21000, Training Loss=86274381.99883555, Testing Loss=85499143.68230078\n",
      "lambda=0.10000,degree=15, gamma=0.41000, Training Loss=394904554.06959075, Testing Loss=394136351.5461506\n",
      "lambda=0.10000,degree=15, gamma=0.61000, Training Loss=440912499.960933, Testing Loss=440254969.8979187\n",
      "lambda=0.10000,degree=15, gamma=0.81000, Training Loss=472535376.1827012, Testing Loss=471250673.1160564\n",
      "lambda=0.10000,degree=16, gamma=0.01000, Training Loss=1876799.4018604327, Testing Loss=837497.7775311843\n",
      "lambda=0.10000,degree=16, gamma=0.21000, Training Loss=54300067.108862184, Testing Loss=53613779.54085203\n",
      "lambda=0.10000,degree=16, gamma=0.41000, Training Loss=345857004.93612766, Testing Loss=345213068.4939955\n",
      "lambda=0.10000,degree=16, gamma=0.61000, Training Loss=230043605.0093926, Testing Loss=229296719.65251946\n",
      "lambda=0.10000,degree=16, gamma=0.81000, Training Loss=374420543.53830117, Testing Loss=373312315.1689984\n",
      "lambda=0.20000,degree=1, gamma=0.01000, Training Loss=2158301.636605977, Testing Loss=818372.4021058555\n",
      "lambda=0.20000,degree=1, gamma=0.21000, Training Loss=288143619.9112148, Testing Loss=287375097.56806654\n",
      "lambda=0.20000,degree=1, gamma=0.41000, Training Loss=1144330485.0761707, Testing Loss=1143560719.3618596\n",
      "lambda=0.20000,degree=1, gamma=0.61000, Training Loss=1985717309.7062476, Testing Loss=1984947751.2229958\n",
      "lambda=0.20000,degree=1, gamma=0.81000, Training Loss=1411958329.5742176, Testing Loss=1410585795.9034326\n",
      "lambda=0.20000,degree=2, gamma=0.01000, Training Loss=2276104.1475590593, Testing Loss=1021940.3241518845\n",
      "lambda=0.20000,degree=2, gamma=0.21000, Training Loss=172074598.66361085, Testing Loss=171305713.1743359\n",
      "lambda=0.20000,degree=2, gamma=0.41000, Training Loss=409034045.9229134, Testing Loss=407663895.4932668\n",
      "lambda=0.20000,degree=2, gamma=0.61000, Training Loss=2521732388.711621, Testing Loss=2520963002.920652\n",
      "lambda=0.20000,degree=2, gamma=0.81000, Training Loss=4483424278.142595, Testing Loss=4482654926.892801\n",
      "lambda=0.20000,degree=3, gamma=0.01000, Training Loss=2142693.532879373, Testing Loss=1006351.7102726535\n",
      "lambda=0.20000,degree=3, gamma=0.21000, Training Loss=184476868.35410324, Testing Loss=183842220.3522522\n",
      "lambda=0.20000,degree=3, gamma=0.41000, Training Loss=248033614.54414234, Testing Loss=247280552.7563018\n",
      "lambda=0.20000,degree=3, gamma=0.61000, Training Loss=2467235815.28211, Testing Loss=2466466640.4591727\n",
      "lambda=0.20000,degree=3, gamma=0.81000, Training Loss=774114066.2792671, Testing Loss=772798378.7809055\n",
      "lambda=0.20000,degree=4, gamma=0.01000, Training Loss=1847387.1768475538, Testing Loss=981993.9135799657\n",
      "lambda=0.20000,degree=4, gamma=0.21000, Training Loss=128561757.04640022, Testing Loss=127885526.89344116\n",
      "lambda=0.20000,degree=4, gamma=0.41000, Training Loss=256939256.8732635, Testing Loss=256253457.97231665\n",
      "lambda=0.20000,degree=4, gamma=0.61000, Training Loss=2443182185.2070885, Testing Loss=2442414699.024833\n",
      "lambda=0.20000,degree=4, gamma=0.81000, Training Loss=4276081152.640196, Testing Loss=4275313562.831217\n",
      "lambda=0.20000,degree=5, gamma=0.01000, Training Loss=2285297.0506035527, Testing Loss=1061182.032650722\n",
      "lambda=0.20000,degree=5, gamma=0.21000, Training Loss=170032613.25770396, Testing Loss=169407506.9381707\n",
      "lambda=0.20000,degree=5, gamma=0.41000, Training Loss=467115780.8407153, Testing Loss=466487639.32789934\n",
      "lambda=0.20000,degree=5, gamma=0.61000, Training Loss=574550454.4527384, Testing Loss=573195285.9666988\n",
      "lambda=0.20000,degree=5, gamma=0.81000, Training Loss=3516353922.6328726, Testing Loss=3515587450.3855796\n",
      "lambda=0.20000,degree=6, gamma=0.01000, Training Loss=2597891.1803618358, Testing Loss=1210521.9728770263\n",
      "lambda=0.20000,degree=6, gamma=0.21000, Training Loss=155280140.30861273, Testing Loss=154658225.78112906\n",
      "lambda=0.20000,degree=6, gamma=0.41000, Training Loss=284680542.3547714, Testing Loss=284040332.558423\n",
      "lambda=0.20000,degree=6, gamma=0.61000, Training Loss=585944208.7552513, Testing Loss=584584800.1401352\n",
      "lambda=0.20000,degree=6, gamma=0.81000, Training Loss=581194291.0045658, Testing Loss=580552766.0733385\n",
      "lambda=0.20000,degree=7, gamma=0.01000, Training Loss=2651873.1071271915, Testing Loss=1149260.8637218087\n",
      "lambda=0.20000,degree=7, gamma=0.21000, Training Loss=107971331.06284162, Testing Loss=106926773.89419162\n",
      "lambda=0.20000,degree=7, gamma=0.41000, Training Loss=396269327.1422993, Testing Loss=394912086.6210994\n",
      "lambda=0.20000,degree=7, gamma=0.61000, Training Loss=316860543.89313143, Testing Loss=316193435.5439397\n",
      "lambda=0.20000,degree=7, gamma=0.81000, Training Loss=489570049.69183946, Testing Loss=488839937.7656538\n",
      "lambda=0.20000,degree=8, gamma=0.01000, Training Loss=2201913.904685199, Testing Loss=1362427.4218394181\n",
      "lambda=0.20000,degree=8, gamma=0.21000, Training Loss=101819673.95479706, Testing Loss=100519413.24117199\n",
      "lambda=0.20000,degree=8, gamma=0.41000, Training Loss=1113209350.9144611, Testing Loss=1112443088.8743613\n",
      "lambda=0.20000,degree=8, gamma=0.61000, Training Loss=655650399.589675, Testing Loss=655002787.8765893\n",
      "lambda=0.20000,degree=8, gamma=0.81000, Training Loss=1074171460.9602213, Testing Loss=1073516335.1102911\n",
      "lambda=0.20000,degree=9, gamma=0.01000, Training Loss=2622470.738696636, Testing Loss=1093422.2272442495\n",
      "lambda=0.20000,degree=9, gamma=0.21000, Training Loss=141452473.46653003, Testing Loss=140807370.71890154\n",
      "lambda=0.20000,degree=9, gamma=0.41000, Training Loss=937431441.5981257, Testing Loss=936666275.7698346\n",
      "lambda=0.20000,degree=9, gamma=0.61000, Training Loss=386677170.4051263, Testing Loss=385738417.3049266\n",
      "lambda=0.20000,degree=9, gamma=0.81000, Training Loss=1295395665.8394217, Testing Loss=1294035773.6750393\n",
      "lambda=0.20000,degree=10, gamma=0.01000, Training Loss=2690189.974725726, Testing Loss=1374049.90331452\n",
      "lambda=0.20000,degree=10, gamma=0.21000, Training Loss=103533379.89147763, Testing Loss=102768070.21056442\n",
      "lambda=0.20000,degree=10, gamma=0.41000, Training Loss=260591864.02346563, Testing Loss=259823653.40197605\n",
      "lambda=0.20000,degree=10, gamma=0.61000, Training Loss=519519827.13631713, Testing Loss=518157723.1443982\n",
      "lambda=0.20000,degree=10, gamma=0.81000, Training Loss=506827951.35065585, Testing Loss=506097768.6769046\n",
      "lambda=0.20000,degree=11, gamma=0.01000, Training Loss=2702375.9955437775, Testing Loss=1377118.848857662\n",
      "lambda=0.20000,degree=11, gamma=0.21000, Training Loss=75000273.0239832, Testing Loss=74225387.45472297\n",
      "lambda=0.20000,degree=11, gamma=0.41000, Training Loss=449803155.49904037, Testing Loss=449163351.933621\n",
      "lambda=0.20000,degree=11, gamma=0.61000, Training Loss=525954294.08230585, Testing Loss=524603658.5442793\n",
      "lambda=0.20000,degree=11, gamma=0.81000, Training Loss=720147774.1262047, Testing Loss=719498113.8379362\n",
      "lambda=0.20000,degree=12, gamma=0.01000, Training Loss=2702106.8809952647, Testing Loss=1382983.6364298423\n",
      "lambda=0.20000,degree=12, gamma=0.21000, Training Loss=82397986.45365475, Testing Loss=81707360.5779005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.20000,degree=12, gamma=0.41000, Training Loss=201054421.05199108, Testing Loss=200298272.34394795\n",
      "lambda=0.20000,degree=12, gamma=0.61000, Training Loss=290904898.0129235, Testing Loss=290216510.7146291\n",
      "lambda=0.20000,degree=12, gamma=0.81000, Training Loss=3979005596.1681333, Testing Loss=3978238040.8987303\n",
      "lambda=0.20000,degree=13, gamma=0.01000, Training Loss=2607622.409452704, Testing Loss=1547386.0459904438\n",
      "lambda=0.20000,degree=13, gamma=0.21000, Training Loss=115789161.88359907, Testing Loss=115019490.48752263\n",
      "lambda=0.20000,degree=13, gamma=0.41000, Training Loss=280436883.39500594, Testing Loss=279129563.01406056\n",
      "lambda=0.20000,degree=13, gamma=0.61000, Training Loss=464424428.5411865, Testing Loss=463096663.19926786\n",
      "lambda=0.20000,degree=13, gamma=0.81000, Training Loss=3676781255.7423353, Testing Loss=3676014149.4410434\n",
      "lambda=0.20000,degree=14, gamma=0.01000, Training Loss=2499435.559632263, Testing Loss=1458080.1071589272\n",
      "lambda=0.20000,degree=14, gamma=0.21000, Training Loss=291234888.93518007, Testing Loss=290466463.6329761\n",
      "lambda=0.20000,degree=14, gamma=0.41000, Training Loss=290852514.0327696, Testing Loss=289536981.69708055\n",
      "lambda=0.20000,degree=14, gamma=0.61000, Training Loss=2567755202.717569, Testing Loss=2566987992.813541\n",
      "lambda=0.20000,degree=14, gamma=0.81000, Training Loss=3450202999.981652, Testing Loss=3449435970.7987585\n",
      "lambda=0.20000,degree=15, gamma=0.01000, Training Loss=2538574.5937922257, Testing Loss=1421765.9587728556\n",
      "lambda=0.20000,degree=15, gamma=0.21000, Training Loss=110528464.39062347, Testing Loss=109880104.3455241\n",
      "lambda=0.20000,degree=15, gamma=0.41000, Training Loss=244285750.03346536, Testing Loss=243650660.87548548\n",
      "lambda=0.20000,degree=15, gamma=0.61000, Training Loss=198205298.74758774, Testing Loss=197256956.54465994\n",
      "lambda=0.20000,degree=15, gamma=0.81000, Training Loss=2462769848.8745384, Testing Loss=2462001499.2332683\n",
      "lambda=0.20000,degree=16, gamma=0.01000, Training Loss=2526857.052630731, Testing Loss=1403864.5158236513\n",
      "lambda=0.20000,degree=16, gamma=0.21000, Training Loss=102414123.18470556, Testing Loss=101141157.16606863\n",
      "lambda=0.20000,degree=16, gamma=0.41000, Training Loss=145979654.42108697, Testing Loss=144731422.59613255\n",
      "lambda=0.20000,degree=16, gamma=0.61000, Training Loss=300319626.5891014, Testing Loss=299626362.06683236\n",
      "lambda=0.20000,degree=16, gamma=0.81000, Training Loss=758485743.7504842, Testing Loss=757722599.9393438\n",
      "lambda=0.00000,degree=1, gamma=0.01000, Training Loss=798791.4196942275, Testing Loss=91355.30073869275\n",
      "lambda=0.00000,degree=1, gamma=0.21000, Training Loss=628146.7954081306, Testing Loss=73144.55100758387\n",
      "lambda=0.00000,degree=1, gamma=0.41000, Training Loss=623749.7585129893, Testing Loss=76446.79493602607\n",
      "lambda=0.00000,degree=1, gamma=0.61000, Training Loss=628180.2773891182, Testing Loss=83945.99289399182\n",
      "lambda=0.00000,degree=1, gamma=0.81000, Training Loss=638576.6918074985, Testing Loss=94309.26730212703\n",
      "lambda=0.00000,degree=2, gamma=0.01000, Training Loss=1305377.2395386042, Testing Loss=150581.73040464893\n",
      "lambda=0.00000,degree=2, gamma=0.21000, Training Loss=513449.34717861476, Testing Loss=58685.72526738065\n",
      "lambda=0.00000,degree=2, gamma=0.41000, Training Loss=498506.4450629246, Testing Loss=62858.42857428587\n",
      "lambda=0.00000,degree=2, gamma=0.61000, Training Loss=497653.2709447753, Testing Loss=72342.28418799263\n",
      "lambda=0.00000,degree=2, gamma=0.81000, Training Loss=507470.68108208675, Testing Loss=85944.69414958092\n",
      "lambda=0.00000,degree=3, gamma=0.01000, Training Loss=1411470.0776093411, Testing Loss=158966.33800557742\n",
      "lambda=0.00000,degree=3, gamma=0.21000, Training Loss=526395.2439469569, Testing Loss=62139.12866694412\n",
      "lambda=0.00000,degree=3, gamma=0.41000, Training Loss=510915.46918120043, Testing Loss=65982.25021507494\n",
      "lambda=0.00000,degree=3, gamma=0.61000, Training Loss=511110.99581403245, Testing Loss=73092.54371405336\n",
      "lambda=0.00000,degree=3, gamma=0.81000, Training Loss=518334.65271899203, Testing Loss=84638.89711324191\n",
      "lambda=0.00000,degree=4, gamma=0.01000, Training Loss=710016.1894751299, Testing Loss=82762.13894097281\n",
      "lambda=0.00000,degree=4, gamma=0.21000, Training Loss=506323.64745002287, Testing Loss=59658.924334395204\n",
      "lambda=0.00000,degree=4, gamma=0.41000, Training Loss=500699.0356788845, Testing Loss=64105.03173566549\n",
      "lambda=0.00000,degree=4, gamma=0.61000, Training Loss=509184.4736836176, Testing Loss=72422.24250978576\n",
      "lambda=0.00000,degree=4, gamma=0.81000, Training Loss=523472.82991946954, Testing Loss=85521.94094744751\n",
      "lambda=0.00000,degree=5, gamma=0.01000, Training Loss=697831.9449791779, Testing Loss=76781.0910044936\n",
      "lambda=0.00000,degree=5, gamma=0.21000, Training Loss=484682.2752218492, Testing Loss=57041.56589665429\n",
      "lambda=0.00000,degree=5, gamma=0.41000, Training Loss=480889.4758510417, Testing Loss=63245.73222636457\n",
      "lambda=0.00000,degree=5, gamma=0.61000, Training Loss=488235.4127832205, Testing Loss=72860.85136092946\n",
      "lambda=0.00000,degree=5, gamma=0.81000, Training Loss=500605.77132904605, Testing Loss=85114.23469780207\n",
      "lambda=0.00000,degree=6, gamma=0.01000, Training Loss=846549.6187817618, Testing Loss=101766.83282585801\n",
      "lambda=0.00000,degree=6, gamma=0.21000, Training Loss=484396.03738275333, Testing Loss=59517.57644837907\n",
      "lambda=0.00000,degree=6, gamma=0.41000, Training Loss=484161.8863085505, Testing Loss=63886.10196068536\n",
      "lambda=0.00000,degree=6, gamma=0.61000, Training Loss=491250.5115196829, Testing Loss=72709.65668517076\n",
      "lambda=0.00000,degree=6, gamma=0.81000, Training Loss=504254.8438327191, Testing Loss=85428.47817061041\n",
      "lambda=0.00000,degree=7, gamma=0.01000, Training Loss=677090.8064094489, Testing Loss=76564.59800557079\n",
      "lambda=0.00000,degree=7, gamma=0.21000, Training Loss=635361.8399721765, Testing Loss=72962.90888242863\n",
      "lambda=0.00000,degree=7, gamma=0.41000, Training Loss=638182.6884742802, Testing Loss=78777.87012513149\n",
      "lambda=0.00000,degree=7, gamma=0.61000, Training Loss=645222.5897148511, Testing Loss=88101.95496751703\n",
      "lambda=0.00000,degree=7, gamma=0.81000, Training Loss=658296.3869527188, Testing Loss=101135.68204992762\n",
      "lambda=0.00000,degree=8, gamma=0.01000, Training Loss=915897.9859298595, Testing Loss=112628.24908035868\n",
      "lambda=0.00000,degree=8, gamma=0.21000, Training Loss=480126.4870735246, Testing Loss=58861.44194254248\n",
      "lambda=0.00000,degree=8, gamma=0.41000, Training Loss=479635.3331443024, Testing Loss=63539.838980315064\n",
      "lambda=0.00000,degree=8, gamma=0.61000, Training Loss=487620.77927966765, Testing Loss=71423.34381518845\n",
      "lambda=0.00000,degree=8, gamma=0.81000, Training Loss=497299.7375598704, Testing Loss=85260.02338831761\n",
      "lambda=0.00000,degree=9, gamma=0.01000, Training Loss=884040.4327036202, Testing Loss=107100.18383471952\n",
      "lambda=0.00000,degree=9, gamma=0.21000, Training Loss=477348.5500011909, Testing Loss=56648.263853503966\n",
      "lambda=0.00000,degree=9, gamma=0.41000, Training Loss=473763.5210016367, Testing Loss=62635.361144027374\n",
      "lambda=0.00000,degree=9, gamma=0.61000, Training Loss=481172.2920916882, Testing Loss=71847.82308009447\n",
      "lambda=0.00000,degree=9, gamma=0.81000, Training Loss=495189.6834112036, Testing Loss=85745.77792027418\n",
      "lambda=0.00000,degree=10, gamma=0.01000, Training Loss=631069.1256272286, Testing Loss=72458.7564074034\n",
      "lambda=0.00000,degree=10, gamma=0.21000, Training Loss=670236.2514114749, Testing Loss=79320.39465148907\n",
      "lambda=0.00000,degree=10, gamma=0.41000, Training Loss=489460.9125266252, Testing Loss=62142.27813822955\n",
      "lambda=0.00000,degree=10, gamma=0.61000, Training Loss=496309.85588839534, Testing Loss=71597.41677324589\n",
      "lambda=0.00000,degree=10, gamma=0.81000, Training Loss=509283.3496330567, Testing Loss=85071.93258875856\n",
      "lambda=0.00000,degree=11, gamma=0.01000, Training Loss=677726.073033444, Testing Loss=75483.44208566801\n",
      "lambda=0.00000,degree=11, gamma=0.21000, Training Loss=606294.1718199663, Testing Loss=69978.33907499503\n",
      "lambda=0.00000,degree=11, gamma=0.41000, Training Loss=612384.3099132469, Testing Loss=75834.16174137518\n",
      "lambda=0.00000,degree=11, gamma=0.61000, Training Loss=621037.3918702903, Testing Loss=85732.41330163351\n",
      "lambda=0.00000,degree=11, gamma=0.81000, Training Loss=634470.15607153, Testing Loss=99844.27009773455\n"
     ]
    }
   ],
   "source": [
    "#separate data with respect to column 24 and remove None\n",
    "split_x_test, _, split_ids_test =  separate(y_donotUse, tX_test, ids_test)\n",
    "\n",
    "\n",
    "split_x_cleaned_test = removeNone(split_x_test, dataStatistics(split_x_test))\n",
    "\n",
    "#median instead of None\n",
    "split_x_with_median = putMedianInsteadOfNone(split_x_cleaned_test)\n",
    "\n",
    "split_x_with_median_with_momentum = add_momentum_vector(split_x_with_median)\n",
    "\n",
    "#line dropped when None\n",
    "#split_x_drop_lines, split_y_dropped_split_indexes_dropped = dropLineIfNone(split_x_cleaned_test, _, split_ids_test)\n",
    "\n",
    "#degrees for polynomial feature expension\n",
    "degrees = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "\n",
    "y_res = []\n",
    "\n",
    "acc = []\n",
    "\n",
    "plot_data_per_jetnum = []\n",
    "\n",
    "\n",
    "for i in range(len(cleaned_with_median)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #training: chose either cross calidation or cross validation for logistic regression with regularization\n",
    "    #w_star, d, accuracy, training_set, plot_data = crossValidation(cleaned_with_median[i], split_y[i], 0.98, degrees ,6)\n",
    "    w_star, d, accuracy, training_set, plot_data = crossValidationForLogistic_reg_with_loss(cleaned_with_median[i], split_y[i], 0.9, degrees ,6)\n",
    "    \n",
    "    \n",
    "    #polynomial feature expension and normalization using the training data\n",
    "    mean = np.mean(build_poly(training_set,d), axis = 0)\n",
    "    std = np.std(build_poly(training_set,d), axis = 0)\n",
    "    \n",
    "      \n",
    "    #put 1 if std = 0\n",
    "    std = std + (std == 0)\n",
    "    \n",
    "    extended_and_normalized = (build_poly(split_x_with_median[i], d) - mean) / std\n",
    "    \n",
    "    #adding bias term\n",
    "    bias = np.ones(shape=split_x_with_median[i].shape)          \n",
    "    x_test_ready = np.c_[bias, extended_and_normalized]\n",
    "    \n",
    "    #prediction for least squares\n",
    "    #y_res.append(predict_labels(w_star, x_test_ready))\n",
    "    \n",
    "    #prediction for logistic\n",
    "    y_res.append(predict_labels_logistic(w_star, x_test_ready))\n",
    "    \n",
    "\n",
    "\n",
    "    acc.append(accuracy)\n",
    "    plot_data_per_jetnum.append(plot_data)\n",
    "\n",
    "print(\"Accuracy per jet nbr: \\n\")\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stem() got an unexpected keyword argument 'use_line_collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-18701bf9b52e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.86\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_line_collection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cross_with_momentum_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stem() got an unexpected keyword argument 'use_line_collection'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "for i in range(len(plot_data_per_jetnum)):\n",
    "    jet_num = plot_data_per_jetnum[i]\n",
    "\n",
    "\n",
    "\n",
    "    X = np.array([x[0] for x in jet_num])\n",
    "    Y = np.array([x[1] for x in jet_num])\n",
    "    Z = np.array([x[3] for x in jet_num])\n",
    "    \n",
    "\n",
    "    plt.figure(dpi=120)\n",
    "    plt.title('Degree against accuracy for jet_num = %d' %i)\n",
    "    plt.xlabel('Degree of polynomial extension')\n",
    "    plt.ylabel('Testing Accuracy')\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.7,0.86])\n",
    "\n",
    "    plt.stem(Y, Z, use_line_collection=True, label=\"hello\")\n",
    "    plt.savefig(\"cross_with_momentum_\"+str(i))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "#reassemble the data for the submission\n",
    "y_pred = put_together(y_res, split_ids_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
