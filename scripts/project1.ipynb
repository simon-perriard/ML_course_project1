{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from validation_helpers import *\n",
    "from plots import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_x, split_y, split_ids = separate(y, tX, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.26145747 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 1.         1.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09751883 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05859584 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.0666396 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "selection = dataStatistics(split_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = removeNone(split_x, selection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can either drop the lines with residual Nones or replace the Nones by the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_with_median = putMedianInsteadOfNone(cleaned)\n",
    "\n",
    "cleaned_with_median_with_momentum = add_momentum_vector(cleaned_with_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropped_x, dropped_y, dropped_ids = dropLineIfNone(cleaned, split_y, split_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, the first values in each of the split data has a PRI_jet_num = 0, then 1 and so on. The data is clean and we can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_donotUse, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.2605448 0.        0.        0.        1.        1.        1.\n",
      " 0.        0.        0.        0.        0.        1.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.        1.        1.        1.        1.\n",
      " 1.        0.       ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09834149 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05881481 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.06376737 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "lambda=0.00000,degree=1, gamma=0.01000, Training Loss=1904199.4432619207, Testing Loss=215309.98537312786\n",
      "lambda=0.00000,degree=1, gamma=0.21000, Training Loss=722714.1066270418, Testing Loss=80196.00737717185\n",
      "lambda=0.00000,degree=1, gamma=0.41000, Training Loss=701606.7030461546, Testing Loss=79650.08705527912\n",
      "lambda=0.00000,degree=1, gamma=0.61000, Training Loss=699114.7846740682, Testing Loss=83379.715938599\n",
      "lambda=0.00000,degree=1, gamma=0.81000, Training Loss=698333.0199053542, Testing Loss=88407.77551246193\n",
      "lambda=0.00000,degree=2, gamma=0.01000, Training Loss=2007487.5873629923, Testing Loss=219857.32272232237\n",
      "lambda=0.00000,degree=2, gamma=0.21000, Training Loss=773003.4704891983, Testing Loss=87440.48820664233\n",
      "lambda=0.00000,degree=2, gamma=0.41000, Training Loss=755366.6068340385, Testing Loss=86382.1609106463\n",
      "lambda=0.00000,degree=2, gamma=0.61000, Training Loss=756957.3173053272, Testing Loss=91389.12067233436\n",
      "lambda=0.00000,degree=2, gamma=0.81000, Training Loss=758546.6254284466, Testing Loss=99066.5195696499\n",
      "lambda=0.00000,degree=3, gamma=0.01000, Training Loss=2018391.5204002564, Testing Loss=210703.89047253487\n",
      "lambda=0.00000,degree=3, gamma=0.21000, Training Loss=702781.72628836, Testing Loss=78571.24866408648\n",
      "lambda=0.00000,degree=3, gamma=0.41000, Training Loss=677469.2537350872, Testing Loss=76657.45574717755\n",
      "lambda=0.00000,degree=3, gamma=0.61000, Training Loss=673054.5688382593, Testing Loss=81241.58481844628\n",
      "lambda=0.00000,degree=3, gamma=0.81000, Training Loss=681250.5729891193, Testing Loss=88750.90253047369\n",
      "lambda=0.00000,degree=4, gamma=0.01000, Training Loss=1917766.1269402388, Testing Loss=208321.9680685167\n",
      "lambda=0.00000,degree=4, gamma=0.21000, Training Loss=699087.5007307346, Testing Loss=76966.74835884734\n",
      "lambda=0.00000,degree=4, gamma=0.41000, Training Loss=672360.9437179428, Testing Loss=77116.44749197041\n",
      "lambda=0.00000,degree=4, gamma=0.61000, Training Loss=668487.8634449379, Testing Loss=81686.89656455998\n",
      "lambda=0.00000,degree=4, gamma=0.81000, Training Loss=676100.8317958347, Testing Loss=87510.09296447883\n",
      "lambda=0.00000,degree=5, gamma=0.01000, Training Loss=1935930.7367795268, Testing Loss=218334.76245031809\n",
      "lambda=0.00000,degree=5, gamma=0.21000, Training Loss=754113.2550240586, Testing Loss=82557.48323756452\n",
      "lambda=0.00000,degree=5, gamma=0.41000, Training Loss=726826.5052908567, Testing Loss=82337.56920539503\n",
      "lambda=0.00000,degree=5, gamma=0.61000, Training Loss=729777.5024449789, Testing Loss=87256.84026262065\n",
      "lambda=0.00000,degree=5, gamma=0.81000, Training Loss=729138.072014428, Testing Loss=93628.290775673\n",
      "lambda=0.00000,degree=6, gamma=0.01000, Training Loss=1156098.8206650459, Testing Loss=124216.66815954776\n",
      "lambda=0.00000,degree=6, gamma=0.21000, Training Loss=710219.0054822675, Testing Loss=76365.91577245588\n",
      "lambda=0.00000,degree=6, gamma=0.41000, Training Loss=699723.0999621359, Testing Loss=80159.94136671523\n",
      "lambda=0.00000,degree=6, gamma=0.61000, Training Loss=700092.4651044047, Testing Loss=84413.61424679635\n",
      "lambda=0.00000,degree=6, gamma=0.81000, Training Loss=707457.7311530501, Testing Loss=92985.25730610167\n",
      "lambda=0.00000,degree=7, gamma=0.01000, Training Loss=794692.2314833227, Testing Loss=87113.37603503016\n",
      "lambda=0.00000,degree=7, gamma=0.21000, Training Loss=797183.1847083498, Testing Loss=90070.79238700874\n",
      "lambda=0.00000,degree=7, gamma=0.41000, Training Loss=805515.4422203718, Testing Loss=98403.04989903072\n",
      "lambda=0.00000,degree=7, gamma=0.61000, Training Loss=819201.9562924382, Testing Loss=112124.10354708954\n",
      "lambda=0.00000,degree=7, gamma=0.81000, Training Loss=838284.51688316, Testing Loss=131206.66413781128\n",
      "lambda=0.00000,degree=8, gamma=0.01000, Training Loss=2163519.356848529, Testing Loss=232124.5579982475\n",
      "lambda=0.00000,degree=8, gamma=0.21000, Training Loss=706116.2347278278, Testing Loss=79604.38420758469\n",
      "lambda=0.00000,degree=8, gamma=0.41000, Training Loss=683309.4040862507, Testing Loss=80675.61466391843\n",
      "lambda=0.00000,degree=8, gamma=0.61000, Training Loss=672605.2994448048, Testing Loss=82208.82599051438\n",
      "lambda=0.00000,degree=8, gamma=0.81000, Training Loss=671356.0795150767, Testing Loss=89391.46903935832\n",
      "lambda=0.00100,degree=1, gamma=0.01000, Training Loss=1906103.0710024948, Testing Loss=217420.33138197853\n",
      "lambda=0.00100,degree=1, gamma=0.21000, Training Loss=1673203.4969175318, Testing Loss=1037210.7550558202\n",
      "lambda=0.00100,degree=1, gamma=0.41000, Training Loss=4327790.790307096, Testing Loss=3713106.3193463353\n",
      "lambda=0.00100,degree=1, gamma=0.61000, Training Loss=8650831.844765656, Testing Loss=8054919.39008103\n",
      "lambda=0.00100,degree=1, gamma=0.81000, Training Loss=14704887.402255109, Testing Loss=14109934.100553624\n",
      "lambda=0.00100,degree=2, gamma=0.01000, Training Loss=2011934.2630066322, Testing Loss=222138.2768264658\n",
      "lambda=0.00100,degree=2, gamma=0.21000, Training Loss=1917653.4790605907, Testing Loss=1228097.478050651\n",
      "lambda=0.00100,degree=2, gamma=0.41000, Training Loss=5092063.18799286, Testing Loss=4422499.75982005\n",
      "lambda=0.00100,degree=2, gamma=0.61000, Training Loss=10310580.371404465, Testing Loss=9649240.828553576\n",
      "lambda=0.00100,degree=2, gamma=0.81000, Training Loss=17534459.044511102, Testing Loss=16880382.262209628\n",
      "lambda=0.00100,degree=3, gamma=0.01000, Training Loss=2014613.6763201326, Testing Loss=212671.17462993538\n",
      "lambda=0.00100,degree=3, gamma=0.21000, Training Loss=1855288.0697079774, Testing Loss=1224034.7060147282\n",
      "lambda=0.00100,degree=3, gamma=0.41000, Training Loss=5032571.920801473, Testing Loss=4434120.090177453\n",
      "lambda=0.00100,degree=3, gamma=0.61000, Training Loss=15681097.610442994, Testing Loss=15059211.415469918\n",
      "lambda=0.00100,degree=3, gamma=0.81000, Training Loss=27113550.598483052, Testing Loss=26492912.009312358\n",
      "lambda=0.00100,degree=4, gamma=0.01000, Training Loss=1919930.4908966413, Testing Loss=210845.47574290584\n",
      "lambda=0.00100,degree=4, gamma=0.21000, Training Loss=1834861.3024356249, Testing Loss=1205834.3152709808\n",
      "lambda=0.00100,degree=4, gamma=0.41000, Training Loss=4965414.466882595, Testing Loss=4368008.265186919\n",
      "lambda=0.00100,degree=4, gamma=0.61000, Training Loss=10125585.84218029, Testing Loss=9541239.580203816\n",
      "lambda=0.00100,degree=4, gamma=0.81000, Training Loss=17270558.902299717, Testing Loss=16693608.010472067\n",
      "lambda=0.00100,degree=5, gamma=0.01000, Training Loss=1938144.512583904, Testing Loss=220942.8085186683\n",
      "lambda=0.00100,degree=5, gamma=0.21000, Training Loss=1866709.5434184978, Testing Loss=1202105.7780030202\n",
      "lambda=0.00100,degree=5, gamma=0.41000, Training Loss=4946585.288155179, Testing Loss=4314252.042632846\n",
      "lambda=0.00100,degree=5, gamma=0.61000, Training Loss=10009389.87513795, Testing Loss=9385019.25297145\n",
      "lambda=0.00100,degree=5, gamma=0.81000, Training Loss=17033683.554683197, Testing Loss=16415279.011864303\n",
      "lambda=0.00100,degree=6, gamma=0.01000, Training Loss=1159582.4343396015, Testing Loss=127313.85871300122\n",
      "lambda=0.00100,degree=6, gamma=0.21000, Training Loss=2049978.751510376, Testing Loss=1419212.865576276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00100,degree=6, gamma=0.41000, Training Loss=5791345.563441181, Testing Loss=5177091.023474201\n",
      "lambda=0.00100,degree=6, gamma=0.61000, Training Loss=11940502.521635419, Testing Loss=11332272.625423754\n",
      "lambda=0.00100,degree=6, gamma=0.81000, Training Loss=20469319.622462776, Testing Loss=19859382.452906035\n",
      "lambda=0.00100,degree=7, gamma=0.01000, Training Loss=801359.2957701853, Testing Loss=93702.7331193503\n",
      "lambda=0.00100,degree=7, gamma=0.21000, Training Loss=3740133.3005526112, Testing Loss=3033020.9066320756\n",
      "lambda=0.00100,degree=7, gamma=0.41000, Training Loss=11928502.018499773, Testing Loss=11221078.768395307\n",
      "lambda=0.00100,degree=7, gamma=0.61000, Training Loss=25075726.29760448, Testing Loss=24368199.427972436\n",
      "lambda=0.00100,degree=7, gamma=0.81000, Training Loss=43373961.31401699, Testing Loss=42666434.44438495\n",
      "lambda=0.00100,degree=8, gamma=0.01000, Training Loss=2164396.054223992, Testing Loss=235363.32356038812\n",
      "lambda=0.00100,degree=8, gamma=0.21000, Training Loss=1861612.2292586593, Testing Loss=1226854.7076737327\n",
      "lambda=0.00100,degree=8, gamma=0.41000, Training Loss=5039132.87360467, Testing Loss=4436504.924352828\n",
      "lambda=0.00100,degree=8, gamma=0.61000, Training Loss=10275032.977185568, Testing Loss=9683596.237772832\n",
      "lambda=0.00100,degree=8, gamma=0.81000, Training Loss=17526412.97880534, Testing Loss=16933309.479396623\n",
      "lambda=0.00200,degree=1, gamma=0.01000, Training Loss=1908220.9032914867, Testing Loss=219555.64429038353\n",
      "lambda=0.00200,degree=1, gamma=0.21000, Training Loss=2621903.6690382436, Testing Loss=1983172.4412193217\n",
      "lambda=0.00200,degree=1, gamma=0.41000, Training Loss=7873501.142866409, Testing Loss=7263853.672762219\n",
      "lambda=0.00200,degree=1, gamma=0.61000, Training Loss=16551817.117087137, Testing Loss=15952639.184602402\n",
      "lambda=0.00200,degree=1, gamma=0.81000, Training Loss=28681079.756179262, Testing Loss=28083118.95862911\n",
      "lambda=0.00200,degree=2, gamma=0.01000, Training Loss=2016987.1846352103, Testing Loss=225161.4209752911\n",
      "lambda=0.00200,degree=2, gamma=0.21000, Training Loss=3057989.0845333952, Testing Loss=2360149.182301212\n",
      "lambda=0.00200,degree=2, gamma=0.41000, Training Loss=9353728.147867184, Testing Loss=8686857.56897557\n",
      "lambda=0.00200,degree=2, gamma=0.61000, Training Loss=19623435.58537022, Testing Loss=18964734.81040104\n",
      "lambda=0.00200,degree=2, gamma=0.81000, Training Loss=49299210.651609756, Testing Loss=48627932.634260185\n",
      "lambda=0.00200,degree=3, gamma=0.01000, Training Loss=2019951.5843129302, Testing Loss=215423.0564804186\n",
      "lambda=0.00200,degree=3, gamma=0.21000, Training Loss=2990400.1247391296, Testing Loss=2362745.695136844\n",
      "lambda=0.00200,degree=3, gamma=0.41000, Training Loss=14225116.629855651, Testing Loss=13596177.159450784\n",
      "lambda=0.00200,degree=3, gamma=0.61000, Training Loss=30603590.653655015, Testing Loss=29983532.93793741\n",
      "lambda=0.00200,degree=3, gamma=0.81000, Training Loss=44059280.34176135, Testing Loss=43498462.11982392\n",
      "lambda=0.00200,degree=4, gamma=0.01000, Training Loss=1924089.2031581705, Testing Loss=213363.80564300047\n",
      "lambda=0.00200,degree=4, gamma=0.21000, Training Loss=2947000.4600391057, Testing Loss=2327029.1273890394\n",
      "lambda=0.00200,degree=4, gamma=0.41000, Training Loss=9166961.297245407, Testing Loss=8582506.549138242\n",
      "lambda=0.00200,degree=4, gamma=0.61000, Training Loss=29693532.114428554, Testing Loss=29066472.044111352\n",
      "lambda=0.00200,degree=4, gamma=0.81000, Training Loss=51721693.926650904, Testing Loss=51098891.58749722\n",
      "lambda=0.00200,degree=5, gamma=0.01000, Training Loss=1939259.6650187173, Testing Loss=223036.8099963442\n",
      "lambda=0.00200,degree=5, gamma=0.21000, Training Loss=2962713.8982754885, Testing Loss=2300593.661475759\n",
      "lambda=0.00200,degree=5, gamma=0.41000, Training Loss=9074836.260270352, Testing Loss=8443726.593602464\n",
      "lambda=0.00200,degree=5, gamma=0.61000, Training Loss=19014860.116516184, Testing Loss=18401326.03438947\n",
      "lambda=0.00200,degree=5, gamma=0.81000, Training Loss=32687858.770103306, Testing Loss=32083892.873457033\n",
      "lambda=0.00200,degree=6, gamma=0.01000, Training Loss=1161711.2789993028, Testing Loss=130409.50083322485\n",
      "lambda=0.00200,degree=6, gamma=0.21000, Training Loss=3381707.830442853, Testing Loss=2752205.342856722\n",
      "lambda=0.00200,degree=6, gamma=0.41000, Training Loss=10813620.313822001, Testing Loss=10202163.177132517\n",
      "lambda=0.00200,degree=6, gamma=0.61000, Training Loss=22951976.479471933, Testing Loss=22342331.78285355\n",
      "lambda=0.00200,degree=6, gamma=0.81000, Training Loss=39674495.96960624, Testing Loss=39068416.26425923\n",
      "lambda=0.00200,degree=7, gamma=0.01000, Training Loss=808027.3823950676, Testing Loss=100299.73665466839\n",
      "lambda=0.00200,degree=7, gamma=0.21000, Training Loss=6611946.331465157, Testing Loss=5904523.08136069\n",
      "lambda=0.00200,degree=7, gamma=0.41000, Training Loss=22566885.607494097, Testing Loss=21859358.737862054\n",
      "lambda=0.00200,degree=7, gamma=0.61000, Training Loss=49342495.47970959, Testing Loss=48635141.30875711\n",
      "lambda=0.00200,degree=7, gamma=0.81000, Training Loss=89114248.85036363, Testing Loss=88407101.91846628\n",
      "lambda=0.00200,degree=8, gamma=0.01000, Training Loss=2165467.9493879313, Testing Loss=238548.89022408455\n",
      "lambda=0.00200,degree=8, gamma=0.21000, Training Loss=2997913.7007145328, Testing Loss=2364901.9722314253\n",
      "lambda=0.00200,degree=8, gamma=0.41000, Training Loss=9317041.78298133, Testing Loss=8710151.680693721\n",
      "lambda=0.00200,degree=8, gamma=0.61000, Training Loss=19613249.7984067, Testing Loss=19019033.93096299\n",
      "lambda=0.00200,degree=8, gamma=0.81000, Training Loss=33819853.46537521, Testing Loss=33222784.147825748\n",
      "lambda=0.00000,degree=1, gamma=0.01000, Training Loss=1351977.848750956, Testing Loss=151792.77573096295\n",
      "lambda=0.00000,degree=1, gamma=0.21000, Training Loss=1398436.5713506518, Testing Loss=159262.04681134262\n",
      "lambda=0.00000,degree=1, gamma=0.41000, Training Loss=1456263.1029462635, Testing Loss=165748.3387408226\n",
      "lambda=0.00000,degree=1, gamma=0.61000, Training Loss=1497702.574211013, Testing Loss=174651.90211178502\n",
      "lambda=0.00000,degree=1, gamma=0.81000, Training Loss=1503367.3677034683, Testing Loss=179744.79965173695\n",
      "lambda=0.00000,degree=2, gamma=0.01000, Training Loss=874796.2535190639, Testing Loss=93956.51944108454\n",
      "lambda=0.00000,degree=2, gamma=0.21000, Training Loss=783039.1696953512, Testing Loss=85745.92304145309\n",
      "lambda=0.00000,degree=2, gamma=0.41000, Training Loss=759649.1797656781, Testing Loss=84314.6062912502\n",
      "lambda=0.00000,degree=2, gamma=0.61000, Training Loss=741125.719567487, Testing Loss=85249.55606729497\n",
      "lambda=0.00000,degree=2, gamma=0.81000, Training Loss=734269.2263593565, Testing Loss=91972.75741628869\n",
      "lambda=0.00000,degree=3, gamma=0.01000, Training Loss=2731719.062544251, Testing Loss=279757.4133625307\n",
      "lambda=0.00000,degree=3, gamma=0.21000, Training Loss=861928.4188173898, Testing Loss=96022.13253758907\n",
      "lambda=0.00000,degree=3, gamma=0.41000, Training Loss=859667.2575268911, Testing Loss=95798.81514921844\n",
      "lambda=0.00000,degree=3, gamma=0.61000, Training Loss=860512.4766529883, Testing Loss=100383.41310057548\n",
      "lambda=0.00000,degree=3, gamma=0.81000, Training Loss=869153.2250757292, Testing Loss=106538.2767090101\n",
      "lambda=0.00000,degree=4, gamma=0.01000, Training Loss=1618193.030187729, Testing Loss=168211.9258133129\n",
      "lambda=0.00000,degree=4, gamma=0.21000, Training Loss=724667.3012342979, Testing Loss=76351.64388454033\n",
      "lambda=0.00000,degree=4, gamma=0.41000, Training Loss=676671.7185916407, Testing Loss=73373.88271401789\n",
      "lambda=0.00000,degree=4, gamma=0.61000, Training Loss=662733.1401450449, Testing Loss=79967.35526889055\n",
      "lambda=0.00000,degree=4, gamma=0.81000, Training Loss=661355.7397226864, Testing Loss=79972.0781124007\n",
      "lambda=0.00000,degree=5, gamma=0.01000, Training Loss=1768869.1967444653, Testing Loss=207369.94536573233\n",
      "lambda=0.00000,degree=5, gamma=0.21000, Training Loss=700039.4305622826, Testing Loss=78110.53904055285\n",
      "lambda=0.00000,degree=5, gamma=0.41000, Training Loss=673334.5038615789, Testing Loss=77768.27150102204\n",
      "lambda=0.00000,degree=5, gamma=0.61000, Training Loss=655673.5560256789, Testing Loss=81181.97328457545\n",
      "lambda=0.00000,degree=5, gamma=0.81000, Training Loss=660431.8648183731, Testing Loss=87003.10145322082\n",
      "lambda=0.00000,degree=6, gamma=0.01000, Training Loss=1762165.9716048848, Testing Loss=202625.2469152441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=6, gamma=0.21000, Training Loss=694009.6188368468, Testing Loss=76741.4708814959\n",
      "lambda=0.00000,degree=6, gamma=0.41000, Training Loss=660067.7442140973, Testing Loss=76133.39645927867\n",
      "lambda=0.00000,degree=6, gamma=0.61000, Training Loss=661597.5244608548, Testing Loss=80723.86588981305\n",
      "lambda=0.00000,degree=6, gamma=0.81000, Training Loss=667191.2747729281, Testing Loss=88337.79457864654\n",
      "lambda=0.00000,degree=7, gamma=0.01000, Training Loss=1386608.285179403, Testing Loss=158199.79205230498\n",
      "lambda=0.00000,degree=7, gamma=0.21000, Training Loss=850477.0774367261, Testing Loss=97203.37524116134\n",
      "lambda=0.00000,degree=7, gamma=0.41000, Training Loss=841892.6289509726, Testing Loss=97009.13882412722\n",
      "lambda=0.00000,degree=7, gamma=0.61000, Training Loss=842091.9385570429, Testing Loss=98658.93809184164\n",
      "lambda=0.00000,degree=7, gamma=0.81000, Training Loss=844416.2828391505, Testing Loss=103168.8919446091\n",
      "lambda=0.00000,degree=8, gamma=0.01000, Training Loss=1390997.2123494935, Testing Loss=157804.56391382747\n",
      "lambda=0.00000,degree=8, gamma=0.21000, Training Loss=864584.2986750624, Testing Loss=94694.46392671396\n",
      "lambda=0.00000,degree=8, gamma=0.41000, Training Loss=737587.9223378956, Testing Loss=80607.44195385602\n",
      "lambda=0.00000,degree=8, gamma=0.61000, Training Loss=868591.1636773003, Testing Loss=98687.31968795047\n",
      "lambda=0.00000,degree=8, gamma=0.81000, Training Loss=879979.1202646019, Testing Loss=111111.41334030358\n",
      "lambda=0.00100,degree=1, gamma=0.01000, Training Loss=1819494.7457420465, Testing Loss=205342.6577217238\n",
      "lambda=0.00100,degree=1, gamma=0.21000, Training Loss=2207269.1245746072, Testing Loss=955881.162105014\n",
      "lambda=0.00100,degree=1, gamma=0.41000, Training Loss=3927007.51688731, Testing Loss=2621868.134757771\n",
      "lambda=0.00100,degree=1, gamma=0.61000, Training Loss=13379218.64750785, Testing Loss=12611076.216507167\n",
      "lambda=0.00100,degree=1, gamma=0.81000, Training Loss=8661816.642716797, Testing Loss=7954072.837030783\n",
      "lambda=0.00100,degree=2, gamma=0.01000, Training Loss=2073221.1998935407, Testing Loss=231386.53373179678\n",
      "lambda=0.00100,degree=2, gamma=0.21000, Training Loss=1651306.7457584594, Testing Loss=933091.2522983754\n",
      "lambda=0.00100,degree=2, gamma=0.41000, Training Loss=4141545.8242131155, Testing Loss=3419518.411478251\n",
      "lambda=0.00100,degree=2, gamma=0.61000, Training Loss=10439996.968428437, Testing Loss=9672471.435563263\n",
      "lambda=0.00100,degree=2, gamma=0.81000, Training Loss=17988951.94049325, Testing Loss=17379137.772992935\n",
      "lambda=0.00100,degree=3, gamma=0.01000, Training Loss=2773943.905369196, Testing Loss=281288.14676624874\n",
      "lambda=0.00100,degree=3, gamma=0.21000, Training Loss=1828947.3939227213, Testing Loss=1062246.2497192025\n",
      "lambda=0.00100,degree=3, gamma=0.41000, Training Loss=4810188.034738587, Testing Loss=4043871.9520213637\n",
      "lambda=0.00100,degree=3, gamma=0.61000, Training Loss=8453942.446553763, Testing Loss=7700845.257039705\n",
      "lambda=0.00100,degree=3, gamma=0.81000, Training Loss=14583745.843370965, Testing Loss=13827268.616278311\n",
      "lambda=0.00100,degree=4, gamma=0.01000, Training Loss=1619990.2490045, Testing Loss=170082.5573121937\n",
      "lambda=0.00100,degree=4, gamma=0.21000, Training Loss=1525068.7007735258, Testing Loss=896570.3747163636\n",
      "lambda=0.00100,degree=4, gamma=0.41000, Training Loss=3774305.913278916, Testing Loss=3178242.3984258645\n",
      "lambda=0.00100,degree=4, gamma=0.61000, Training Loss=7503157.019192895, Testing Loss=6920682.435511331\n",
      "lambda=0.00100,degree=4, gamma=0.81000, Training Loss=12668752.672494577, Testing Loss=12094537.471355952\n",
      "lambda=0.00100,degree=5, gamma=0.01000, Training Loss=1770944.6757683493, Testing Loss=209800.303957134\n",
      "lambda=0.00100,degree=5, gamma=0.21000, Training Loss=1715299.239869248, Testing Loss=1088805.524737604\n",
      "lambda=0.00100,degree=5, gamma=0.41000, Training Loss=4491208.503108012, Testing Loss=3902816.04354718\n",
      "lambda=0.00100,degree=5, gamma=0.61000, Training Loss=9102191.843485741, Testing Loss=8519923.630725283\n",
      "lambda=0.00100,degree=5, gamma=0.81000, Training Loss=15509816.066574505, Testing Loss=14928467.640925976\n",
      "lambda=0.00100,degree=6, gamma=0.01000, Training Loss=1764231.4913389443, Testing Loss=204378.11594308953\n",
      "lambda=0.00100,degree=6, gamma=0.21000, Training Loss=1786099.8737765602, Testing Loss=1172028.3363092227\n",
      "lambda=0.00100,degree=6, gamma=0.41000, Training Loss=4811859.217693716, Testing Loss=4219594.512863629\n",
      "lambda=0.00100,degree=6, gamma=0.61000, Training Loss=9833337.982077323, Testing Loss=9253365.675865578\n",
      "lambda=0.00100,degree=6, gamma=0.81000, Training Loss=16775000.317744493, Testing Loss=16201002.33677517\n",
      "lambda=0.00100,degree=7, gamma=0.01000, Training Loss=1388965.3093998716, Testing Loss=159082.24502620433\n",
      "lambda=0.00100,degree=7, gamma=0.21000, Training Loss=1551140.712192956, Testing Loss=806059.2251910376\n",
      "lambda=0.00100,degree=7, gamma=0.41000, Training Loss=3505917.19214407, Testing Loss=2778956.8283315124\n",
      "lambda=0.00100,degree=7, gamma=0.61000, Training Loss=6769070.582786792, Testing Loss=6038328.374011954\n",
      "lambda=0.00100,degree=7, gamma=0.81000, Training Loss=11171423.90329291, Testing Loss=10462652.92075516\n",
      "lambda=0.00100,degree=8, gamma=0.01000, Training Loss=1387568.353300227, Testing Loss=159397.79149338507\n",
      "lambda=0.00100,degree=8, gamma=0.21000, Training Loss=1637795.1133788945, Testing Loss=867787.630031954\n",
      "lambda=0.00100,degree=8, gamma=0.41000, Training Loss=7290692.581021594, Testing Loss=6523292.874131507\n",
      "lambda=0.00100,degree=8, gamma=0.61000, Training Loss=15736209.11557485, Testing Loss=14972867.45180578\n",
      "lambda=0.00100,degree=8, gamma=0.81000, Training Loss=23064622.439424116, Testing Loss=22297101.73198566\n",
      "lambda=0.00200,degree=1, gamma=0.01000, Training Loss=3382676.397607085, Testing Loss=357880.6498169631\n",
      "lambda=0.00200,degree=1, gamma=0.21000, Training Loss=2651188.862888557, Testing Loss=1355200.7687942632\n",
      "lambda=0.00200,degree=1, gamma=0.41000, Training Loss=5783375.546678783, Testing Loss=5037631.036831527\n",
      "lambda=0.00200,degree=1, gamma=0.61000, Training Loss=16361325.16406408, Testing Loss=15703177.211497135\n",
      "lambda=0.00200,degree=1, gamma=0.81000, Training Loss=26433090.673338108, Testing Loss=25744873.861140743\n",
      "lambda=0.00200,degree=2, gamma=0.01000, Training Loss=1369567.5972296451, Testing Loss=154410.6602315125\n",
      "lambda=0.00200,degree=2, gamma=0.21000, Training Loss=2634282.6174136493, Testing Loss=1890099.8473369232\n",
      "lambda=0.00200,degree=2, gamma=0.41000, Training Loss=9267833.95612067, Testing Loss=8641409.415239776\n",
      "lambda=0.00200,degree=2, gamma=0.61000, Training Loss=16138039.693086848, Testing Loss=15411988.486742051\n",
      "lambda=0.00200,degree=2, gamma=0.81000, Training Loss=54519621.58175193, Testing Loss=53754104.03618826\n",
      "lambda=0.00200,degree=3, gamma=0.01000, Training Loss=2848259.7815687708, Testing Loss=287464.3471814216\n",
      "lambda=0.00200,degree=3, gamma=0.21000, Training Loss=2817231.214776292, Testing Loss=2047157.5808734186\n",
      "lambda=0.00200,degree=3, gamma=0.41000, Training Loss=7907780.230263287, Testing Loss=7147454.761950004\n",
      "lambda=0.00200,degree=3, gamma=0.61000, Training Loss=13992029.178281358, Testing Loss=13311523.457288338\n",
      "lambda=0.00200,degree=3, gamma=0.81000, Training Loss=23226256.953239813, Testing Loss=22534595.845774192\n",
      "lambda=0.00200,degree=4, gamma=0.01000, Training Loss=1621201.4270843733, Testing Loss=171963.47093510063\n",
      "lambda=0.00200,degree=4, gamma=0.21000, Training Loss=2338738.404065741, Testing Loss=1707509.934961904\n",
      "lambda=0.00200,degree=4, gamma=0.41000, Training Loss=6820136.167058756, Testing Loss=6227137.427836224\n",
      "lambda=0.00200,degree=4, gamma=0.61000, Training Loss=14152021.83269241, Testing Loss=13577496.939424323\n",
      "lambda=0.00200,degree=4, gamma=0.81000, Training Loss=24290091.091760404, Testing Loss=23710871.011095848\n",
      "lambda=0.00200,degree=5, gamma=0.01000, Training Loss=1772208.45519285, Testing Loss=212291.22970625904\n",
      "lambda=0.00200,degree=5, gamma=0.21000, Training Loss=2714799.4998950902, Testing Loss=2087612.0982103064\n",
      "lambda=0.00200,degree=5, gamma=0.41000, Training Loss=8275031.62114149, Testing Loss=7681353.108264907\n",
      "lambda=0.00200,degree=5, gamma=0.61000, Training Loss=17421886.037417814, Testing Loss=16838750.474022444\n",
      "lambda=0.00200,degree=5, gamma=0.81000, Training Loss=30119179.196149044, Testing Loss=29548372.39045209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00200,degree=6, gamma=0.01000, Training Loss=1767397.9800006058, Testing Loss=207001.90102980504\n",
      "lambda=0.00200,degree=6, gamma=0.21000, Training Loss=2877345.5898449225, Testing Loss=2256680.5129934335\n",
      "lambda=0.00200,degree=6, gamma=0.41000, Training Loss=8832799.717914235, Testing Loss=8245628.089219003\n",
      "lambda=0.00200,degree=6, gamma=0.61000, Training Loss=18302701.725287933, Testing Loss=17721034.210825574\n",
      "lambda=0.00200,degree=6, gamma=0.81000, Training Loss=30562371.77667205, Testing Loss=29986385.599416126\n",
      "lambda=0.00200,degree=7, gamma=0.01000, Training Loss=1387687.6524912321, Testing Loss=159341.0899412715\n",
      "lambda=0.00200,degree=7, gamma=0.21000, Training Loss=2254626.230100647, Testing Loss=1508231.218595647\n",
      "lambda=0.00200,degree=7, gamma=0.41000, Training Loss=6123584.226839415, Testing Loss=5408198.876651007\n",
      "lambda=0.00200,degree=7, gamma=0.61000, Training Loss=12416253.18920031, Testing Loss=11711782.105393786\n",
      "lambda=0.00200,degree=7, gamma=0.81000, Training Loss=21114379.255657192, Testing Loss=20416133.42065513\n",
      "lambda=0.00200,degree=8, gamma=0.01000, Training Loss=1392978.321652598, Testing Loss=161799.53989312748\n",
      "lambda=0.00200,degree=8, gamma=0.21000, Training Loss=4345861.368792251, Testing Loss=3578536.3867699737\n",
      "lambda=0.00200,degree=8, gamma=0.41000, Training Loss=6961727.109859516, Testing Loss=6284518.349865733\n",
      "lambda=0.00200,degree=8, gamma=0.61000, Training Loss=20105153.628312275, Testing Loss=19511520.52069968\n",
      "lambda=0.00200,degree=8, gamma=0.81000, Training Loss=25078383.683469765, Testing Loss=23888936.082685493\n",
      "lambda=0.00000,degree=1, gamma=0.01000, Training Loss=815656.8331679567, Testing Loss=94234.09034306342\n",
      "lambda=0.00000,degree=1, gamma=0.21000, Training Loss=644192.7821819079, Testing Loss=75729.97585734104\n",
      "lambda=0.00000,degree=1, gamma=0.41000, Training Loss=639412.8603680197, Testing Loss=75922.85471671121\n",
      "lambda=0.00000,degree=1, gamma=0.61000, Training Loss=640372.3414636877, Testing Loss=80088.20632115606\n",
      "lambda=0.00000,degree=1, gamma=0.81000, Training Loss=645964.0147320796, Testing Loss=85611.09806523366\n",
      "lambda=0.00000,degree=2, gamma=0.01000, Training Loss=1379334.5183462938, Testing Loss=157909.4860641448\n",
      "lambda=0.00000,degree=2, gamma=0.21000, Training Loss=600825.8787021429, Testing Loss=69235.62465187194\n",
      "lambda=0.00000,degree=2, gamma=0.41000, Training Loss=586272.522400959, Testing Loss=69779.28101822661\n",
      "lambda=0.00000,degree=2, gamma=0.61000, Training Loss=587044.819908425, Testing Loss=74771.57122016451\n",
      "lambda=0.00000,degree=2, gamma=0.81000, Training Loss=593298.2095124449, Testing Loss=81498.86894946307\n",
      "lambda=0.00000,degree=3, gamma=0.01000, Training Loss=1363031.990528474, Testing Loss=160467.41914794207\n",
      "lambda=0.00000,degree=3, gamma=0.21000, Training Loss=599507.420537944, Testing Loss=70495.04864546338\n",
      "lambda=0.00000,degree=3, gamma=0.41000, Training Loss=583089.240722957, Testing Loss=70336.27422627612\n",
      "lambda=0.00000,degree=3, gamma=0.61000, Training Loss=582909.0722616315, Testing Loss=73533.18990656668\n",
      "lambda=0.00000,degree=3, gamma=0.81000, Training Loss=585498.7626718425, Testing Loss=79874.78580090559\n",
      "lambda=0.00000,degree=4, gamma=0.01000, Training Loss=731534.5048704796, Testing Loss=80891.97716772808\n",
      "lambda=0.00000,degree=4, gamma=0.21000, Training Loss=628014.2172707793, Testing Loss=71808.60895306955\n",
      "lambda=0.00000,degree=4, gamma=0.41000, Training Loss=625672.918297655, Testing Loss=73963.6291933032\n",
      "lambda=0.00000,degree=4, gamma=0.61000, Training Loss=626889.9197535296, Testing Loss=77235.54907876704\n",
      "lambda=0.00000,degree=4, gamma=0.81000, Training Loss=631535.2131321232, Testing Loss=82143.17309256556\n",
      "lambda=0.00000,degree=5, gamma=0.01000, Training Loss=798743.8003667684, Testing Loss=97678.78496497215\n",
      "lambda=0.00000,degree=5, gamma=0.21000, Training Loss=621245.936044117, Testing Loss=71477.0577824192\n",
      "lambda=0.00000,degree=5, gamma=0.41000, Training Loss=619751.0511911114, Testing Loss=72651.39795516679\n",
      "lambda=0.00000,degree=5, gamma=0.61000, Training Loss=620232.7545301195, Testing Loss=75278.15052471813\n",
      "lambda=0.00000,degree=5, gamma=0.81000, Training Loss=625675.001135176, Testing Loss=80369.4222071807\n",
      "lambda=0.00000,degree=6, gamma=0.01000, Training Loss=816649.9222774091, Testing Loss=100715.13019411593\n",
      "lambda=0.00000,degree=6, gamma=0.21000, Training Loss=513321.43638538464, Testing Loss=60782.15221264602\n",
      "lambda=0.00000,degree=6, gamma=0.41000, Training Loss=512082.6057959083, Testing Loss=61993.178051350034\n",
      "lambda=0.00000,degree=6, gamma=0.61000, Training Loss=511241.22548674, Testing Loss=64958.83830544778\n",
      "lambda=0.00000,degree=6, gamma=0.81000, Training Loss=516041.71369985666, Testing Loss=69034.49589004868\n",
      "lambda=0.00000,degree=7, gamma=0.01000, Training Loss=704251.651880628, Testing Loss=80105.30603056453\n",
      "lambda=0.00000,degree=7, gamma=0.21000, Training Loss=632852.8129871293, Testing Loss=71689.34970400362\n",
      "lambda=0.00000,degree=7, gamma=0.41000, Training Loss=628598.0136793546, Testing Loss=72414.95729915013\n",
      "lambda=0.00000,degree=7, gamma=0.61000, Training Loss=631687.3715667613, Testing Loss=75247.15287544242\n",
      "lambda=0.00000,degree=7, gamma=0.81000, Training Loss=634377.4290782464, Testing Loss=79152.78185488013\n",
      "lambda=0.00000,degree=8, gamma=0.01000, Training Loss=701526.8430072372, Testing Loss=78813.09931668796\n",
      "lambda=0.00000,degree=8, gamma=0.21000, Training Loss=640349.3836255675, Testing Loss=73756.778963486\n",
      "lambda=0.00000,degree=8, gamma=0.41000, Training Loss=641924.5515186457, Testing Loss=74772.59613021211\n",
      "lambda=0.00000,degree=8, gamma=0.61000, Training Loss=642681.8416282473, Testing Loss=77808.49360520173\n",
      "lambda=0.00000,degree=8, gamma=0.81000, Training Loss=646528.5638119977, Testing Loss=81913.18449650565\n",
      "lambda=0.00100,degree=1, gamma=0.01000, Training Loss=817748.1375231711, Testing Loss=96378.78596001148\n",
      "lambda=0.00100,degree=1, gamma=0.21000, Training Loss=1591660.05737539, Testing Loss=1023477.5785652188\n",
      "lambda=0.00100,degree=1, gamma=0.41000, Training Loss=4243512.701696763, Testing Loss=3682154.4056432\n",
      "lambda=0.00100,degree=1, gamma=0.61000, Training Loss=8610798.425849829, Testing Loss=8050481.01466091\n",
      "lambda=0.00100,degree=1, gamma=0.81000, Training Loss=14685739.272197384, Testing Loss=14128162.183685103\n",
      "lambda=0.00100,degree=2, gamma=0.01000, Training Loss=1382044.8841884446, Testing Loss=160212.33821105506\n",
      "lambda=0.00100,degree=2, gamma=0.21000, Training Loss=1587945.3883291949, Testing Loss=1057161.8723220667\n",
      "lambda=0.00100,degree=2, gamma=0.41000, Training Loss=4345143.959993836, Testing Loss=3828001.6785157565\n",
      "lambda=0.00100,degree=2, gamma=0.61000, Training Loss=8879674.97681658, Testing Loss=8364932.777141956\n",
      "lambda=0.00100,degree=2, gamma=0.81000, Training Loss=15194427.948322259, Testing Loss=14681721.631179621\n",
      "lambda=0.00100,degree=3, gamma=0.01000, Training Loss=1365284.8589721671, Testing Loss=162652.359415456\n",
      "lambda=0.00100,degree=3, gamma=0.21000, Training Loss=1552002.5556386579, Testing Loss=1024082.7541379395\n",
      "lambda=0.00100,degree=3, gamma=0.41000, Training Loss=4212921.5498602195, Testing Loss=3699740.2324661496\n",
      "lambda=0.00100,degree=3, gamma=0.61000, Training Loss=8594305.258368457, Testing Loss=8085734.985123128\n",
      "lambda=0.00100,degree=3, gamma=0.81000, Training Loss=14678866.565745398, Testing Loss=14173575.398181716\n",
      "lambda=0.00100,degree=4, gamma=0.01000, Training Loss=733367.3401807564, Testing Loss=82715.92616930068\n",
      "lambda=0.00100,degree=4, gamma=0.21000, Training Loss=1429362.5370799552, Testing Loss=872242.7751094216\n",
      "lambda=0.00100,degree=4, gamma=0.41000, Training Loss=3668346.060854912, Testing Loss=3117298.714940891\n",
      "lambda=0.00100,degree=4, gamma=0.61000, Training Loss=7345912.458603534, Testing Loss=6795707.318758814\n",
      "lambda=0.00100,degree=4, gamma=0.81000, Training Loss=12447120.975683656, Testing Loss=11896544.961637978\n",
      "lambda=0.00100,degree=5, gamma=0.01000, Training Loss=800433.7249652194, Testing Loss=99373.20386049456\n",
      "lambda=0.00100,degree=5, gamma=0.21000, Training Loss=1394780.554514722, Testing Loss=842630.5971282038\n",
      "lambda=0.00100,degree=5, gamma=0.41000, Training Loss=3546395.385235735, Testing Loss=3001605.966019455\n",
      "lambda=0.00100,degree=5, gamma=0.61000, Training Loss=7084705.228800086, Testing Loss=6539746.3387131365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00100,degree=5, gamma=0.81000, Training Loss=11987465.991472825, Testing Loss=11446249.268698948\n",
      "lambda=0.00100,degree=6, gamma=0.01000, Training Loss=818199.6886127052, Testing Loss=102371.61793047684\n",
      "lambda=0.00100,degree=6, gamma=0.21000, Training Loss=1248148.6872603195, Testing Loss=793611.1109647176\n",
      "lambda=0.00100,degree=6, gamma=0.41000, Training Loss=3298550.5002353713, Testing Loss=2848877.9166710274\n",
      "lambda=0.00100,degree=6, gamma=0.61000, Training Loss=6657634.626302415, Testing Loss=6212310.951564091\n",
      "lambda=0.00100,degree=6, gamma=0.81000, Training Loss=11320639.804462887, Testing Loss=10877532.21564814\n",
      "lambda=0.00100,degree=7, gamma=0.01000, Training Loss=705903.301886114, Testing Loss=81503.3519023161\n",
      "lambda=0.00100,degree=7, gamma=0.21000, Training Loss=1245334.8328339132, Testing Loss=681700.963513697\n",
      "lambda=0.00100,degree=7, gamma=0.41000, Training Loss=2948157.838265796, Testing Loss=2389517.2957917363\n",
      "lambda=0.00100,degree=7, gamma=0.61000, Training Loss=5748420.7016214505, Testing Loss=5190625.285637449\n",
      "lambda=0.00100,degree=7, gamma=0.81000, Training Loss=9620511.60875565, Testing Loss=9063268.64630319\n",
      "lambda=0.00100,degree=8, gamma=0.01000, Training Loss=702900.0191886518, Testing Loss=80208.99612622104\n",
      "lambda=0.00100,degree=8, gamma=0.21000, Training Loss=1253809.4832671834, Testing Loss=688267.2498961217\n",
      "lambda=0.00100,degree=8, gamma=0.41000, Training Loss=2973157.3824508195, Testing Loss=2407127.219365773\n",
      "lambda=0.00100,degree=8, gamma=0.61000, Training Loss=5785811.855995003, Testing Loss=5218984.607969482\n",
      "lambda=0.00100,degree=8, gamma=0.81000, Training Loss=9686269.589039482, Testing Loss=9119342.562271845\n",
      "lambda=0.00200,degree=1, gamma=0.01000, Training Loss=819837.8660155217, Testing Loss=98523.11154717681\n",
      "lambda=0.00200,degree=1, gamma=0.21000, Training Loss=2536180.989825758, Testing Loss=1968064.5261955047\n",
      "lambda=0.00200,degree=1, gamma=0.41000, Training Loss=7834518.2277229, Testing Loss=7270457.87648259\n",
      "lambda=0.00200,degree=1, gamma=0.61000, Training Loss=16526724.118337844, Testing Loss=15962423.69690682\n",
      "lambda=0.00200,degree=1, gamma=0.81000, Training Loss=28581193.02272106, Testing Loss=28019571.208403207\n",
      "lambda=0.00200,degree=2, gamma=0.01000, Training Loss=1384740.958940531, Testing Loss=162513.4115725371\n",
      "lambda=0.00200,degree=2, gamma=0.21000, Training Loss=2570957.145572189, Testing Loss=2039070.8565262596\n",
      "lambda=0.00200,degree=2, gamma=0.41000, Training Loss=8071006.999403991, Testing Loss=7552416.414518904\n",
      "lambda=0.00200,degree=2, gamma=0.61000, Training Loss=17062827.536865972, Testing Loss=16551269.317345422\n",
      "lambda=0.00200,degree=2, gamma=0.81000, Training Loss=29495303.454610728, Testing Loss=28980714.342293315\n",
      "lambda=0.00200,degree=3, gamma=0.01000, Training Loss=1367572.3636594906, Testing Loss=164841.0399695594\n",
      "lambda=0.00200,degree=3, gamma=0.21000, Training Loss=2501808.2067710776, Testing Loss=1974192.9674265631\n",
      "lambda=0.00200,degree=3, gamma=0.41000, Training Loss=7806344.462830941, Testing Loss=7291093.632050063\n",
      "lambda=0.00200,degree=3, gamma=0.61000, Training Loss=16492831.03877144, Testing Loss=15983626.685568737\n",
      "lambda=0.00200,degree=3, gamma=0.81000, Training Loss=28504165.812852938, Testing Loss=27997430.835011322\n",
      "lambda=0.00200,degree=4, gamma=0.01000, Training Loss=735214.2935990755, Testing Loss=84541.20953545047\n",
      "lambda=0.00200,degree=4, gamma=0.21000, Training Loss=2224211.1960721146, Testing Loss=1668968.0783826103\n",
      "lambda=0.00200,degree=4, gamma=0.41000, Training Loss=6679407.157956755, Testing Loss=6127762.321307696\n",
      "lambda=0.00200,degree=4, gamma=0.61000, Training Loss=13952566.984960115, Testing Loss=13401272.654231803\n",
      "lambda=0.00200,degree=4, gamma=0.81000, Training Loss=24003227.925841328, Testing Loss=23452324.522222396\n",
      "lambda=0.00200,degree=5, gamma=0.01000, Training Loss=803534.1637855098, Testing Loss=100080.64716725158\n",
      "lambda=0.00200,degree=5, gamma=0.21000, Training Loss=2159115.817967051, Testing Loss=1608388.5597404568\n",
      "lambda=0.00200,degree=5, gamma=0.41000, Training Loss=6441508.734181885, Testing Loss=5896197.454610682\n",
      "lambda=0.00200,degree=5, gamma=0.61000, Training Loss=13441042.58774935, Testing Loss=12896943.439434307\n",
      "lambda=0.00200,degree=5, gamma=0.81000, Training Loss=23096060.146853287, Testing Loss=22553945.053986542\n",
      "lambda=0.00200,degree=6, gamma=0.01000, Training Loss=819745.8958213491, Testing Loss=104026.99346711027\n",
      "lambda=0.00200,degree=6, gamma=0.21000, Training Loss=1975720.511904553, Testing Loss=1521841.5308824573\n",
      "lambda=0.00200,degree=6, gamma=0.41000, Training Loss=6048435.357014623, Testing Loss=5600925.415527036\n",
      "lambda=0.00200,degree=6, gamma=0.61000, Training Loss=12691282.10712561, Testing Loss=12245359.476460999\n",
      "lambda=0.00200,degree=6, gamma=0.81000, Training Loss=21853548.72118769, Testing Loss=21409937.405103803\n",
      "lambda=0.00200,degree=7, gamma=0.01000, Training Loss=707268.0251417297, Testing Loss=82898.69755373069\n",
      "lambda=0.00200,degree=7, gamma=0.21000, Training Loss=1849600.8702050888, Testing Loss=1288401.6900532227\n",
      "lambda=0.00200,degree=7, gamma=0.41000, Training Loss=5234960.996280869, Testing Loss=4675753.213995028\n",
      "lambda=0.00200,degree=7, gamma=0.61000, Training Loss=12439832.432358589, Testing Loss=11991692.141211594\n",
      "lambda=0.00200,degree=7, gamma=0.81000, Training Loss=21413088.30815761, Testing Loss=20966185.238473363\n",
      "lambda=0.00200,degree=8, gamma=0.01000, Training Loss=704274.5343323087, Testing Loss=81603.79443476119\n",
      "lambda=0.00200,degree=8, gamma=0.21000, Training Loss=1864684.7202407331, Testing Loss=1296941.348682935\n",
      "lambda=0.00200,degree=8, gamma=0.41000, Training Loss=5273307.628759433, Testing Loss=4707120.094713706\n",
      "lambda=0.00200,degree=8, gamma=0.61000, Training Loss=10820069.73816511, Testing Loss=10251769.505737929\n",
      "lambda=0.00200,degree=8, gamma=0.81000, Training Loss=20930208.98907045, Testing Loss=20485896.79436075\n",
      "lambda=0.00000,degree=1, gamma=0.01000, Training Loss=1027941.2757390469, Testing Loss=99550.67784153129\n",
      "lambda=0.00000,degree=1, gamma=0.21000, Training Loss=218105.95212984394, Testing Loss=23795.538380801452\n",
      "lambda=0.00000,degree=1, gamma=0.41000, Training Loss=208139.17231647007, Testing Loss=21487.887688482515\n",
      "lambda=0.00000,degree=1, gamma=0.61000, Training Loss=207447.4938856084, Testing Loss=21906.339728284325\n",
      "lambda=0.00000,degree=1, gamma=0.81000, Training Loss=204265.26997008463, Testing Loss=22926.85364505947\n",
      "lambda=0.00000,degree=2, gamma=0.01000, Training Loss=378222.3667151056, Testing Loss=41265.808024464\n",
      "lambda=0.00000,degree=2, gamma=0.21000, Training Loss=208308.53863539657, Testing Loss=21726.20770796445\n",
      "lambda=0.00000,degree=2, gamma=0.41000, Training Loss=207958.23493048112, Testing Loss=22920.070557332674\n",
      "lambda=0.00000,degree=2, gamma=0.61000, Training Loss=209771.3607423207, Testing Loss=22746.43098318582\n",
      "lambda=0.00000,degree=2, gamma=0.81000, Training Loss=206411.6140684547, Testing Loss=22818.194825162813\n",
      "lambda=0.00000,degree=3, gamma=0.01000, Training Loss=584859.489873859, Testing Loss=59090.78305098519\n",
      "lambda=0.00000,degree=3, gamma=0.21000, Training Loss=209217.267506662, Testing Loss=21718.351087350547\n",
      "lambda=0.00000,degree=3, gamma=0.41000, Training Loss=365379.56247833, Testing Loss=40232.68073534691\n",
      "lambda=0.00000,degree=3, gamma=0.61000, Training Loss=211910.492423016, Testing Loss=23466.927612783562\n",
      "lambda=0.00000,degree=3, gamma=0.81000, Training Loss=199458.16529900715, Testing Loss=22229.45777514396\n",
      "lambda=0.00000,degree=4, gamma=0.01000, Training Loss=1486211.9731982888, Testing Loss=150070.71125735523\n",
      "lambda=0.00000,degree=4, gamma=0.21000, Training Loss=375988.3058827378, Testing Loss=41915.111960475224\n",
      "lambda=0.00000,degree=4, gamma=0.41000, Training Loss=196015.88548201576, Testing Loss=20838.24924377958\n",
      "lambda=0.00000,degree=4, gamma=0.61000, Training Loss=208248.83923083547, Testing Loss=22528.026824114666\n",
      "lambda=0.00000,degree=4, gamma=0.81000, Training Loss=209395.49309159847, Testing Loss=23927.501064050837\n",
      "lambda=0.00000,degree=5, gamma=0.01000, Training Loss=415606.4700969559, Testing Loss=44680.744447586105\n",
      "lambda=0.00000,degree=5, gamma=0.21000, Training Loss=218666.12272638953, Testing Loss=23211.68768768522\n",
      "lambda=0.00000,degree=5, gamma=0.41000, Training Loss=256770.26833453565, Testing Loss=28279.126335096105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=5, gamma=0.61000, Training Loss=255917.7866133711, Testing Loss=29505.799675521866\n",
      "lambda=0.00000,degree=5, gamma=0.81000, Training Loss=250001.79517402803, Testing Loss=29716.01659617128\n",
      "lambda=0.00000,degree=6, gamma=0.01000, Training Loss=792275.2230947935, Testing Loss=88040.93393363371\n",
      "lambda=0.00000,degree=6, gamma=0.21000, Training Loss=459177.82131484034, Testing Loss=52744.16730388376\n",
      "lambda=0.00000,degree=6, gamma=0.41000, Training Loss=210518.22916226654, Testing Loss=22455.450850050882\n",
      "lambda=0.00000,degree=6, gamma=0.61000, Training Loss=208930.25005891552, Testing Loss=22778.4854263879\n",
      "lambda=0.00000,degree=6, gamma=0.81000, Training Loss=202765.7979251206, Testing Loss=22580.269733828893\n",
      "lambda=0.00000,degree=7, gamma=0.01000, Training Loss=390345.29247438256, Testing Loss=42575.078310567646\n",
      "lambda=0.00000,degree=7, gamma=0.21000, Training Loss=336509.19251614576, Testing Loss=37071.036064968\n",
      "lambda=0.00000,degree=7, gamma=0.41000, Training Loss=459598.15946261864, Testing Loss=52939.7013538514\n",
      "lambda=0.00000,degree=7, gamma=0.61000, Training Loss=448030.84480292577, Testing Loss=52242.42633765456\n",
      "lambda=0.00000,degree=7, gamma=0.81000, Training Loss=321618.34238224686, Testing Loss=35777.68871181803\n",
      "lambda=0.00000,degree=8, gamma=0.01000, Training Loss=251521.0937537556, Testing Loss=25566.800349642526\n",
      "lambda=0.00000,degree=8, gamma=0.21000, Training Loss=321674.6998437967, Testing Loss=34932.52003111613\n",
      "lambda=0.00000,degree=8, gamma=0.41000, Training Loss=306578.67961898027, Testing Loss=34097.113854319934\n",
      "lambda=0.00000,degree=8, gamma=0.61000, Training Loss=303288.4064160905, Testing Loss=33891.0706262237\n",
      "lambda=0.00000,degree=8, gamma=0.81000, Training Loss=305470.47302412003, Testing Loss=34091.1633816774\n",
      "lambda=0.00100,degree=1, gamma=0.01000, Training Loss=1028076.4165437545, Testing Loss=99778.7531340775\n",
      "lambda=0.00100,degree=1, gamma=0.21000, Training Loss=325847.7701283882, Testing Loss=129773.53550355157\n",
      "lambda=0.00100,degree=1, gamma=0.41000, Training Loss=609671.1777810846, Testing Loss=423290.1045980638\n",
      "lambda=0.00100,degree=1, gamma=0.61000, Training Loss=1093921.5074095512, Testing Loss=909220.4037202505\n",
      "lambda=0.00100,degree=1, gamma=0.81000, Training Loss=1772963.5308399936, Testing Loss=1591465.2700538037\n",
      "lambda=0.00100,degree=2, gamma=0.01000, Training Loss=377846.9308323035, Testing Loss=41373.100818006584\n",
      "lambda=0.00100,degree=2, gamma=0.21000, Training Loss=313642.82652232196, Testing Loss=124457.20577850078\n",
      "lambda=0.00100,degree=2, gamma=0.41000, Training Loss=764749.819177411, Testing Loss=577380.1569223771\n",
      "lambda=0.00100,degree=2, gamma=0.61000, Training Loss=1992486.7372126598, Testing Loss=1803974.0948496377\n",
      "lambda=0.00100,degree=2, gamma=0.81000, Training Loss=1580069.7590986982, Testing Loss=1401172.0165010863\n",
      "lambda=0.00100,degree=3, gamma=0.01000, Training Loss=594117.7957955124, Testing Loss=59507.8547812198\n",
      "lambda=0.00100,degree=3, gamma=0.21000, Training Loss=420810.64616298507, Testing Loss=232154.75236575844\n",
      "lambda=0.00100,degree=3, gamma=0.41000, Training Loss=600688.8658061057, Testing Loss=361014.15567259747\n",
      "lambda=0.00100,degree=3, gamma=0.61000, Training Loss=991481.4065002233, Testing Loss=759323.315820614\n",
      "lambda=0.00100,degree=3, gamma=0.81000, Training Loss=1483049.836126417, Testing Loss=1208570.25994709\n",
      "lambda=0.00100,degree=4, gamma=0.01000, Training Loss=1426415.08613381, Testing Loss=145815.97151425705\n",
      "lambda=0.00100,degree=4, gamma=0.21000, Training Loss=416634.9638038323, Testing Loss=227541.8623943105\n",
      "lambda=0.00100,degree=4, gamma=0.41000, Training Loss=509715.95783603494, Testing Loss=316073.43168261513\n",
      "lambda=0.00100,degree=4, gamma=0.61000, Training Loss=1009321.0964463525, Testing Loss=824908.5336987447\n",
      "lambda=0.00100,degree=4, gamma=0.81000, Training Loss=1228955.1361275697, Testing Loss=990878.9838615645\n",
      "lambda=0.00100,degree=5, gamma=0.01000, Training Loss=415598.9551757225, Testing Loss=44846.355820125136\n",
      "lambda=0.00100,degree=5, gamma=0.21000, Training Loss=300636.6066685708, Testing Loss=107699.24574868404\n",
      "lambda=0.00100,degree=5, gamma=0.41000, Training Loss=655773.0287783863, Testing Loss=344298.2477503156\n",
      "lambda=0.00100,degree=5, gamma=0.61000, Training Loss=1037070.4428907346, Testing Loss=723851.6703409223\n",
      "lambda=0.00100,degree=5, gamma=0.81000, Training Loss=1572057.1228059377, Testing Loss=1261294.2561638248\n",
      "lambda=0.00100,degree=6, gamma=0.01000, Training Loss=794441.8810719482, Testing Loss=88652.34417155365\n",
      "lambda=0.00100,degree=6, gamma=0.21000, Training Loss=414788.0988033649, Testing Loss=226956.0498753397\n",
      "lambda=0.00100,degree=6, gamma=0.41000, Training Loss=562643.10058845, Testing Loss=309643.14332891593\n",
      "lambda=0.00100,degree=6, gamma=0.61000, Training Loss=884386.0620049268, Testing Loss=633882.9336841331\n",
      "lambda=0.00100,degree=6, gamma=0.81000, Training Loss=3329472.2866644305, Testing Loss=3141028.7186558084\n",
      "lambda=0.00100,degree=7, gamma=0.01000, Training Loss=394277.2051438988, Testing Loss=43319.25943609197\n",
      "lambda=0.00100,degree=7, gamma=0.21000, Training Loss=405063.629288294, Testing Loss=101125.08088665361\n",
      "lambda=0.00100,degree=7, gamma=0.41000, Training Loss=518834.5578636392, Testing Loss=273372.21771691163\n",
      "lambda=0.00100,degree=7, gamma=0.61000, Training Loss=794970.7500534306, Testing Loss=566409.8772870941\n",
      "lambda=0.00100,degree=7, gamma=0.81000, Training Loss=1642363.3192059135, Testing Loss=1454492.278450542\n",
      "lambda=0.00100,degree=8, gamma=0.01000, Training Loss=251637.31936716227, Testing Loss=26022.88422378803\n",
      "lambda=0.00100,degree=8, gamma=0.21000, Training Loss=391621.7373133672, Testing Loss=109754.71855801041\n",
      "lambda=0.00100,degree=8, gamma=0.41000, Training Loss=598669.9018428653, Testing Loss=331432.58395301254\n",
      "lambda=0.00100,degree=8, gamma=0.61000, Training Loss=947444.5043437423, Testing Loss=683864.211729375\n",
      "lambda=0.00100,degree=8, gamma=0.81000, Training Loss=1419953.512385605, Testing Loss=1153243.6071295082\n",
      "lambda=0.00200,degree=1, gamma=0.01000, Training Loss=1027541.5654729658, Testing Loss=100007.48667848206\n",
      "lambda=0.00200,degree=1, gamma=0.21000, Training Loss=428679.69873224595, Testing Loss=234761.45893052066\n",
      "lambda=0.00200,degree=1, gamma=0.41000, Training Loss=1016250.2810233412, Testing Loss=827182.4894679498\n",
      "lambda=0.00200,degree=1, gamma=0.61000, Training Loss=1982823.2275333325, Testing Loss=1798741.9364585534\n",
      "lambda=0.00200,degree=1, gamma=0.81000, Training Loss=3331674.426721645, Testing Loss=3147772.172453533\n",
      "lambda=0.00200,degree=2, gamma=0.01000, Training Loss=378199.49989733036, Testing Loss=41526.959285464036\n",
      "lambda=0.00200,degree=2, gamma=0.21000, Training Loss=539020.1475481174, Testing Loss=351509.12970054767\n",
      "lambda=0.00200,degree=2, gamma=0.41000, Training Loss=973932.279957033, Testing Loss=715979.557996663\n",
      "lambda=0.00200,degree=2, gamma=0.61000, Training Loss=1735996.7350435387, Testing Loss=1507501.601778391\n",
      "lambda=0.00200,degree=2, gamma=0.81000, Training Loss=2875424.200534948, Testing Loss=2654487.1298708306\n",
      "lambda=0.00200,degree=3, gamma=0.01000, Training Loss=604261.9643079578, Testing Loss=61592.572461761614\n",
      "lambda=0.00200,degree=3, gamma=0.21000, Training Loss=453536.718941276, Testing Loss=204439.72922606004\n",
      "lambda=0.00200,degree=3, gamma=0.41000, Training Loss=1053104.2463758893, Testing Loss=874217.3697488218\n",
      "lambda=0.00200,degree=3, gamma=0.61000, Training Loss=1420468.9789724583, Testing Loss=1233659.5608012544\n",
      "lambda=0.00200,degree=3, gamma=0.81000, Training Loss=3109871.9895524983, Testing Loss=2858172.998067977\n",
      "lambda=0.00200,degree=4, gamma=0.01000, Training Loss=1388296.5495075332, Testing Loss=139020.82402630048\n",
      "lambda=0.00200,degree=4, gamma=0.21000, Training Loss=613147.089375972, Testing Loss=425600.0331545012\n",
      "lambda=0.00200,degree=4, gamma=0.41000, Training Loss=875094.9059071795, Testing Loss=690548.9969861809\n",
      "lambda=0.00200,degree=4, gamma=0.61000, Training Loss=1652852.386014963, Testing Loss=1467767.6303426744\n",
      "lambda=0.00200,degree=4, gamma=0.81000, Training Loss=2497207.174310201, Testing Loss=2300763.4858957585\n",
      "lambda=0.00200,degree=5, gamma=0.01000, Training Loss=415653.1596967743, Testing Loss=45014.26493764897\n",
      "lambda=0.00200,degree=5, gamma=0.21000, Training Loss=525573.3678002894, Testing Loss=201931.0641564678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00200,degree=5, gamma=0.41000, Training Loss=885951.2432447829, Testing Loss=703284.9614761752\n",
      "lambda=0.00200,degree=5, gamma=0.61000, Training Loss=1721807.7599913112, Testing Loss=1408771.3004697105\n",
      "lambda=0.00200,degree=5, gamma=0.81000, Training Loss=2752024.149686138, Testing Loss=2441267.101406548\n",
      "lambda=0.00200,degree=6, gamma=0.01000, Training Loss=798396.932419888, Testing Loss=88562.41156649348\n",
      "lambda=0.00200,degree=6, gamma=0.21000, Training Loss=532060.5566251406, Testing Loss=176134.05480864114\n",
      "lambda=0.00200,degree=6, gamma=0.41000, Training Loss=817774.5232523663, Testing Loss=561554.3363319377\n",
      "lambda=0.00200,degree=6, gamma=0.61000, Training Loss=1517650.411242994, Testing Loss=1232293.9661737646\n",
      "lambda=0.00200,degree=6, gamma=0.81000, Training Loss=2310236.0075162915, Testing Loss=2067641.0560561102\n",
      "lambda=0.00200,degree=7, gamma=0.01000, Training Loss=403698.9236314634, Testing Loss=44555.60363713422\n",
      "lambda=0.00200,degree=7, gamma=0.21000, Training Loss=370292.41291647224, Testing Loss=172884.14368908398\n",
      "lambda=0.00200,degree=7, gamma=0.41000, Training Loss=672415.816660908, Testing Loss=438979.5593531437\n",
      "lambda=0.00200,degree=7, gamma=0.61000, Training Loss=1432142.8787737382, Testing Loss=1245802.0281568612\n",
      "lambda=0.00200,degree=7, gamma=0.81000, Training Loss=1813219.9764071868, Testing Loss=1587338.248415233\n",
      "lambda=0.00200,degree=8, gamma=0.01000, Training Loss=252489.25634232213, Testing Loss=26485.27846770251\n",
      "lambda=0.00200,degree=8, gamma=0.21000, Training Loss=466834.34243420535, Testing Loss=185542.03216986207\n",
      "lambda=0.00200,degree=8, gamma=0.41000, Training Loss=879664.4050537902, Testing Loss=608912.7411725824\n",
      "lambda=0.00200,degree=8, gamma=0.61000, Training Loss=1614833.2593175902, Testing Loss=1359445.0212859006\n",
      "lambda=0.00200,degree=8, gamma=0.81000, Training Loss=2563317.587585987, Testing Loss=2298889.2817928833\n",
      "Accuracy per jet nbr: \n",
      "\n",
      "[76365.91577245588, 73373.88271401789, 60782.15221264602, 20838.24924377958]\n"
     ]
    }
   ],
   "source": [
    "#separate data with respect to column 24 and remove None\n",
    "split_x_test, _, split_ids_test =  separate(y_donotUse, tX_test, ids_test)\n",
    "\n",
    "\n",
    "split_x_cleaned_test = removeNone(split_x_test, dataStatistics(split_x_test))\n",
    "\n",
    "#median instead of None\n",
    "split_x_with_median = putMedianInsteadOfNone(split_x_cleaned_test)\n",
    "\n",
    "split_x_with_median_with_momentum = add_momentum_vector(split_x_with_median)\n",
    "\n",
    "#line dropped when None\n",
    "#split_x_drop_lines, split_y_dropped_split_indexes_dropped = dropLineIfNone(split_x_cleaned_test, _, split_ids_test)\n",
    "\n",
    "#degrees for polynomial feature expension\n",
    "degrees = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "y_res = []\n",
    "\n",
    "acc = []\n",
    "\n",
    "plot_data_per_jetnum = []\n",
    "\n",
    "\n",
    "for i in range(len(cleaned_with_median)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #training: chose either cross calidation or cross validation for logistic regression with regularization\n",
    "    #w_star, d, accuracy, training_set, plot_data = crossValidation(cleaned_with_median[i], split_y[i], 0.98, degrees ,6)\n",
    "    w_star, d, accuracy, training_set, plot_data = crossValidationForLogistic_reg_with_loss(cleaned_with_median[i], split_y[i], 0.9, degrees ,6)\n",
    "    \n",
    "    \n",
    "    #polynomial feature expension and normalization using the training data\n",
    "    mean = np.mean(build_poly(training_set,d), axis = 0)\n",
    "    std = np.std(build_poly(training_set,d), axis = 0)\n",
    "    \n",
    "      \n",
    "    #put 1 if std = 0\n",
    "    std = std + (std == 0)\n",
    "    \n",
    "    extended_and_normalized = (build_poly(split_x_with_median[i], d) - mean) / std\n",
    "    \n",
    "    #adding bias term\n",
    "    bias = np.ones(shape=split_x_with_median[i].shape)          \n",
    "    x_test_ready = np.c_[bias, extended_and_normalized]\n",
    "    \n",
    "    #prediction for least squares\n",
    "    #y_res.append(predict_labels(w_star, x_test_ready))\n",
    "    \n",
    "    #prediction for logistic\n",
    "    y_res.append(predict_labels_logistic(w_star, x_test_ready))\n",
    "    \n",
    "\n",
    "\n",
    "    acc.append(accuracy)\n",
    "    plot_data_per_jetnum.append(plot_data)\n",
    "\n",
    "print(\"Accuracy per jet nbr: \\n\")\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stem() got an unexpected keyword argument 'use_line_collection'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-18701bf9b52e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.86\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_line_collection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cross_with_momentum_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stem() got an unexpected keyword argument 'use_line_collection'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "\n",
    "for i in range(len(plot_data_per_jetnum)):\n",
    "    jet_num = plot_data_per_jetnum[i]\n",
    "\n",
    "\n",
    "\n",
    "    X = np.array([x[0] for x in jet_num])\n",
    "    Y = np.array([x[1] for x in jet_num])\n",
    "    Z = np.array([x[3] for x in jet_num])\n",
    "    \n",
    "\n",
    "    plt.figure(dpi=120)\n",
    "    plt.title('Degree against accuracy for jet_num = %d' %i)\n",
    "    plt.xlabel('Degree of polynomial extension')\n",
    "    plt.ylabel('Testing Accuracy')\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.7,0.86])\n",
    "\n",
    "    plt.stem(Y, Z, use_line_collection=True, label=\"hello\")\n",
    "    plt.savefig(\"cross_with_momentum_\"+str(i))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "#reassemble the data for the submission\n",
    "y_pred = put_together(y_res, split_ids_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
