{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data according to the value of column 24 (PRI_jet_num) \n",
    "\n",
    "def separate(y, tX, ids):\n",
    "    \n",
    "    split_x = []\n",
    "    split_y = []\n",
    "    split_ids = []\n",
    "    \n",
    "    jet_column_nbr = 22\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        split_x.append(tX[np.where(tX[:,jet_column_nbr] == i)])\n",
    "        split_y.append(y[np.where(tX[:,jet_column_nbr] == i)])\n",
    "        split_ids.append(ids[np.where(tX[:,jet_column_nbr] == i)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return split_x, split_y, split_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_x, split_y, split_ids = separate(y, tX, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns from each set of data given a boolean array\n",
    "\n",
    "def removeNone(data, selection):\n",
    "   \n",
    "    cleaned=[]\n",
    "    \n",
    "    for i in range(4):\n",
    "        curr_data = data[i]\n",
    "        \n",
    "        cleaned.append(curr_data[:,selection[i]])\n",
    "      \n",
    "    return cleaned\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print statistics about the None values (-999) for each columns\n",
    "#returns a boolean array that can be used to filter the columns that have 100% of undefined values (-999)\n",
    "def dataStatistics(data):\n",
    "    \n",
    "    stats=[]\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        print(\"Statistics \")\n",
    "        print(\"Type :\")\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "        nones = (data[i] == -999)\n",
    "    \n",
    "        mean = np.sum(nones, axis=0)/nones.shape[0]\n",
    "        print(mean) \n",
    "        stats.append(mean != 1)\n",
    "    \n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.26145747 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 1.         1.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09751883 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05859584 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.0666396 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "selection = dataStatistics(split_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = removeNone(split_x, selection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can either drop the lines with residual Nones or replace the Nones by the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the value of column 0 (can be None sometimes) by the median value of this column\n",
    "\n",
    "def putMedianInsteadOfNone(cleaned):\n",
    "    \n",
    "    completed_data = []\n",
    "    \n",
    "    for i in range(len(cleaned)):\n",
    "        #current PRI_jet_num\n",
    "        current = cleaned[i]\n",
    "        \n",
    "        median = np.median(current[np.where(current[:,0] != -999)], axis = 0)\n",
    "        \n",
    "        #replace -999 by median value\n",
    "        current[np.where(current[:,0] == -999)] = median\n",
    "        \n",
    "        completed_data.append(current)\n",
    "    \n",
    "    \n",
    "    return completed_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_with_median = putMedianInsteadOfNone(cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of putting the median we can simply drop the data where columns 0 == -999\n",
    "def dropLineIfNone(cleaned, split_y, split_ids):\n",
    "    \n",
    "    res_x=[]\n",
    "    res_y=[]\n",
    "    res_ids=[]\n",
    "    \n",
    "    for i in range(len(cleaned)):\n",
    "        \n",
    "        current = cleaned[i]\n",
    "        \n",
    "        drop_indexes = np.where(current[:,0] != -999)\n",
    "        \n",
    "        res_x.append(current[drop_indexes])\n",
    "        res_y.append(current[drop_indexes])\n",
    "        res_ids.append(current[drop_indexes])\n",
    "        \n",
    "    return res_x, res_y, res_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropped_x, dropped_y, dropped_ids = dropLineIfNone(cleaned, split_y, split_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, the first values in each of the split data has a PRI_jet_num = 0, then 1 and so on. The data is clean and we can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Expension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to perform polynomial feature expension\n",
    "\n",
    "def build_poly(x, degree):\n",
    "   \n",
    "    x_extended = x\n",
    "\n",
    "    for d in range (2, degree +1):\n",
    "        x_extended = np.c_[x_extended, x**d]\n",
    "        \n",
    "\n",
    "    return x_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to split the training set into a (new) training set and a test set (same as in lab03)\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    " \n",
    "    # split the data based on the given ratio\n",
    "\n",
    "    training_nbr = int(x.shape[0] * ratio)\n",
    "    indexes = np.random.choice(x.shape[0],training_nbr, replace=False)\n",
    "    \n",
    "    x_train = x[indexes]\n",
    "    y_train = y[indexes]\n",
    "    x_test = np.delete(x, indexes, axis = 0)\n",
    "    y_test = np.delete(y, indexes, axis = 0)\n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross-validation \n",
    "\n",
    "def crossValidation(x, y, splitRatio, degrees, seed =1):\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, splitRatio, seed)\n",
    "    \n",
    "    a_training = []\n",
    "    a_testing = []\n",
    "    weights = []\n",
    "    degr = []\n",
    "    \n",
    "    # define parameter (just add more for loops if there are more parameters for the model)\n",
    "    lambdas = np.arange(0,0.000001,0.0000001)\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        for ind_d, d in enumerate(degrees):\n",
    "            \n",
    "            \n",
    "            #perform polynomial feature expension\n",
    "            x_test_poly = build_poly(x_test,d)\n",
    "            x_train_poly = build_poly(x_train, d)\n",
    "           \n",
    "            \n",
    "            #normalize data (DANGER: the test set must be normalized with the training set's mean and std)\n",
    "            mean = np.mean(x_train_poly, axis =0)\n",
    "            std = np.std(x_train_poly, axis = 0)\n",
    "            \n",
    "              \n",
    "            #put 1 if std = 0\n",
    "            std = std + (std == 0)\n",
    "\n",
    "            \n",
    "            x_train_ready = (x_train_poly - mean) / std\n",
    "            x_test_ready = (x_test_poly - mean) / std\n",
    "            \n",
    "            \n",
    "            #add bias term\n",
    "            bias_tr = np.ones(shape=x_train.shape)\n",
    "            bias_te = np.ones(shape=x_test.shape)\n",
    "            \n",
    "            x_train_ready = np.c_[bias_tr, x_train_ready]\n",
    "            x_test_ready = np.c_[bias_te, x_test_ready]\n",
    "            \n",
    "            \n",
    "            #Models\n",
    "        \n",
    "            #ideal :  lambdas = np.arange(0,0.000001,0.0000001) => 81.9 %\n",
    "            w_star, e_tr = ridge_regression(y_train,x_train_ready, lambda_)\n",
    "        \n",
    "            #ideal : lambdas = np.arange(0,0.3,0.1)\n",
    "            #w_star, e_tr = logistic_regression(y_train, x_train_ready,np.ones(x_train_ready.shape[1])  ,400, lambda_)\n",
    "        \n",
    "            #don't usel least squares with lambda bigger than 0.35 ideal: lambdas = np.arange(0.001,0.13,0.01)\n",
    "            #w_star, e_tr = least_squares_GD(y_train, x_train_ready,np.ones(x_train_ready.shape[1])  ,400, lambda_)    \n",
    "            #w_star, e_tr = least_squares_SGD(y_train, x_train,np.ones(x_train.shape[1])  ,400, lambda_)\n",
    "        \n",
    "            #DON'T REALLY NEED TO DO CROSS VALIDATION FOR THIS ONE ;) BUT PRACTICAL TO RUN IT HERE\n",
    "            #w_star, e_tr = least_squares(y_train, x_train_ready)  \n",
    "        \n",
    "            degr.append(d)\n",
    "        \n",
    "            #compare the prediction with the reality\n",
    "            accuracy_training = np.count_nonzero(predict_labels(w_star, x_train_ready) + y_train)/len(y_train)\n",
    "            accuracy_testing = np.count_nonzero(predict_labels(w_star, x_test_ready) + y_test)/len(y_test)\n",
    "        \n",
    "            a_training.append(accuracy_training)\n",
    "            a_testing.append(accuracy_testing)\n",
    "            weights.append(w_star)\n",
    "            print(\"lambda={l:.5f},degree={deg}, Training Accuracy={tr}, Testing Accuracy={te}\".format(\n",
    "                   l=lambda_, tr=a_training[ind*len(degrees)+ind_d], te=a_testing[ind*len(degrees)+ind_d], deg=d))\n",
    "        \n",
    "            #plt.plot(lambdas, a_training,'r--' , lambdas, a_testing, 'g--')\n",
    "            #plt.show\n",
    "    \n",
    "    return weights[np.argmax(a_testing)], degr[np.argmax(a_testing)], a_testing[np.argmax(a_testing)], x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross-validation \n",
    "\n",
    "def crossValidationForLogistic_reg(x, y, splitRatio, degrees, seed =1):\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, splitRatio, seed)\n",
    "    \n",
    "    a_training = []\n",
    "    a_testing = []\n",
    "    weights = []\n",
    "    degr = []\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    # define parameter (just add more for loops if there are more parameters for the model)\n",
    "    lambdas = np.arange(0.0001,0.3,0.1)\n",
    "    gammas = np.arange(0.01,1,0.3)\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        for ind_d, d in enumerate(degrees):\n",
    "            \n",
    "            for ind_g, gamma in enumerate(gammas):\n",
    "            \n",
    "                #perform polynomial feature expension\n",
    "                x_test_poly = build_poly(x_test,d)\n",
    "                x_train_poly = build_poly(x_train, d)\n",
    "            \n",
    "                #normalize data (DANGER: the test set must be normalized with the training set's mean and std)\n",
    "                mean = np.mean(x_train_poly, axis =0)\n",
    "                std = np.std(x_train_poly, axis = 0)\n",
    "            \n",
    "                #put 1 if std = 0\n",
    "                std = std + (std == 0)\n",
    "            \n",
    "                x_train_ready = (x_train_poly - mean) / std\n",
    "                x_test_ready = (x_test_poly - mean) / std\n",
    "                \n",
    "               \n",
    "                #add bias term\n",
    "                \n",
    "                bias_tr = np.ones(shape=x_train.shape)\n",
    "                bias_te = np.ones(shape=x_test.shape)\n",
    "            \n",
    "                x_train_ready = np.c_[bias_tr, x_train_ready]\n",
    "                x_test_ready = np.c_[bias_te, x_test_ready]\n",
    "                \n",
    "           \n",
    "\n",
    "                #Model\n",
    "        \n",
    "                #ideal :lambdas = np.arange(0,0.3,0.01)\n",
    "                #       gammas = np.arange(0,3,0.5)\n",
    "                w_star, e_tr = reg_logistic_regression(y_train, x_train_ready, lambda_, np.ones(x_test_ready.shape[1]), 30, gamma)\n",
    "        \n",
    "           \n",
    "                degr.append(d)\n",
    "        \n",
    "                #compare the prediction with the reality\n",
    "                accuracy_training = np.count_nonzero(predict_labels(w_star, x_train_ready) + y_train)/len(y_train)\n",
    "                accuracy_testing = np.count_nonzero(predict_labels(w_star, x_test_ready) + y_test)/len(y_test)\n",
    "        \n",
    "                a_training.append(accuracy_training)\n",
    "                a_testing.append(accuracy_testing)\n",
    "                weights.append(w_star)\n",
    "                print(\"lambda={l:.5f},degree={deg}, gamma={ga:.5f}, Training Accuracy={tr}, Testing Accuracy={te}\".format(\n",
    "                       l=lambda_, tr=a_training[index], te=a_testing[index], deg=d, ga=gamma))\n",
    "        \n",
    "                #increment index\n",
    "                index = index + 1\n",
    "    \n",
    "    return weights[np.argmax(a_testing)], degr[np.argmax(a_testing)], a_testing[np.argmax(a_testing)], x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we separated the data according to PRI_jet_num\n",
    "# we have to make separate prediction and then put them together for the submission\n",
    "\n",
    "def put_together(labels, indices):\n",
    "    \n",
    "    #First build first chunk\n",
    "    ids_0 = np.matrix(indices[0]).T\n",
    "    lab_0 = np.matrix(labels[0]).T\n",
    "    \n",
    "    unsorted_res = np.concatenate((ids_0, lab_0), axis=1)\n",
    "    \n",
    "    for i in range(1,len(labels)):\n",
    "        ids = np.matrix(indices[i]).T\n",
    "        lab = np.matrix(labels[i]).T\n",
    "        by_jet_num = np.concatenate((ids, lab), axis=1)\n",
    "        unsorted_res = np.concatenate((unsorted_res, by_jet_num), axis=0)\n",
    "    \n",
    "    sorted_res = unsorted_res[np.lexsort(np.fliplr(unsorted_res).T)]\n",
    "    \n",
    "    return sorted_res[0,:,:][:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_donotUse, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.2605448 0.        0.        0.        1.        1.        1.\n",
      " 0.        0.        0.        0.        0.        1.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.        1.        1.        1.        1.\n",
      " 1.        0.       ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09834149 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05881481 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.06376737 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282492410004337, Testing Accuracy=0.8346677341873499\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.8327531944707021, Testing Accuracy=0.8415732586068855\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8369235217579876, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8380133672890648, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8389920040924812, Testing Accuracy=0.8454763811048839\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.840037366132494, Testing Accuracy=0.8481785428342674\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8416498926835778, Testing Accuracy=0.850880704563651\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8423727494133739, Testing Accuracy=0.8511809447558046\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8423616285406079, Testing Accuracy=0.8515812650120096\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8426285294869942, Testing Accuracy=0.8507806244995997\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8424617163955027, Testing Accuracy=0.8506805444355484\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.842606287741462, Testing Accuracy=0.850880704563651\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8427619799601873, Testing Accuracy=0.8513811048839072\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.84283982606955, Testing Accuracy=0.8505804643714971\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8426285294869942, Testing Accuracy=0.8511809447558046\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8347678142514011\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832864403198363, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.836856796521391, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8378131915792751, Testing Accuracy=0.8444755804643715\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8382802682354511, Testing Accuracy=0.8450760608486789\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8386917405277966, Testing Accuracy=0.8456765412329864\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.839092091947376, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8393812346392945, Testing Accuracy=0.8459767814251401\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8393589928937623, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8394813224941894, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8394813224941894, Testing Accuracy=0.84677742193755\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8396926190767452, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8396814982039791, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.839548047730786, Testing Accuracy=0.8468775020016013\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8395925312218503, Testing Accuracy=0.8468775020016013\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244459025144293, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8347678142514011\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.8328755240711291, Testing Accuracy=0.8418734987990393\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8368456756486249, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8377575872154447, Testing Accuracy=0.844675740592474\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8381801803805563, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8384804439452408, Testing Accuracy=0.8449759807846277\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8388363118737558, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8391032128201421, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8391810589295048, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8392366632933352, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8393145094026979, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8394590807486572, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8395146851124876, Testing Accuracy=0.84677742193755\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8395814103490842, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8395369268580198, Testing Accuracy=0.8466773418734987\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244347816416633, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8347678142514011\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.8328866449438952, Testing Accuracy=0.8418734987990393\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8368456756486249, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8376686202333159, Testing Accuracy=0.8449759807846277\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380912133984275, Testing Accuracy=0.8454763811048839\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8384359604541765, Testing Accuracy=0.8452762209767815\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8386472570367323, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8391476963112064, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8390809710746099, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8391254545656743, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8392700259116336, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8392922676571657, Testing Accuracy=0.8466773418734987\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8393701137665284, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8394813224941894, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8395035642397215, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244347816416633, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282714827459658, Testing Accuracy=0.8347678142514011\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.8328755240711291, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8368123130303267, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8376574993605498, Testing Accuracy=0.8450760608486789\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380467299073632, Testing Accuracy=0.8451761409127302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=7, Training Accuracy=0.8384025978358781, Testing Accuracy=0.8454763811048839\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8385805318001357, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8389586414741829, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.838847432746522, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8390809710746099, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8391588171839726, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8392366632933352, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8393478720209963, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8393589928937623, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8394145972575928, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282714827459658, Testing Accuracy=0.8347678142514011\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832853282325597, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8368345547758588, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8376463784877837, Testing Accuracy=0.8450760608486789\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380800925256614, Testing Accuracy=0.8451761409127302\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8382913891082172, Testing Accuracy=0.8458767013610888\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.838502685690773, Testing Accuracy=0.8459767814251401\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8389030371103524, Testing Accuracy=0.8454763811048839\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8389141579831185, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8390253667107794, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8391254545656743, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8391143336929082, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8392922676571657, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.839325630275464, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8393367511482301, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282492410004337, Testing Accuracy=0.8346677341873499\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832864403198363, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8368011921575605, Testing Accuracy=0.8452762209767815\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8376686202333159, Testing Accuracy=0.8449759807846277\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380689716528953, Testing Accuracy=0.8451761409127302\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8381913012533223, Testing Accuracy=0.8456765412329864\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8384248395814103, Testing Accuracy=0.8458767013610888\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8387695866371593, Testing Accuracy=0.8455764611689351\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8388363118737558, Testing Accuracy=0.8458767013610888\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8389586414741829, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8391143336929082, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8391032128201421, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8393145094026979, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8392477841661014, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8392700259116336, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282381201276676, Testing Accuracy=0.8346677341873499\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832864403198363, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8367678295392622, Testing Accuracy=0.8452762209767815\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8377909498337429, Testing Accuracy=0.8449759807846277\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380356090345971, Testing Accuracy=0.8452762209767815\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8382246638716206, Testing Accuracy=0.8454763811048839\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8383025099809833, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.838736224018861, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8388363118737558, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8389030371103524, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8391254545656743, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8391476963112064, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8392255424205691, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.839203300675037, Testing Accuracy=0.8466773418734987\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8392144215478031, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8346677341873499\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832864403198363, Testing Accuracy=0.8418734987990393\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8367678295392622, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8377464663426786, Testing Accuracy=0.8450760608486789\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380022464162987, Testing Accuracy=0.8452762209767815\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8382469056171529, Testing Accuracy=0.8456765412329864\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8383692352175799, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.838736224018861, Testing Accuracy=0.8458767013610888\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8388029492554576, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.838847432746522, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8390364875835455, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8390698502018439, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8392144215478031, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8391476963112064, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8391476963112064, Testing Accuracy=0.84677742193755\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244459025144293, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8346677341873499\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832864403198363, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8367455877937301, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8378576750703395, Testing Accuracy=0.8447758206565252\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8379911255435326, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8382135429988545, Testing Accuracy=0.8455764611689351\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8384470813269426, Testing Accuracy=0.8455764611689351\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8385916526729018, Testing Accuracy=0.8459767814251401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=10, Training Accuracy=0.8387584657643932, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8388251910009897, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.838969762346949, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8390587293290778, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8391365754384404, Testing Accuracy=0.8459767814251401\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8391588171839726, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8391810589295048, Testing Accuracy=0.8466773418734987\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7147258163894024, Testing Accuracy=0.7120567375886525\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7606929458797231, Testing Accuracy=0.7595099935525468\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714396251558269, Testing Accuracy=0.7676337846550613\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7809110318244996, Testing Accuracy=0.7773049645390071\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7838341285875997, Testing Accuracy=0.779110251450677\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.787487999541475, Testing Accuracy=0.7829787234042553\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7947384258264196, Testing Accuracy=0.7922630560928433\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8020891544512746, Testing Accuracy=0.7987105093488073\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8051555402713895, Testing Accuracy=0.8010315925209542\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8046826863832409, Testing Accuracy=0.8016763378465506\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8050265801200762, Testing Accuracy=0.8014184397163121\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8053991316683144, Testing Accuracy=0.8007736943907157\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8055567496310306, Testing Accuracy=0.800902643455835\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8054277894797174, Testing Accuracy=0.8006447453255964\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8057716832165528, Testing Accuracy=0.7994842037395229\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.805628394159538, Testing Accuracy=0.8007736943907157\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7146828296722979, Testing Accuracy=0.7120567375886525\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607072747854247, Testing Accuracy=0.7595099935525468\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7715112696843343, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808967029187981, Testing Accuracy=0.7774339136041264\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7836191950020777, Testing Accuracy=0.7788523533204385\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7864993050480735, Testing Accuracy=0.7809155383623468\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7874450128243706, Testing Accuracy=0.7824629271437782\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7882331026379515, Testing Accuracy=0.7829787234042553\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7891071658857413, Testing Accuracy=0.7822050290135396\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7895943486795913, Testing Accuracy=0.7838813668600902\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7898809267936208, Testing Accuracy=0.7847840103159253\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7899669002278296, Testing Accuracy=0.7845261121856867\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7901245181905457, Testing Accuracy=0.7847840103159253\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.790253478341859, Testing Accuracy=0.785170857511283\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7905113986444855, Testing Accuracy=0.7850419084461637\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7906546877015003, Testing Accuracy=0.7850419084461637\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7146828296722979, Testing Accuracy=0.7120567375886525\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607072747854247, Testing Accuracy=0.7595099935525468\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7715255985900357, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808393872959922, Testing Accuracy=0.7774339136041264\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834185903222571, Testing Accuracy=0.778207607994842\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7858974910086117, Testing Accuracy=0.7807865892972276\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7868145409735059, Testing Accuracy=0.7820760799484203\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7872730659559529, Testing Accuracy=0.781431334622824\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7875596440699824, Testing Accuracy=0.7809155383623468\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7879895112410266, Testing Accuracy=0.7823339780786589\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7887059565261001, Testing Accuracy=0.7819471308833011\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7888779033945178, Testing Accuracy=0.782849774339136\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7887059565261001, Testing Accuracy=0.7829787234042553\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7888205877717118, Testing Accuracy=0.782849774339136\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7888492455831149, Testing Accuracy=0.782849774339136\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7887776010546075, Testing Accuracy=0.7825918762088975\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7146828296722979, Testing Accuracy=0.7120567375886525\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607072747854247, Testing Accuracy=0.7595099935525468\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714969407786327, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808537162016936, Testing Accuracy=0.7774339136041264\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834185903222571, Testing Accuracy=0.778207607994842\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7854532949318661, Testing Accuracy=0.7803997421018698\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.786241384745447, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7867715542564014, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7868718565963118, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7873160526730574, Testing Accuracy=0.7806576402321083\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7873876972015648, Testing Accuracy=0.7813023855577047\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7878462221840118, Testing Accuracy=0.7816892327530626\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7883190760721603, Testing Accuracy=0.7818181818181819\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7883190760721603, Testing Accuracy=0.7820760799484203\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7883190760721603, Testing Accuracy=0.7818181818181819\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7883620627892648, Testing Accuracy=0.7815602836879433\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7146828296722979, Testing Accuracy=0.7120567375886525\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607072747854247, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714826118729312, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808393872959922, Testing Accuracy=0.7775628626692457\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834329192279585, Testing Accuracy=0.7785944551901999\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7850234277608219, Testing Accuracy=0.7803997421018698\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7859404777257161, Testing Accuracy=0.7813023855577047\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7862557136511484, Testing Accuracy=0.7807865892972276\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7862987003682529, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7867428964449985, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7869435011248191, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7872587370502515, Testing Accuracy=0.7806576402321083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=13, Training Accuracy=0.787487999541475, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7875023284471765, Testing Accuracy=0.7809155383623468\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7876886042212956, Testing Accuracy=0.7807865892972276\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7877315909384001, Testing Accuracy=0.7806576402321083\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7146685007665965, Testing Accuracy=0.7119277885235332\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607216036911261, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714826118729312, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808393872959922, Testing Accuracy=0.7774339136041264\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834615770393615, Testing Accuracy=0.7783365570599613\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7849517832323145, Testing Accuracy=0.7788523533204385\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7858545042915073, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7860121222542235, Testing Accuracy=0.7802707930367505\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7860551089713279, Testing Accuracy=0.7793681495809155\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.786184069122641, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7865996073879837, Testing Accuracy=0.7792392005157963\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7868431987849088, Testing Accuracy=0.7801418439716312\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7872014214274455, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7872300792388486, Testing Accuracy=0.7802707930367505\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7871584347103412, Testing Accuracy=0.7802707930367505\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7871441058046397, Testing Accuracy=0.7802707930367505\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7147114874837008, Testing Accuracy=0.7119277885235332\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607359325968276, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714396251558269, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808823740130966, Testing Accuracy=0.7774339136041264\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7833039590766453, Testing Accuracy=0.7783365570599613\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7847368496467925, Testing Accuracy=0.7784655061250806\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7858688331972087, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7859548066314176, Testing Accuracy=0.7801418439716312\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7857971886687014, Testing Accuracy=0.7788523533204385\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7859261488200147, Testing Accuracy=0.7796260477111541\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7863273581796558, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7864706472366705, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7868575276906102, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7869148433134161, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7870724612761324, Testing Accuracy=0.7801418439716312\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7869148433134161, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7147114874837008, Testing Accuracy=0.7119277885235332\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607216036911261, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714109673444239, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808680451073952, Testing Accuracy=0.777691811734365\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7832896301709439, Testing Accuracy=0.7785944551901999\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7847225207410911, Testing Accuracy=0.778207607994842\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7856825574230896, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7856252418002837, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7856825574230896, Testing Accuracy=0.7785944551901999\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7858258464801043, Testing Accuracy=0.7794970986460348\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.785969135537119, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7862557136511484, Testing Accuracy=0.7801418439716312\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7866282651993868, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7866712519164911, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7866139362936853, Testing Accuracy=0.7796260477111541\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7867142386335956, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7147114874837008, Testing Accuracy=0.7119277885235332\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7606929458797231, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7713679806273195, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808393872959922, Testing Accuracy=0.7775628626692457\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834329192279585, Testing Accuracy=0.7789813023855577\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7845792316840763, Testing Accuracy=0.7778207607994843\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7853529925919558, Testing Accuracy=0.7809155383623468\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7855249394603734, Testing Accuracy=0.7792392005157963\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7856109128945823, Testing Accuracy=0.7787234042553192\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7856109128945823, Testing Accuracy=0.7793681495809155\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7858545042915073, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7860694378770293, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7864133316138646, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7864706472366705, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7865566206708794, Testing Accuracy=0.7802707930367505\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7865422917651779, Testing Accuracy=0.7796260477111541\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7147258163894024, Testing Accuracy=0.7119277885235332\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7606929458797231, Testing Accuracy=0.7597678916827852\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7713536517216181, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808823740130966, Testing Accuracy=0.7779497098646034\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834185903222571, Testing Accuracy=0.779110251450677\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7844072848156586, Testing Accuracy=0.7783365570599613\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.785209703534941, Testing Accuracy=0.7807865892972276\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7854676238375675, Testing Accuracy=0.7788523533204385\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7856109128945823, Testing Accuracy=0.7787234042553192\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7854962816489705, Testing Accuracy=0.7789813023855577\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.785725544140194, Testing Accuracy=0.7793681495809155\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7858115175744028, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7861124245941338, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7862843714625514, Testing Accuracy=0.7801418439716312\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7864419894252676, Testing Accuracy=0.7797549967762734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=16, Training Accuracy=0.7865996073879837, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7369488983480735, Testing Accuracy=0.722707423580786\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7791844026377892, Testing Accuracy=0.7633981738785233\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7990119318056506, Testing Accuracy=0.7852322350138944\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.807745748880704, Testing Accuracy=0.797141722905915\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8104585253964403, Testing Accuracy=0.7943628423977769\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.813612403784654, Testing Accuracy=0.8017070265978563\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8230078736684238, Testing Accuracy=0.8108376339817388\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.83090359718577, Testing Accuracy=0.8229456133386265\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8333517125780199, Testing Accuracy=0.8233425962683605\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8330429412672857, Testing Accuracy=0.8233425962683605\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8334840431397631, Testing Accuracy=0.8235410877332274\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8339251450122406, Testing Accuracy=0.8249305279872965\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.834189806135727, Testing Accuracy=0.8237395791980945\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8338148695441212, Testing Accuracy=0.8215561730845574\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8338810348249929, Testing Accuracy=0.8199682413656213\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8340574755739838, Testing Accuracy=0.8213576816196904\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7371253390970645, Testing Accuracy=0.722707423580786\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7796034494166427, Testing Accuracy=0.7637951568082573\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7989678216184027, Testing Accuracy=0.7850337435490274\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8076134183189607, Testing Accuracy=0.7975387058356491\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.809995368430339, Testing Accuracy=0.79614926558158\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8133256875675438, Testing Accuracy=0.8005160778086542\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8145607728104806, Testing Accuracy=0.8023025009924574\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8171412187644737, Testing Accuracy=0.8044859071059944\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8172294391389692, Testing Accuracy=0.8042874156411274\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8174058798879601, Testing Accuracy=0.8032949583167924\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8180454776030525, Testing Accuracy=0.8044859071059944\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8189497364416312, Testing Accuracy=0.8044859071059944\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8190600119097505, Testing Accuracy=0.8050813815005955\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8190600119097505, Testing Accuracy=0.8040889241762604\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8186630202245209, Testing Accuracy=0.8044859071059944\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8190600119097505, Testing Accuracy=0.8046843985708615\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7371032840034406, Testing Accuracy=0.722707423580786\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7796034494166427, Testing Accuracy=0.7637951568082573\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7989457665247789, Testing Accuracy=0.7850337435490274\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8076134183189607, Testing Accuracy=0.7973402143707821\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8096865971196048, Testing Accuracy=0.795950774116713\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8131933570058005, Testing Accuracy=0.7999206034140532\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8134359630356631, Testing Accuracy=0.7999206034140532\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8156635274916743, Testing Accuracy=0.8021040095275903\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.816744227079244, Testing Accuracy=0.8026994839221914\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8167001168919962, Testing Accuracy=0.8030964668519254\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8168986127346111, Testing Accuracy=0.8026994839221914\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8172735493262169, Testing Accuracy=0.8028979753870583\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8178469817604376, Testing Accuracy=0.8034934497816594\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8180675326966763, Testing Accuracy=0.8032949583167924\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8178249266668137, Testing Accuracy=0.8028979753870583\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8177367062923182, Testing Accuracy=0.8036919412465264\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7370812289098168, Testing Accuracy=0.722707423580786\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7795813943230189, Testing Accuracy=0.7637951568082573\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7989457665247789, Testing Accuracy=0.7850337435490274\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8076354734125847, Testing Accuracy=0.7973402143707821\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.809818927681348, Testing Accuracy=0.796347757046447\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8127302000396992, Testing Accuracy=0.7981341802302501\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8128404755078186, Testing Accuracy=0.8001190948789202\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8150459848702057, Testing Accuracy=0.8005160778086542\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8157517478661697, Testing Accuracy=0.8017070265978563\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8161487395513994, Testing Accuracy=0.8026994839221914\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8161928497386471, Testing Accuracy=0.8021040095275903\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8168986127346111, Testing Accuracy=0.8025009924573243\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8167001168919962, Testing Accuracy=0.8019055180627233\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8173397146070885, Testing Accuracy=0.8026994839221914\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8171632738580975, Testing Accuracy=0.8025009924573243\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8171632738580975, Testing Accuracy=0.8030964668519254\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7371032840034406, Testing Accuracy=0.722707423580786\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7795813943230189, Testing Accuracy=0.7637951568082573\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7989457665247789, Testing Accuracy=0.7850337435490274\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8077236937870801, Testing Accuracy=0.797141722905915\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.809995368430339, Testing Accuracy=0.796347757046447\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8123773185417172, Testing Accuracy=0.7979356887653831\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8126199245715798, Testing Accuracy=0.8003175863437872\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8143843320614896, Testing Accuracy=0.7995236204843191\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8152003705255729, Testing Accuracy=0.8015085351329893\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.815862023334289, Testing Accuracy=0.8023025009924574\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8157517478661697, Testing Accuracy=0.8026994839221914\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8161046293641516, Testing Accuracy=0.8019055180627233\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8165016210493814, Testing Accuracy=0.8013100436681223\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.81672217198562, Testing Accuracy=0.8019055180627233\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8167883372664917, Testing Accuracy=0.8013100436681223\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.81672217198562, Testing Accuracy=0.8017070265978563\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7371032840034406, Testing Accuracy=0.722707423580786\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7795813943230189, Testing Accuracy=0.7637951568082573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=3, Training Accuracy=0.798923711431155, Testing Accuracy=0.7850337435490274\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8077016386934562, Testing Accuracy=0.797141722905915\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8099733133367151, Testing Accuracy=0.796744739976181\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8121567676054785, Testing Accuracy=0.7979356887653831\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8123773185417172, Testing Accuracy=0.7999206034140532\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8140755607507554, Testing Accuracy=0.7995236204843191\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8150239297765819, Testing Accuracy=0.8005160778086542\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8156855825852981, Testing Accuracy=0.8015085351329893\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8152444807128206, Testing Accuracy=0.8019055180627233\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8155311969299309, Testing Accuracy=0.8017070265978563\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8162590150195188, Testing Accuracy=0.8011115522032553\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.816391345581262, Testing Accuracy=0.8015085351329893\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8165898414238768, Testing Accuracy=0.8005160778086542\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8164795659557574, Testing Accuracy=0.8011115522032553\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7371032840034406, Testing Accuracy=0.722707423580786\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7795813943230189, Testing Accuracy=0.7637951568082573\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.798923711431155, Testing Accuracy=0.7850337435490274\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.807745748880704, Testing Accuracy=0.797141722905915\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8098850929622197, Testing Accuracy=0.796347757046447\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8119141615756159, Testing Accuracy=0.7977371973005161\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8121788226991024, Testing Accuracy=0.7999206034140532\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8137888445336451, Testing Accuracy=0.7987296546248511\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8148033788403432, Testing Accuracy=0.7999206034140532\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8152224256191968, Testing Accuracy=0.8013100436681223\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8151562603383251, Testing Accuracy=0.8017070265978563\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.81535475618094, Testing Accuracy=0.8013100436681223\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8159061335215368, Testing Accuracy=0.8007145692735212\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8160164089896561, Testing Accuracy=0.8011115522032553\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8161487395513994, Testing Accuracy=0.8013100436681223\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.816567786330253, Testing Accuracy=0.8005160778086542\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7370591738161929, Testing Accuracy=0.722905915045653\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7795813943230189, Testing Accuracy=0.7637951568082573\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.798923711431155, Testing Accuracy=0.7850337435490274\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8077678039743279, Testing Accuracy=0.797141722905915\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8099071480558435, Testing Accuracy=0.796347757046447\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8118921064819921, Testing Accuracy=0.7973402143707821\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8120023819501114, Testing Accuracy=0.7991266375545851\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8134800732229108, Testing Accuracy=0.7979356887653831\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8145828279041044, Testing Accuracy=0.8001190948789202\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8149136543084625, Testing Accuracy=0.8011115522032553\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8150680399638296, Testing Accuracy=0.8021040095275903\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8150680399638296, Testing Accuracy=0.8019055180627233\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8152444807128206, Testing Accuracy=0.8009130607383882\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8158399682406652, Testing Accuracy=0.8011115522032553\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8158399682406652, Testing Accuracy=0.8009130607383882\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8162369599258948, Testing Accuracy=0.8005160778086542\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7370591738161929, Testing Accuracy=0.722905915045653\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7795813943230189, Testing Accuracy=0.7637951568082573\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7989457665247789, Testing Accuracy=0.7850337435490274\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8077678039743279, Testing Accuracy=0.7969432314410481\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8097527624004764, Testing Accuracy=0.7965462485113141\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8118700513883681, Testing Accuracy=0.7969432314410481\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8117818310138727, Testing Accuracy=0.7987296546248511\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8134359630356631, Testing Accuracy=0.7979356887653831\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8142299464061225, Testing Accuracy=0.8001190948789202\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8145607728104806, Testing Accuracy=0.8009130607383882\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8149798195893342, Testing Accuracy=0.8023025009924574\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.814825433933967, Testing Accuracy=0.8019055180627233\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8150680399638296, Testing Accuracy=0.8017070265978563\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8155091418363071, Testing Accuracy=0.8009130607383882\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8158840784279129, Testing Accuracy=0.8011115522032553\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8159502437087846, Testing Accuracy=0.8003175863437872\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7370591738161929, Testing Accuracy=0.722905915045653\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7795813943230189, Testing Accuracy=0.7637951568082573\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7989457665247789, Testing Accuracy=0.7850337435490274\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8077898590679518, Testing Accuracy=0.7969432314410481\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8095763216514854, Testing Accuracy=0.796347757046447\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8118700513883681, Testing Accuracy=0.7973402143707821\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8117597759202487, Testing Accuracy=0.7983326716951171\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8132154120994244, Testing Accuracy=0.7985311631599841\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8141196709380031, Testing Accuracy=0.8001190948789202\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8145828279041044, Testing Accuracy=0.8009130607383882\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8148033788403432, Testing Accuracy=0.8021040095275903\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.814847489027591, Testing Accuracy=0.8019055180627233\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8151342052447013, Testing Accuracy=0.8019055180627233\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8152885909000683, Testing Accuracy=0.8009130607383882\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8155973622108026, Testing Accuracy=0.8007145692735212\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8157738029597935, Testing Accuracy=0.8005160778086542\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7264250263197474, Testing Accuracy=0.7392873252142534\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7626710783576478, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7941545094500426, Testing Accuracy=0.8064952638700947\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8052840026069084, Testing Accuracy=0.8168696436626072\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8079410437659799, Testing Accuracy=0.8177717636445647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=6, Training Accuracy=0.8093948964756605, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8178172156213968, Testing Accuracy=0.8204781235904375\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8307013586002908, Testing Accuracy=0.8272440234551195\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8362661051787236, Testing Accuracy=0.8272440234551195\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8357146438060862, Testing Accuracy=0.8281461434370772\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.837970622148694, Testing Accuracy=0.8258908434821831\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8381711535569258, Testing Accuracy=0.821831303563374\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.839073544893969, Testing Accuracy=0.8240866035182679\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8378202235925202, Testing Accuracy=0.8249887235002256\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8372687622198827, Testing Accuracy=0.8245376635092467\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8387727477816213, Testing Accuracy=0.8249887235002256\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7264250263197474, Testing Accuracy=0.7388362652232747\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.762420414097358, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7946558379706221, Testing Accuracy=0.8055931438881371\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8055346668671981, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.807790645209806, Testing Accuracy=0.8173207036535859\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8089938336591969, Testing Accuracy=0.8168696436626072\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8105479520729935, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8123026018950218, Testing Accuracy=0.8155164636896707\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8124530004511956, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8135057903444127, Testing Accuracy=0.8128101037437979\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8151601744623251, Testing Accuracy=0.8155164636896707\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8159623000952524, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8156113701308467, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8162129643555421, Testing Accuracy=0.8191249436175011\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8155612372787888, Testing Accuracy=0.8186738836265224\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8158119015390786, Testing Accuracy=0.8195760036084799\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7264250263197474, Testing Accuracy=0.7388362652232747\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7623201483932421, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7946558379706221, Testing Accuracy=0.8055931438881371\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8054845340151401, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8075901138015742, Testing Accuracy=0.8173207036535859\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8092946307715446, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8095954278838923, Testing Accuracy=0.8173207036535859\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8103975535168195, Testing Accuracy=0.8159675236806495\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8123026018950218, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8126033990073696, Testing Accuracy=0.8119079837618404\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.813104727527949, Testing Accuracy=0.8119079837618404\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8140071188649922, Testing Accuracy=0.8128101037437979\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8147591116458616, Testing Accuracy=0.8155164636896707\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8153105730184991, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8152103073143832, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8155111044267308, Testing Accuracy=0.8159675236806495\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7263748934676894, Testing Accuracy=0.7388362652232747\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7623201483932421, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7946558379706221, Testing Accuracy=0.8055931438881371\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8054845340151401, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8076402466536321, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8093447636236025, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8095452950318344, Testing Accuracy=0.8164185836716283\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8096956935880082, Testing Accuracy=0.8155164636896707\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.812051937634732, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.812152203338848, Testing Accuracy=0.8123590437528191\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8118514062265002, Testing Accuracy=0.8123590437528191\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8129041961197172, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8137564546047025, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8140071188649922, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8147089787938036, Testing Accuracy=0.8155164636896707\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8148593773499775, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7263748934676894, Testing Accuracy=0.7388362652232747\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7623201483932421, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7946558379706221, Testing Accuracy=0.8055931438881371\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8053842683110242, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.807740512357748, Testing Accuracy=0.8182228236355436\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8095954278838923, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8094951621797765, Testing Accuracy=0.8168696436626072\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8090940993633128, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.811099413445631, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8118012733744423, Testing Accuracy=0.8119079837618404\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8115004762620945, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8127036647114855, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8130044618238331, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8133052589361809, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8137564546047025, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8145084473855717, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7263748934676894, Testing Accuracy=0.7388362652232747\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7623702812453, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7946558379706221, Testing Accuracy=0.8055931438881371\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8054344011630822, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.807790645209806, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8095954278838923, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8093447636236025, Testing Accuracy=0.8168696436626072\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8082919737303855, Testing Accuracy=0.8132611637347767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=9, Training Accuracy=0.8108988820373991, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8117010076703264, Testing Accuracy=0.8123590437528191\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8114503434100366, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8123026018950218, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8127537975635434, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8131548603800071, Testing Accuracy=0.8155164636896707\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8133052589361809, Testing Accuracy=0.8159675236806495\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.81430791597734, Testing Accuracy=0.8155164636896707\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7263247606156314, Testing Accuracy=0.7388362652232747\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7623702812453, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7946558379706221, Testing Accuracy=0.8055931438881371\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8053842683110242, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8078909109139218, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8093948964756605, Testing Accuracy=0.8173207036535859\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8093948964756605, Testing Accuracy=0.8168696436626072\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8079911766180378, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8102471549606457, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8115506091141526, Testing Accuracy=0.8123590437528191\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8116508748182684, Testing Accuracy=0.8137122237257556\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8123527347470798, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8124028675991377, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8129543289717752, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8133052589361809, Testing Accuracy=0.8155164636896707\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8138567203088184, Testing Accuracy=0.8155164636896707\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7263247606156314, Testing Accuracy=0.7388362652232747\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7623702812453, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7946057051185642, Testing Accuracy=0.8055931438881371\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8053842683110242, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8076903795056901, Testing Accuracy=0.8177717636445647\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8095954278838923, Testing Accuracy=0.8182228236355436\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8093948964756605, Testing Accuracy=0.8168696436626072\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8078407780618639, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8103474206647616, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8112498120018048, Testing Accuracy=0.8128101037437979\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8119015390785582, Testing Accuracy=0.8137122237257556\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8119015390785582, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8123527347470798, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8125031333032536, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8129041961197172, Testing Accuracy=0.8155164636896707\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8137063217526445, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7263247606156314, Testing Accuracy=0.7388362652232747\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7623702812453, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7946057051185642, Testing Accuracy=0.8055931438881371\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8052840026069084, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8076402466536321, Testing Accuracy=0.8182228236355436\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8095452950318344, Testing Accuracy=0.8182228236355436\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8094951621797765, Testing Accuracy=0.8159675236806495\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8079911766180378, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8103474206647616, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.810949014889457, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8116007419662105, Testing Accuracy=0.8128101037437979\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.81210207048679, Testing Accuracy=0.8123590437528191\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8123026018950218, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8124530004511956, Testing Accuracy=0.8137122237257556\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8126033990073696, Testing Accuracy=0.815065403698692\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8133052589361809, Testing Accuracy=0.8137122237257556\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7263247606156314, Testing Accuracy=0.7388362652232747\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7623702812453, Testing Accuracy=0.774018944519621\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7946057051185642, Testing Accuracy=0.8055931438881371\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8052840026069084, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8076903795056901, Testing Accuracy=0.8186738836265224\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8094951621797765, Testing Accuracy=0.8182228236355436\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8093948964756605, Testing Accuracy=0.8159675236806495\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8079911766180378, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8101468892565298, Testing Accuracy=0.8141632837167343\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8108487491853411, Testing Accuracy=0.8132611637347767\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8116007419662105, Testing Accuracy=0.8128101037437979\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.812152203338848, Testing Accuracy=0.8128101037437979\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8120018047826741, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8126535318594275, Testing Accuracy=0.8137122237257556\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8126535318594275, Testing Accuracy=0.8146143437077131\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8128039304156014, Testing Accuracy=0.8132611637347767\n",
      "Accuracy per jet nbr: \n",
      "\n",
      "[0.8515812650120096, 0.8016763378465506, 0.8249305279872965, 0.8281461434370772]\n"
     ]
    }
   ],
   "source": [
    "#separate data with respect to column 24 and remove None\n",
    "split_x_test, _, split_ids_test =  separate(y_donotUse, tX_test, ids_test)\n",
    "\n",
    "\n",
    "split_x_cleaned_test = removeNone(split_x_test, dataStatistics(split_x_test))\n",
    "\n",
    "#median instead of None\n",
    "split_x_with_median = putMedianInsteadOfNone(split_x_cleaned_test)\n",
    "\n",
    "\n",
    "\n",
    "#line dropped when None\n",
    "#split_x_drop_lines, split_y_dropped_split_indexes_dropped = dropLineIfNone(split_x_cleaned_test, _, split_ids_test)\n",
    "\n",
    "#degrees for polynomial feature expension\n",
    "degrees = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
    "\n",
    "y_res = []\n",
    "\n",
    "acc = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(cleaned_with_median)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #training: chose either cross calidation or cross validation for logistic regression with regularization\n",
    "    w_star, d, accuracy, training_set = crossValidation(cleaned_with_median[i], split_y[i], 0.9, degrees ,6)\n",
    "    #w_star, d, accuracy, training_set = crossValidationForLogistic_reg(cleaned_with_median[i], split_y[i], 0.9, degrees ,6)\n",
    "    \n",
    "    \n",
    "    #polynomial feature expension and normalization using the training data\n",
    "    mean = np.mean(build_poly(training_set,d), axis = 0)\n",
    "    std = np.std(build_poly(training_set,d), axis = 0)\n",
    "    \n",
    "      \n",
    "    #put 1 if std = 0\n",
    "    std = std + (std == 0)\n",
    "    \n",
    "    extended_and_normalized = (build_poly(split_x_with_median[i], d) - mean) / std\n",
    "    \n",
    "    #adding bias term\n",
    "    bias = np.ones(shape=split_x_with_median[i].shape)          \n",
    "    x_test_ready = np.c_[bias, extended_and_normalized]\n",
    "    \n",
    "    #prediction\n",
    "    y_res.append(predict_labels(w_star, x_test_ready))\n",
    "\n",
    "\n",
    "    acc.append(accuracy)\n",
    "\n",
    "print(\"Accuracy per jet nbr: \\n\")\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "\n",
    "#reassemble the data for the submission\n",
    "y_pred = put_together(y_res, split_ids_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
