{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data according to the value of column 24 (PRI_jet_num) \n",
    "\n",
    "def separate(y, tX, ids):\n",
    "    \n",
    "    split_x = []\n",
    "    split_y = []\n",
    "    split_ids = []\n",
    "    \n",
    "    jet_column_nbr = 22\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        split_x.append(tX[np.where(tX[:,jet_column_nbr] == i)])\n",
    "        split_y.append(y[np.where(tX[:,jet_column_nbr] == i)])\n",
    "        split_ids.append(ids[np.where(tX[:,jet_column_nbr] == i)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return split_x, split_y, split_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_x, split_y, split_ids = separate(y, tX, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns from each set of data given a boolean array\n",
    "\n",
    "def removeNone(data, selection):\n",
    "   \n",
    "    cleaned=[]\n",
    "    \n",
    "    for i in range(4):\n",
    "        curr_data = data[i]\n",
    "        \n",
    "        cleaned.append(curr_data[:,selection[i]])\n",
    "      \n",
    "    return cleaned\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print statistics about the None values (-999) for each columns\n",
    "#returns a boolean array that can be used to filter the columns that have 100% of undefined values (-999)\n",
    "def dataStatistics(data):\n",
    "    \n",
    "    stats=[]\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        print(\"Statistics \")\n",
    "        print(\"Type :\")\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "        nones = (data[i] == -999)\n",
    "    \n",
    "        mean = np.sum(nones, axis=0)/nones.shape[0]\n",
    "        print(mean) \n",
    "        stats.append(mean != 1)\n",
    "    \n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.26145747 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 1.         1.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09751883 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05859584 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.0666396 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "selection = dataStatistics(split_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = removeNone(split_x, selection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can either drop the lines with residual Nones or replace the Nones by the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the value of column 0 (can be None sometimes) by the median value of this column\n",
    "\n",
    "def putMedianInsteadOfNone(cleaned):\n",
    "    \n",
    "    completed_data = []\n",
    "    \n",
    "    for i in range(len(cleaned)):\n",
    "        #current PRI_jet_num\n",
    "        current = cleaned[i]\n",
    "        \n",
    "        median = np.median(current[np.where(current[:,0] != -999)], axis = 0)\n",
    "        \n",
    "        #replace -999 by median value\n",
    "        current[np.where(current[:,0] == -999)] = median\n",
    "        \n",
    "        completed_data.append(current)\n",
    "    \n",
    "    \n",
    "    return completed_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_with_median = putMedianInsteadOfNone(cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of putting the median we can simply drop the data where columns 0 == -999\n",
    "def dropLineIfNone(cleaned, split_y, split_ids):\n",
    "    \n",
    "    res_x=[]\n",
    "    res_y=[]\n",
    "    res_ids=[]\n",
    "    \n",
    "    for i in range(len(cleaned)):\n",
    "        \n",
    "        current = cleaned[i]\n",
    "        \n",
    "        drop_indexes = np.where(current[:,0] != -999)\n",
    "        \n",
    "        res_x.append(current[drop_indexes])\n",
    "        res_y.append(current[drop_indexes])\n",
    "        res_ids.append(current[drop_indexes])\n",
    "        \n",
    "    return res_x, res_y, res_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropped_x, dropped_y, dropped_ids = dropLineIfNone(cleaned, split_y, split_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, the first values in each of the split data has a PRI_jet_num = 0, then 1 and so on. The data is clean and we can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Expension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to perform polynomial feature expension\n",
    "\n",
    "def build_poly(x, degree):\n",
    "   \n",
    "    x_extended = x\n",
    "\n",
    "    for d in range (2, degree +1):\n",
    "        x_extended = np.c_[x_extended, x**d]\n",
    "        \n",
    "\n",
    "    return x_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to split the training set into a (new) training set and a test set (same as in lab03)\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    " \n",
    "    # split the data based on the given ratio\n",
    "\n",
    "    training_nbr = int(x.shape[0] * ratio)\n",
    "    indexes = np.random.choice(x.shape[0],training_nbr, replace=False)\n",
    "    \n",
    "    x_train = x[indexes]\n",
    "    y_train = y[indexes]\n",
    "    x_test = np.delete(x, indexes, axis = 0)\n",
    "    y_test = np.delete(y, indexes, axis = 0)\n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross-validation \n",
    "\n",
    "def crossValidation(x, y, splitRatio, degrees, seed =1):\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, splitRatio, seed)\n",
    "    \n",
    "    a_training = []\n",
    "    a_testing = []\n",
    "    weights = []\n",
    "    degr = []\n",
    "    \n",
    "    # define parameter (just add more for loops if there are more parameters for the model)\n",
    "    lambdas = np.arange(0.1,5,0.5)\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        for ind_d, d in enumerate(degrees):\n",
    "            \n",
    "            \n",
    "            #perform polynomial feature expension\n",
    "            x_test_poly = build_poly(x_test,d)\n",
    "            x_train_poly = build_poly(x_train, d)\n",
    "           \n",
    "            \n",
    "            #normalize data (DANGER: the test set must be normalized with the training set's mean and std)\n",
    "            mean = np.mean(x_train_poly, axis =0)\n",
    "            std = np.std(x_train_poly, axis = 0)\n",
    "            \n",
    "            for i in range(len(std)):\n",
    "        \n",
    "                if(std[i] == 0):\n",
    "                    std[i] = 1\n",
    "            \n",
    "            x_train_ready = (x_train_poly - mean) / std\n",
    "            x_test_ready = (x_test_poly - mean) / std\n",
    "            \n",
    "            \n",
    "            #add bias term\n",
    "            bias_tr = np.ones(shape=x_train.shape)\n",
    "            bias_te = np.ones(shape=x_test.shape)\n",
    "            \n",
    "            x_train_ready = np.c_[bias_tr, x_train_ready]\n",
    "            x_test_ready = np.c_[bias_te, x_test_ready]\n",
    "            \n",
    "            \n",
    "            #Models\n",
    "        \n",
    "            #ideal : lambdas = np.arange(0,0.03,0.001)\n",
    "            w_star, e_tr = ridge_regression(y_train,x_train_ready, lambda_)\n",
    "        \n",
    "            #ideal : lambdas = np.arange(0,0.3,0.1)\n",
    "            #w_star, e_tr = logistic_regression(y_train, x_train_ready,np.ones(x_train_ready.shape[1])  ,400, lambda_)\n",
    "        \n",
    "            #don't usel least squares with lambda bigger than 0.35 ideal: lambdas = np.arange(0.001,0.13,0.01)\n",
    "            #w_star, e_tr = least_squares_GD(y_train, x_train_ready,np.ones(x_train_ready.shape[1])  ,400, lambda_)    \n",
    "            #w_star, e_tr = least_squares_SGD(y_train, x_train,np.ones(x_train.shape[1])  ,400, lambda_)\n",
    "        \n",
    "            #DON'T REALLY NEED TO DO CROSS VALIDATION FOR THIS ONE ;) BUT PRACTICAL TO RUN IT HERE\n",
    "            #w_star, e_tr = least_squares(y_train, x_train_ready)  \n",
    "        \n",
    "            degr.append(d)\n",
    "        \n",
    "            #compare the prediction with the reality\n",
    "            accuracy_training = np.count_nonzero(predict_labels(w_star, x_train_ready) + y_train)/len(y_train)\n",
    "            accuracy_testing = np.count_nonzero(predict_labels(w_star, x_test_ready) + y_test)/len(y_test)\n",
    "        \n",
    "            a_training.append(accuracy_training)\n",
    "            a_testing.append(accuracy_testing)\n",
    "            weights.append(w_star)\n",
    "            print(\"lambda={l:.5f},degree={deg}, Training Accuracy={tr}, Testing Accuracy={te}\".format(\n",
    "                   l=lambda_, tr=a_training[ind*len(degrees)+ind_d], te=a_testing[ind*len(degrees)+ind_d], deg=d))\n",
    "        \n",
    "            #plt.plot(lambdas, a_training,'r--' , lambdas, a_testing, 'g--')\n",
    "            #plt.show\n",
    "    \n",
    "    return weights[np.argmax(a_testing)], degr[np.argmax(a_testing)], a_testing[np.argmax(a_testing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross-validation \n",
    "\n",
    "def crossValidationForLogistic_reg(x, y, splitRatio, degrees, seed =1):\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, splitRatio, seed)\n",
    "    \n",
    "    a_training = []\n",
    "    a_testing = []\n",
    "    weights = []\n",
    "    degr = []\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    # define parameter (just add more for loops if there are more parameters for the model)\n",
    "    lambdas = np.arange(0.0001,0.3,0.1)\n",
    "    gammas = np.arange(0.01,1,0.3)\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        for ind_d, d in enumerate(degrees):\n",
    "            \n",
    "            for ind_g, gamma in enumerate(gammas):\n",
    "            \n",
    "                #perform polynomial feature expension\n",
    "                x_test_poly = build_poly(x_test,d)\n",
    "                x_train_poly = build_poly(x_train, d)\n",
    "            \n",
    "                #normalize data (DANGER: the test set must be normalized with the training set's mean and std)\n",
    "                mean = np.mean(x_train_poly, axis =0)\n",
    "                std = np.std(x_train_poly, axis = 0)\n",
    "            \n",
    "                for i in range(len(std)):\n",
    "        \n",
    "                    if(std[i] == 0):\n",
    "                        std[i] = 1\n",
    "            \n",
    "                x_train_ready = (x_train_poly - mean) / std\n",
    "                x_test_ready = (x_test_poly - mean) / std\n",
    "                \n",
    "               \n",
    "                #add bias term\n",
    "                \n",
    "                bias_tr = np.ones(shape=x_train.shape)\n",
    "                bias_te = np.ones(shape=x_test.shape)\n",
    "            \n",
    "                x_train_ready = np.c_[bias_tr, x_train_ready]\n",
    "                x_test_ready = np.c_[bias_te, x_test_ready]\n",
    "                \n",
    "           \n",
    "\n",
    "                #Model\n",
    "        \n",
    "                #ideal :lambdas = np.arange(0,0.3,0.01)\n",
    "                #       gammas = np.arange(0,3,0.5)\n",
    "                w_star, e_tr = reg_logistic_regression(y_train, x_train_ready, lambda_, np.ones(x_test_ready.shape[1]), 30, gamma)\n",
    "        \n",
    "           \n",
    "                degr.append(d)\n",
    "        \n",
    "                #compare the prediction with the reality\n",
    "                accuracy_training = np.count_nonzero(predict_labels(w_star, x_train_ready) + y_train)/len(y_train)\n",
    "                accuracy_testing = np.count_nonzero(predict_labels(w_star, x_test_ready) + y_test)/len(y_test)\n",
    "        \n",
    "                a_training.append(accuracy_training)\n",
    "                a_testing.append(accuracy_testing)\n",
    "                weights.append(w_star)\n",
    "                print(\"lambda={l:.5f},degree={deg}, gamma={ga:.5f}, Training Accuracy={tr}, Testing Accuracy={te}\".format(\n",
    "                       l=lambda_, tr=a_training[index], te=a_testing[index], deg=d, ga=gamma))\n",
    "        \n",
    "                #increment index\n",
    "                index = index + 1\n",
    "    \n",
    "    return weights[np.argmax(a_testing)], degr[np.argmax(a_testing)], a_testing[np.argmax(a_testing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we separated the data according to PRI_jet_num\n",
    "# we have to make separate prediction and then put them together for the submission\n",
    "\n",
    "def put_together(labels, indices):\n",
    "    \n",
    "    #First build first chunk\n",
    "    ids_0 = np.matrix(indices[0]).T\n",
    "    lab_0 = np.matrix(labels[0]).T\n",
    "    \n",
    "    unsorted_res = np.concatenate((ids_0, lab_0), axis=1)\n",
    "    \n",
    "    for i in range(1,len(labels)):\n",
    "        ids = np.matrix(indices[i]).T\n",
    "        lab = np.matrix(labels[i]).T\n",
    "        by_jet_num = np.concatenate((ids, lab), axis=1)\n",
    "        unsorted_res = np.concatenate((unsorted_res, by_jet_num), axis=0)\n",
    "    \n",
    "    sorted_res = unsorted_res[np.lexsort(np.fliplr(unsorted_res).T)]\n",
    "    \n",
    "    return sorted_res[0,:,:][:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_donotUse, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.2605448 0.        0.        0.        1.        1.        1.\n",
      " 0.        0.        0.        0.        0.        1.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.        1.        1.        1.        1.\n",
      " 1.        0.       ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09834149 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05881481 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.06376737 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "lambda=0.10000,degree=1, Training Accuracy=0.7997686858464652, Testing Accuracy=0.8050440352281826\n",
      "lambda=0.10000,degree=2, Training Accuracy=0.8149597980449506, Testing Accuracy=0.8205564451561249\n",
      "lambda=0.10000,degree=3, Training Accuracy=0.8167613794330579, Testing Accuracy=0.8212570056044836\n",
      "lambda=0.10000,degree=4, Training Accuracy=0.8183627851113755, Testing Accuracy=0.8233586869495596\n",
      "lambda=0.10000,degree=5, Training Accuracy=0.8187075321671244, Testing Accuracy=0.8227582065652522\n",
      "lambda=0.10000,degree=6, Training Accuracy=0.8196305646067104, Testing Accuracy=0.8242594075260208\n",
      "lambda=0.10000,degree=7, Training Accuracy=0.8196972898433069, Testing Accuracy=0.824359487590072\n",
      "lambda=0.10000,degree=8, Training Accuracy=0.8197751359526696, Testing Accuracy=0.8253602882305845\n",
      "lambda=0.60000,degree=1, Training Accuracy=0.7664394301664794, Testing Accuracy=0.7682145716573259\n",
      "lambda=0.60000,degree=2, Training Accuracy=0.7827759922598726, Testing Accuracy=0.7860288230584468\n",
      "lambda=0.60000,degree=3, Training Accuracy=0.7847443867394713, Testing Accuracy=0.7880304243394716\n",
      "lambda=0.60000,degree=4, Training Accuracy=0.7869240778016259, Testing Accuracy=0.7903322658126501\n",
      "lambda=0.60000,degree=5, Training Accuracy=0.7875468466765272, Testing Accuracy=0.791533226581265\n",
      "lambda=0.60000,degree=6, Training Accuracy=0.7880250442054693, Testing Accuracy=0.7909327461969575\n",
      "lambda=0.60000,degree=7, Training Accuracy=0.7882474616607912, Testing Accuracy=0.7920336269015212\n",
      "lambda=0.60000,degree=8, Training Accuracy=0.7886033295893061, Testing Accuracy=0.7924339471577262\n",
      "lambda=1.10000,degree=1, Training Accuracy=0.7495579453075477, Testing Accuracy=0.7512009607686149\n",
      "lambda=1.10000,degree=2, Training Accuracy=0.762980838736224, Testing Accuracy=0.7662129703763011\n",
      "lambda=1.10000,degree=3, Training Accuracy=0.7662392544566897, Testing Accuracy=0.7689151321056845\n",
      "lambda=1.10000,degree=4, Training Accuracy=0.7687859343201254, Testing Accuracy=0.7714171337069655\n",
      "lambda=1.10000,degree=5, Training Accuracy=0.7696644832686469, Testing Accuracy=0.7710168134507606\n",
      "lambda=1.10000,degree=6, Training Accuracy=0.7706319991992971, Testing Accuracy=0.7730184147317855\n",
      "lambda=1.10000,degree=7, Training Accuracy=0.7710657132371749, Testing Accuracy=0.7735188150520417\n",
      "lambda=1.10000,degree=8, Training Accuracy=0.7713103724380289, Testing Accuracy=0.774919935948759\n",
      "lambda=1.60000,degree=1, Training Accuracy=0.7448315743819575, Testing Accuracy=0.7477982385908727\n",
      "lambda=1.60000,degree=2, Training Accuracy=0.7518488450973633, Testing Accuracy=0.7538030424339471\n",
      "lambda=1.60000,degree=3, Training Accuracy=0.7554186452552797, Testing Accuracy=0.7567053642914331\n",
      "lambda=1.60000,degree=4, Training Accuracy=0.7574315232259428, Testing Accuracy=0.7603082465972778\n",
      "lambda=1.60000,degree=5, Training Accuracy=0.7594666429421381, Testing Accuracy=0.7609087269815853\n",
      "lambda=1.60000,degree=6, Training Accuracy=0.7607677850557711, Testing Accuracy=0.7632105684547638\n",
      "lambda=1.60000,degree=7, Training Accuracy=0.7616463340042926, Testing Accuracy=0.7642113690952762\n",
      "lambda=1.60000,degree=8, Training Accuracy=0.7622023776425974, Testing Accuracy=0.7643114491593275\n",
      "lambda=2.10000,degree=1, Training Accuracy=0.7437306079781141, Testing Accuracy=0.7473979183346677\n",
      "lambda=2.10000,degree=2, Training Accuracy=0.7469334193347494, Testing Accuracy=0.7502001601281025\n",
      "lambda=2.10000,degree=3, Training Accuracy=0.7495801870530799, Testing Accuracy=0.7522017614091273\n",
      "lambda=2.10000,degree=4, Training Accuracy=0.7516375485148075, Testing Accuracy=0.7544035228182546\n",
      "lambda=2.10000,degree=5, Training Accuracy=0.7532723168114234, Testing Accuracy=0.7559047237790232\n",
      "lambda=2.10000,degree=6, Training Accuracy=0.7546068215433547, Testing Accuracy=0.7569055244195356\n",
      "lambda=2.10000,degree=7, Training Accuracy=0.7557856340565607, Testing Accuracy=0.7585068054443554\n",
      "lambda=2.10000,degree=8, Training Accuracy=0.7568421169693398, Testing Accuracy=0.7593074459567654\n",
      "lambda=2.60000,degree=1, Training Accuracy=0.7440531132883309, Testing Accuracy=0.7473979183346677\n",
      "lambda=2.60000,degree=2, Training Accuracy=0.7448871787457879, Testing Accuracy=0.7477982385908727\n",
      "lambda=2.60000,degree=3, Training Accuracy=0.7464774635513395, Testing Accuracy=0.7490992794235388\n",
      "lambda=2.60000,degree=4, Training Accuracy=0.7485904293768975, Testing Accuracy=0.7509007205764612\n",
      "lambda=2.60000,degree=5, Training Accuracy=0.7497470001445713, Testing Accuracy=0.7522017614091273\n",
      "lambda=2.60000,degree=6, Training Accuracy=0.7508590874211808, Testing Accuracy=0.7540032025620497\n",
      "lambda=2.60000,degree=7, Training Accuracy=0.7520045373160885, Testing Accuracy=0.7553042433947158\n",
      "lambda=2.60000,degree=8, Training Accuracy=0.752871965391844, Testing Accuracy=0.7566052842273819\n",
      "lambda=3.10000,degree=1, Training Accuracy=0.744186563761524, Testing Accuracy=0.7472978382706165\n",
      "lambda=3.10000,degree=2, Training Accuracy=0.7439196628151378, Testing Accuracy=0.7472978382706165\n",
      "lambda=3.10000,degree=3, Training Accuracy=0.7448426952547236, Testing Accuracy=0.7483987189751802\n",
      "lambda=3.10000,degree=4, Training Accuracy=0.7461660791138889, Testing Accuracy=0.7488991192954364\n",
      "lambda=3.10000,degree=5, Training Accuracy=0.7472670455177323, Testing Accuracy=0.7497998398718975\n",
      "lambda=3.10000,degree=6, Training Accuracy=0.7482123197028503, Testing Accuracy=0.7516012810248198\n",
      "lambda=3.10000,degree=7, Training Accuracy=0.7491019895241379, Testing Accuracy=0.7524019215372297\n",
      "lambda=3.10000,degree=8, Training Accuracy=0.7500472637092559, Testing Accuracy=0.7542033626901521\n",
      "lambda=3.60000,degree=1, Training Accuracy=0.7442532889981206, Testing Accuracy=0.747497998398719\n",
      "lambda=3.60000,degree=2, Training Accuracy=0.7436861244870497, Testing Accuracy=0.7473979183346677\n",
      "lambda=3.60000,degree=3, Training Accuracy=0.7442310472525884, Testing Accuracy=0.747497998398719\n",
      "lambda=3.60000,degree=4, Training Accuracy=0.7448204535091913, Testing Accuracy=0.7485988791032826\n",
      "lambda=3.60000,degree=5, Training Accuracy=0.7456322772211164, Testing Accuracy=0.7488991192954364\n",
      "lambda=3.60000,degree=6, Training Accuracy=0.7467332436249597, Testing Accuracy=0.749599679743795\n",
      "lambda=3.60000,degree=7, Training Accuracy=0.7474449794819897, Testing Accuracy=0.7506004803843075\n",
      "lambda=3.60000,degree=8, Training Accuracy=0.7481122318479554, Testing Accuracy=0.7515012009607687\n",
      "lambda=4.10000,degree=1, Training Accuracy=0.7443422559802493, Testing Accuracy=0.7476981585268214\n",
      "lambda=4.10000,degree=2, Training Accuracy=0.7436527618687515, Testing Accuracy=0.7471977582065652\n",
      "lambda=4.10000,degree=3, Training Accuracy=0.743953025433436, Testing Accuracy=0.7472978382706165\n",
      "lambda=4.10000,degree=4, Training Accuracy=0.7442866516164188, Testing Accuracy=0.7477982385908727\n",
      "lambda=4.10000,degree=5, Training Accuracy=0.7446981239087643, Testing Accuracy=0.7483987189751802\n",
      "lambda=4.10000,degree=6, Training Accuracy=0.7456100354755841, Testing Accuracy=0.7491993594875901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=4.10000,degree=7, Training Accuracy=0.746188320859421, Testing Accuracy=0.749599679743795\n",
      "lambda=4.10000,degree=8, Training Accuracy=0.7467777271160241, Testing Accuracy=0.7501000800640513\n",
      "lambda=4.60000,degree=1, Training Accuracy=0.7443533768530154, Testing Accuracy=0.747898318654924\n",
      "lambda=4.60000,degree=2, Training Accuracy=0.7437528497236463, Testing Accuracy=0.747097678142514\n",
      "lambda=4.60000,degree=3, Training Accuracy=0.7437750914691785, Testing Accuracy=0.7471977582065652\n",
      "lambda=4.60000,degree=4, Training Accuracy=0.7441643220159918, Testing Accuracy=0.7473979183346677\n",
      "lambda=4.60000,degree=5, Training Accuracy=0.7443644977257815, Testing Accuracy=0.7479983987189752\n",
      "lambda=4.60000,degree=6, Training Accuracy=0.7448315743819575, Testing Accuracy=0.7481985588470776\n",
      "lambda=4.60000,degree=7, Training Accuracy=0.7454321015113267, Testing Accuracy=0.7487990392313851\n",
      "lambda=4.60000,degree=8, Training Accuracy=0.7461549582411228, Testing Accuracy=0.7494995996797438\n",
      "lambda=0.10000,degree=1, Training Accuracy=0.6970439467537864, Testing Accuracy=0.6943907156673114\n",
      "lambda=0.10000,degree=2, Training Accuracy=0.734069839086389, Testing Accuracy=0.7409413281753707\n",
      "lambda=0.10000,degree=3, Training Accuracy=0.7361188726016994, Testing Accuracy=0.7422308188265635\n",
      "lambda=0.10000,degree=4, Training Accuracy=0.7380676037770996, Testing Accuracy=0.7428755641521599\n",
      "lambda=0.10000,degree=5, Training Accuracy=0.7387553912507702, Testing Accuracy=0.7435203094777563\n",
      "lambda=0.10000,degree=6, Training Accuracy=0.7395291521586497, Testing Accuracy=0.7430045132172792\n",
      "lambda=0.10000,degree=7, Training Accuracy=0.7396724412156643, Testing Accuracy=0.7430045132172792\n",
      "lambda=0.10000,degree=8, Training Accuracy=0.7405321755577526, Testing Accuracy=0.7450676982591876\n",
      "lambda=0.60000,degree=1, Training Accuracy=0.6640301480175959, Testing Accuracy=0.6665377176015473\n",
      "lambda=0.60000,degree=2, Training Accuracy=0.7051970940979237, Testing Accuracy=0.7159252095422308\n",
      "lambda=0.60000,degree=3, Training Accuracy=0.7083924400693519, Testing Accuracy=0.7188910380399742\n",
      "lambda=0.60000,degree=4, Training Accuracy=0.7117310750977948, Testing Accuracy=0.723404255319149\n",
      "lambda=0.60000,degree=5, Training Accuracy=0.7138517531416125, Testing Accuracy=0.7243068987749839\n",
      "lambda=0.60000,degree=6, Training Accuracy=0.7133072547249567, Testing Accuracy=0.7262411347517731\n",
      "lambda=0.60000,degree=7, Training Accuracy=0.7136654773674934, Testing Accuracy=0.7261121856866538\n",
      "lambda=0.60000,degree=8, Training Accuracy=0.7129776898938228, Testing Accuracy=0.7241779497098646\n",
      "lambda=1.10000,degree=1, Training Accuracy=0.6558770006734586, Testing Accuracy=0.6586718246292714\n",
      "lambda=1.10000,degree=2, Training Accuracy=0.6846207855106106, Testing Accuracy=0.6889748549323017\n",
      "lambda=1.10000,degree=3, Training Accuracy=0.6900371118657668, Testing Accuracy=0.6976144422952933\n",
      "lambda=1.10000,degree=4, Training Accuracy=0.6982619037384115, Testing Accuracy=0.7081882656350742\n",
      "lambda=1.10000,degree=5, Training Accuracy=0.7009843958216911, Testing Accuracy=0.7117988394584139\n",
      "lambda=1.10000,degree=6, Training Accuracy=0.7019301035979882, Testing Accuracy=0.7117988394584139\n",
      "lambda=1.10000,degree=7, Training Accuracy=0.7028901402799869, Testing Accuracy=0.7120567375886525\n",
      "lambda=1.10000,degree=8, Training Accuracy=0.7026178910716588, Testing Accuracy=0.7120567375886525\n",
      "lambda=1.60000,degree=1, Training Accuracy=0.6526673257963289, Testing Accuracy=0.6559638942617666\n",
      "lambda=1.60000,degree=2, Training Accuracy=0.6705211423003625, Testing Accuracy=0.6755641521598968\n",
      "lambda=1.60000,degree=3, Training Accuracy=0.6765965983177865, Testing Accuracy=0.6804642166344294\n",
      "lambda=1.60000,degree=4, Training Accuracy=0.6885182478614108, Testing Accuracy=0.6946486137975499\n",
      "lambda=1.60000,degree=5, Training Accuracy=0.6908538594907507, Testing Accuracy=0.6973565441650548\n",
      "lambda=1.60000,degree=6, Training Accuracy=0.6933327601771053, Testing Accuracy=0.7017408123791102\n",
      "lambda=1.60000,degree=7, Training Accuracy=0.6953388069753113, Testing Accuracy=0.7043197936814958\n",
      "lambda=1.60000,degree=8, Training Accuracy=0.6953817936924157, Testing Accuracy=0.7048355899419729\n",
      "lambda=2.10000,degree=1, Training Accuracy=0.6502887274498846, Testing Accuracy=0.6541586073500967\n",
      "lambda=2.10000,degree=2, Training Accuracy=0.6620670879364943, Testing Accuracy=0.6660219213410703\n",
      "lambda=2.10000,degree=3, Training Accuracy=0.6667812979122785, Testing Accuracy=0.6724693745970342\n",
      "lambda=2.10000,degree=4, Training Accuracy=0.6801644958374529, Testing Accuracy=0.6862669245647969\n",
      "lambda=2.10000,degree=5, Training Accuracy=0.6827007121466133, Testing Accuracy=0.6902643455834945\n",
      "lambda=2.10000,degree=6, Training Accuracy=0.6866268323088166, Testing Accuracy=0.6947775628626692\n",
      "lambda=2.10000,degree=7, Training Accuracy=0.688188683030277, Testing Accuracy=0.6974854932301741\n",
      "lambda=2.10000,degree=8, Training Accuracy=0.6897218759403344, Testing Accuracy=0.7001934235976789\n",
      "lambda=2.60000,degree=1, Training Accuracy=0.6489274814082449, Testing Accuracy=0.6535138620245003\n",
      "lambda=2.60000,degree=2, Training Accuracy=0.6572525756207999, Testing Accuracy=0.6620245003223727\n",
      "lambda=2.60000,degree=3, Training Accuracy=0.6608348020461677, Testing Accuracy=0.6639587362991618\n",
      "lambda=2.60000,degree=4, Training Accuracy=0.6737594749888951, Testing Accuracy=0.6789168278529981\n",
      "lambda=2.60000,degree=5, Training Accuracy=0.6764962959778762, Testing Accuracy=0.6809800128949065\n",
      "lambda=2.60000,degree=6, Training Accuracy=0.6812391637650632, Testing Accuracy=0.6876853642811089\n",
      "lambda=2.60000,degree=7, Training Accuracy=0.6829299746378369, Testing Accuracy=0.6914248871695681\n",
      "lambda=2.60000,degree=8, Training Accuracy=0.6839616558483429, Testing Accuracy=0.6949065119277885\n",
      "lambda=3.10000,degree=1, Training Accuracy=0.6480534181604551, Testing Accuracy=0.6541586073500967\n",
      "lambda=3.10000,degree=2, Training Accuracy=0.6543151499519981, Testing Accuracy=0.6588007736943907\n",
      "lambda=3.10000,degree=3, Training Accuracy=0.6568513662611586, Testing Accuracy=0.6611218568665377\n",
      "lambda=3.10000,degree=4, Training Accuracy=0.6679276103683962, Testing Accuracy=0.6744036105738234\n",
      "lambda=3.10000,degree=5, Training Accuracy=0.6705784579231684, Testing Accuracy=0.6755641521598968\n",
      "lambda=3.10000,degree=6, Training Accuracy=0.676037770995429, Testing Accuracy=0.6822695035460993\n",
      "lambda=3.10000,degree=7, Training Accuracy=0.678373382624769, Testing Accuracy=0.6847195357833655\n",
      "lambda=3.10000,degree=8, Training Accuracy=0.6804367450457809, Testing Accuracy=0.6887169568020632\n",
      "lambda=3.60000,degree=1, Training Accuracy=0.647838484574933, Testing Accuracy=0.6541586073500967\n",
      "lambda=3.60000,degree=2, Training Accuracy=0.6524953789279113, Testing Accuracy=0.6568665377176015\n",
      "lambda=3.60000,degree=3, Training Accuracy=0.6541145452721776, Testing Accuracy=0.6586718246292714\n",
      "lambda=3.60000,degree=4, Training Accuracy=0.6639011878662826, Testing Accuracy=0.6695035460992907\n",
      "lambda=3.60000,degree=5, Training Accuracy=0.6666809955723682, Testing Accuracy=0.6723404255319149\n",
      "lambda=3.60000,degree=6, Training Accuracy=0.6718537305305994, Testing Accuracy=0.6778852353320438\n",
      "lambda=3.60000,degree=7, Training Accuracy=0.6739170929516113, Testing Accuracy=0.6808510638297872\n",
      "lambda=3.60000,degree=8, Training Accuracy=0.6765965983177865, Testing Accuracy=0.683558994197292\n",
      "lambda=4.10000,degree=1, Training Accuracy=0.6472796572525756, Testing Accuracy=0.654287556415216\n",
      "lambda=4.10000,degree=2, Training Accuracy=0.6515353422459127, Testing Accuracy=0.6560928433268859\n",
      "lambda=4.10000,degree=3, Training Accuracy=0.6527389703248363, Testing Accuracy=0.6581560283687943\n",
      "lambda=4.10000,degree=4, Training Accuracy=0.660419263780825, Testing Accuracy=0.6657640232108317\n",
      "lambda=4.10000,degree=5, Training Accuracy=0.6631417558641046, Testing Accuracy=0.6686009026434558\n",
      "lambda=4.10000,degree=6, Training Accuracy=0.6689019759560962, Testing Accuracy=0.676079948420374\n",
      "lambda=4.10000,degree=7, Training Accuracy=0.670263221997736, Testing Accuracy=0.6780141843971631\n",
      "lambda=4.10000,degree=8, Training Accuracy=0.6727707804954935, Testing Accuracy=0.6813668600902644\n",
      "lambda=4.60000,degree=1, Training Accuracy=0.646864118987233, Testing Accuracy=0.6540296582849774\n",
      "lambda=4.60000,degree=2, Training Accuracy=0.6504750032240038, Testing Accuracy=0.655190199871051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=4.60000,degree=3, Training Accuracy=0.6518505781713451, Testing Accuracy=0.6563507414571245\n",
      "lambda=4.60000,degree=4, Training Accuracy=0.6581266388685896, Testing Accuracy=0.6640876853642811\n",
      "lambda=4.60000,degree=5, Training Accuracy=0.659960738798378, Testing Accuracy=0.6655061250805931\n",
      "lambda=4.60000,degree=6, Training Accuracy=0.6664087463640401, Testing Accuracy=0.6735009671179883\n",
      "lambda=4.60000,degree=7, Training Accuracy=0.6678702947455902, Testing Accuracy=0.6744036105738234\n",
      "lambda=4.60000,degree=8, Training Accuracy=0.6700339595065125, Testing Accuracy=0.678401031592521\n",
      "lambda=0.10000,degree=1, Training Accuracy=0.6904347058953265, Testing Accuracy=0.6750694720127035\n",
      "lambda=0.10000,degree=2, Training Accuracy=0.7447564014909244, Testing Accuracy=0.7312425565700675\n",
      "lambda=0.10000,degree=3, Training Accuracy=0.7453739441123928, Testing Accuracy=0.7324335053592695\n",
      "lambda=0.10000,degree=4, Training Accuracy=0.7477117840365233, Testing Accuracy=0.7362048431917427\n",
      "lambda=0.10000,degree=5, Training Accuracy=0.7493659160583137, Testing Accuracy=0.7364033346566098\n",
      "lambda=0.10000,degree=6, Training Accuracy=0.750005513773406, Testing Accuracy=0.7360063517268758\n",
      "lambda=0.10000,degree=7, Training Accuracy=0.7502922299905163, Testing Accuracy=0.7358078602620087\n",
      "lambda=0.10000,degree=8, Training Accuracy=0.7507333318629937, Testing Accuracy=0.7362048431917427\n",
      "lambda=0.60000,degree=1, Training Accuracy=0.6776648066871044, Testing Accuracy=0.6645494243747518\n",
      "lambda=0.60000,degree=2, Training Accuracy=0.7292075604860943, Testing Accuracy=0.7151647479158396\n",
      "lambda=0.60000,degree=3, Training Accuracy=0.7306190864780221, Testing Accuracy=0.7135768161969035\n",
      "lambda=0.60000,degree=4, Training Accuracy=0.7301779846055446, Testing Accuracy=0.7135768161969035\n",
      "lambda=0.60000,degree=5, Training Accuracy=0.7293619461414613, Testing Accuracy=0.7123858674077015\n",
      "lambda=0.60000,degree=6, Training Accuracy=0.7288326238944884, Testing Accuracy=0.7125843588725684\n",
      "lambda=0.60000,degree=7, Training Accuracy=0.72889878917536, Testing Accuracy=0.7133783247320366\n",
      "lambda=0.60000,degree=8, Training Accuracy=0.7281268608985245, Testing Accuracy=0.7121873759428344\n",
      "lambda=1.10000,degree=1, Training Accuracy=0.6743344875498997, Testing Accuracy=0.6613735609368797\n",
      "lambda=1.10000,degree=2, Training Accuracy=0.7247083213868243, Testing Accuracy=0.7092100039698293\n",
      "lambda=1.10000,degree=3, Training Accuracy=0.7250612028848062, Testing Accuracy=0.7084160381103612\n",
      "lambda=1.10000,degree=4, Training Accuracy=0.7246862662932004, Testing Accuracy=0.7086145295752283\n",
      "lambda=1.10000,degree=5, Training Accuracy=0.7232967953948964, Testing Accuracy=0.7074235807860262\n",
      "lambda=1.10000,degree=6, Training Accuracy=0.7227674731479236, Testing Accuracy=0.7076220722508932\n",
      "lambda=1.10000,degree=7, Training Accuracy=0.7224587018371893, Testing Accuracy=0.7054386661373561\n",
      "lambda=1.10000,degree=8, Training Accuracy=0.7216647184667299, Testing Accuracy=0.7060341405319571\n",
      "lambda=1.60000,degree=1, Training Accuracy=0.6725259698727422, Testing Accuracy=0.6578007145692735\n",
      "lambda=1.60000,degree=2, Training Accuracy=0.7220396550583357, Testing Accuracy=0.7074235807860262\n",
      "lambda=1.60000,degree=3, Training Accuracy=0.7217529388412254, Testing Accuracy=0.7056371576022231\n",
      "lambda=1.60000,degree=4, Training Accuracy=0.722171985620079, Testing Accuracy=0.7062326319968242\n",
      "lambda=1.60000,degree=5, Training Accuracy=0.7207384045345272, Testing Accuracy=0.7056371576022231\n",
      "lambda=1.60000,degree=6, Training Accuracy=0.7203855230365452, Testing Accuracy=0.7048431917427551\n",
      "lambda=1.60000,degree=7, Training Accuracy=0.7190622174191129, Testing Accuracy=0.7030567685589519\n",
      "lambda=1.60000,degree=8, Training Accuracy=0.7180256280187909, Testing Accuracy=0.7012703453751489\n",
      "lambda=2.10000,degree=1, Training Accuracy=0.6720848680002647, Testing Accuracy=0.6562127828503375\n",
      "lambda=2.10000,degree=2, Training Accuracy=0.7201870271939305, Testing Accuracy=0.7050416832076221\n",
      "lambda=2.10000,degree=3, Training Accuracy=0.7197238702278291, Testing Accuracy=0.7050416832076221\n",
      "lambda=2.10000,degree=4, Training Accuracy=0.7205840188791601, Testing Accuracy=0.703255260023819\n",
      "lambda=2.10000,degree=5, Training Accuracy=0.7190622174191129, Testing Accuracy=0.7024612941643509\n",
      "lambda=2.10000,degree=6, Training Accuracy=0.7175404159590657, Testing Accuracy=0.702858277094085\n",
      "lambda=2.10000,degree=7, Training Accuracy=0.7167464325886064, Testing Accuracy=0.7016673283048829\n",
      "lambda=2.10000,degree=8, Training Accuracy=0.7156216228137888, Testing Accuracy=0.7010718539102818\n",
      "lambda=2.60000,degree=1, Training Accuracy=0.6711806091616859, Testing Accuracy=0.6566097657800715\n",
      "lambda=2.60000,degree=2, Training Accuracy=0.7183785095167728, Testing Accuracy=0.704049225883287\n",
      "lambda=2.60000,degree=3, Training Accuracy=0.7183343993295251, Testing Accuracy=0.7026597856292179\n",
      "lambda=2.60000,degree=4, Training Accuracy=0.7190401623254891, Testing Accuracy=0.7026597856292179\n",
      "lambda=2.60000,degree=5, Training Accuracy=0.717496305771818, Testing Accuracy=0.7004763795156809\n",
      "lambda=2.60000,degree=6, Training Accuracy=0.7164156061842483, Testing Accuracy=0.7008733624454149\n",
      "lambda=2.60000,degree=7, Training Accuracy=0.7154010718775501, Testing Accuracy=0.7000793965859468\n",
      "lambda=2.60000,degree=8, Training Accuracy=0.7140998213537416, Testing Accuracy=0.6984914648670107\n",
      "lambda=3.10000,degree=1, Training Accuracy=0.6703204605103549, Testing Accuracy=0.6552203255260024\n",
      "lambda=3.10000,degree=2, Training Accuracy=0.7169008182439734, Testing Accuracy=0.7024612941643509\n",
      "lambda=3.10000,degree=3, Training Accuracy=0.7172757548355793, Testing Accuracy=0.7022628026994839\n",
      "lambda=3.10000,degree=4, Training Accuracy=0.7176286363335612, Testing Accuracy=0.7026597856292179\n",
      "lambda=3.10000,degree=5, Training Accuracy=0.7167023224013586, Testing Accuracy=0.7004763795156809\n",
      "lambda=3.10000,degree=6, Training Accuracy=0.7153569616903024, Testing Accuracy=0.6996824136562128\n",
      "lambda=3.10000,degree=7, Training Accuracy=0.7140998213537416, Testing Accuracy=0.6980944819372767\n",
      "lambda=3.10000,degree=8, Training Accuracy=0.7127985708299331, Testing Accuracy=0.6969035331480746\n",
      "lambda=3.60000,degree=1, Training Accuracy=0.6692397609227851, Testing Accuracy=0.6538308852719333\n",
      "lambda=3.60000,degree=2, Training Accuracy=0.7159965594053946, Testing Accuracy=0.7022628026994839\n",
      "lambda=3.60000,degree=3, Training Accuracy=0.7165038265587438, Testing Accuracy=0.7020643112346169\n",
      "lambda=3.60000,degree=4, Training Accuracy=0.7167905427758541, Testing Accuracy=0.7024612941643509\n",
      "lambda=3.60000,degree=5, Training Accuracy=0.7155775126265411, Testing Accuracy=0.7002778880508138\n",
      "lambda=3.60000,degree=6, Training Accuracy=0.7143424273836042, Testing Accuracy=0.6994839221913458\n",
      "lambda=3.60000,degree=7, Training Accuracy=0.7132837828896583, Testing Accuracy=0.6969035331480746\n",
      "lambda=3.60000,degree=8, Training Accuracy=0.7120928078339692, Testing Accuracy=0.6961095672886066\n",
      "lambda=4.10000,degree=1, Training Accuracy=0.6690192099865464, Testing Accuracy=0.6530369194124652\n",
      "lambda=4.10000,degree=2, Training Accuracy=0.7155113473456695, Testing Accuracy=0.7018658197697499\n",
      "lambda=4.10000,degree=3, Training Accuracy=0.7159965594053946, Testing Accuracy=0.7010718539102818\n",
      "lambda=4.10000,degree=4, Training Accuracy=0.715753953375532, Testing Accuracy=0.7014688368400159\n",
      "lambda=4.10000,degree=5, Training Accuracy=0.7145409232262191, Testing Accuracy=0.6994839221913458\n",
      "lambda=4.10000,degree=6, Training Accuracy=0.7135263889195209, Testing Accuracy=0.6990869392616118\n",
      "lambda=4.10000,degree=7, Training Accuracy=0.7123795240510796, Testing Accuracy=0.6963080587534736\n",
      "lambda=4.10000,degree=8, Training Accuracy=0.7112988244635099, Testing Accuracy=0.6951171099642716\n",
      "lambda=4.60000,degree=1, Training Accuracy=0.6686663284885644, Testing Accuracy=0.6522429535529972\n",
      "lambda=4.60000,degree=2, Training Accuracy=0.7151584658476875, Testing Accuracy=0.7026597856292179\n",
      "lambda=4.60000,degree=3, Training Accuracy=0.7154010718775501, Testing Accuracy=0.7004763795156809\n",
      "lambda=4.60000,degree=4, Training Accuracy=0.715070245473192, Testing Accuracy=0.7016673283048829\n",
      "lambda=4.60000,degree=5, Training Accuracy=0.7139233806047507, Testing Accuracy=0.6996824136562128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=4.60000,degree=6, Training Accuracy=0.7127985708299331, Testing Accuracy=0.6971020246129417\n",
      "lambda=4.60000,degree=7, Training Accuracy=0.7117178712423634, Testing Accuracy=0.6965065502183406\n",
      "lambda=4.60000,degree=8, Training Accuracy=0.7104607309058026, Testing Accuracy=0.6947201270345376\n",
      "lambda=0.10000,degree=1, Training Accuracy=0.7188048328069384, Testing Accuracy=0.7383852052322959\n",
      "lambda=0.10000,degree=2, Training Accuracy=0.7349977440216574, Testing Accuracy=0.7478574650428507\n",
      "lambda=0.10000,degree=3, Training Accuracy=0.7368025266957438, Testing Accuracy=0.7519170049616599\n",
      "lambda=0.10000,degree=4, Training Accuracy=0.7382563794054243, Testing Accuracy=0.7510148849797023\n",
      "lambda=0.10000,degree=5, Training Accuracy=0.7386073093698301, Testing Accuracy=0.749210645015787\n",
      "lambda=0.10000,degree=6, Training Accuracy=0.7388078407780618, Testing Accuracy=0.7514659449706811\n",
      "lambda=0.10000,degree=7, Training Accuracy=0.7387577079260039, Testing Accuracy=0.7519170049616599\n",
      "lambda=0.10000,degree=8, Training Accuracy=0.7390083721862937, Testing Accuracy=0.7523680649526387\n",
      "lambda=0.60000,degree=1, Training Accuracy=0.7077756053541886, Testing Accuracy=0.7235002255299955\n",
      "lambda=0.60000,degree=2, Training Accuracy=0.7234170551962701, Testing Accuracy=0.7338746053225079\n",
      "lambda=0.60000,degree=3, Training Accuracy=0.7272772848047325, Testing Accuracy=0.7365809652683807\n",
      "lambda=0.60000,degree=4, Training Accuracy=0.7274778162129644, Testing Accuracy=0.7392873252142534\n",
      "lambda=0.60000,degree=5, Training Accuracy=0.7276282147691382, Testing Accuracy=0.7410915651781687\n",
      "lambda=0.60000,degree=6, Training Accuracy=0.7279791447335439, Testing Accuracy=0.7419936851601263\n",
      "lambda=0.60000,degree=7, Training Accuracy=0.7278788790294279, Testing Accuracy=0.7424447451511051\n",
      "lambda=0.60000,degree=8, Training Accuracy=0.7281295432897178, Testing Accuracy=0.7419936851601263\n",
      "lambda=1.10000,degree=1, Training Accuracy=0.7007068732140171, Testing Accuracy=0.7176364456472711\n",
      "lambda=1.10000,degree=2, Training Accuracy=0.7188549656589963, Testing Accuracy=0.7334235453315291\n",
      "lambda=1.10000,degree=3, Training Accuracy=0.7227151952674588, Testing Accuracy=0.7325214253495715\n",
      "lambda=1.10000,degree=4, Training Accuracy=0.7241690479771394, Testing Accuracy=0.7356788452864231\n",
      "lambda=1.10000,degree=5, Training Accuracy=0.7248207750538929, Testing Accuracy=0.7383852052322959\n",
      "lambda=1.10000,degree=6, Training Accuracy=0.7255226349827042, Testing Accuracy=0.7383852052322959\n",
      "lambda=1.10000,degree=7, Training Accuracy=0.7264250263197474, Testing Accuracy=0.7392873252142534\n",
      "lambda=1.10000,degree=8, Training Accuracy=0.7266756905800371, Testing Accuracy=0.7392873252142534\n",
      "lambda=1.60000,degree=1, Training Accuracy=0.6973981049781922, Testing Accuracy=0.7167343256653135\n",
      "lambda=1.60000,degree=2, Training Accuracy=0.7143931418258385, Testing Accuracy=0.7298150654036987\n",
      "lambda=1.60000,degree=3, Training Accuracy=0.7187546999548804, Testing Accuracy=0.7334235453315291\n",
      "lambda=1.60000,degree=4, Training Accuracy=0.721361608261894, Testing Accuracy=0.7338746053225079\n",
      "lambda=1.60000,degree=5, Training Accuracy=0.7219632024865895, Testing Accuracy=0.7352277852954443\n",
      "lambda=1.60000,degree=6, Training Accuracy=0.7222138667468793, Testing Accuracy=0.7356788452864231\n",
      "lambda=1.60000,degree=7, Training Accuracy=0.7238682508647917, Testing Accuracy=0.7370320252593595\n",
      "lambda=1.60000,degree=8, Training Accuracy=0.7244698450894871, Testing Accuracy=0.7388362652232747\n",
      "lambda=2.10000,degree=1, Training Accuracy=0.6957938537123377, Testing Accuracy=0.7162832656743346\n",
      "lambda=2.10000,degree=2, Training Accuracy=0.7111846392941295, Testing Accuracy=0.7284618854307623\n",
      "lambda=2.10000,degree=3, Training Accuracy=0.7153456660149395, Testing Accuracy=0.731168245376635\n",
      "lambda=2.10000,degree=4, Training Accuracy=0.7191557627713441, Testing Accuracy=0.731168245376635\n",
      "lambda=2.10000,degree=5, Training Accuracy=0.7199578884042713, Testing Accuracy=0.7325214253495715\n",
      "lambda=2.10000,degree=6, Training Accuracy=0.7205093497769088, Testing Accuracy=0.7347767253044655\n",
      "lambda=2.10000,degree=7, Training Accuracy=0.7211109440016042, Testing Accuracy=0.7374830852503383\n",
      "lambda=2.10000,degree=8, Training Accuracy=0.7217125382262997, Testing Accuracy=0.7374830852503383\n",
      "lambda=2.60000,degree=1, Training Accuracy=0.6949415952273524, Testing Accuracy=0.7162832656743346\n",
      "lambda=2.60000,degree=2, Training Accuracy=0.7088785280994636, Testing Accuracy=0.7235002255299955\n",
      "lambda=2.60000,degree=3, Training Accuracy=0.7133904847846794, Testing Accuracy=0.7293640054127198\n",
      "lambda=2.60000,degree=4, Training Accuracy=0.7168496515766782, Testing Accuracy=0.7316193053676139\n",
      "lambda=2.60000,degree=5, Training Accuracy=0.718203238582243, Testing Accuracy=0.7320703653585927\n",
      "lambda=2.60000,degree=6, Training Accuracy=0.7195568255878076, Testing Accuracy=0.7329724853405503\n",
      "lambda=2.60000,degree=7, Training Accuracy=0.7201082869604452, Testing Accuracy=0.7338746053225079\n",
      "lambda=2.60000,degree=8, Training Accuracy=0.7204090840727929, Testing Accuracy=0.7343256653134866\n",
      "lambda=3.10000,degree=1, Training Accuracy=0.6945405324108889, Testing Accuracy=0.7162832656743346\n",
      "lambda=3.10000,degree=2, Training Accuracy=0.7066225497568557, Testing Accuracy=0.7230491655390167\n",
      "lambda=3.10000,degree=3, Training Accuracy=0.711736100666767, Testing Accuracy=0.7262065854758682\n",
      "lambda=3.10000,degree=4, Training Accuracy=0.7146939389381862, Testing Accuracy=0.7293640054127198\n",
      "lambda=3.10000,degree=5, Training Accuracy=0.71705018298491, Testing Accuracy=0.7302661253946775\n",
      "lambda=3.10000,degree=6, Training Accuracy=0.7183035042863588, Testing Accuracy=0.7316193053676139\n",
      "lambda=3.10000,degree=7, Training Accuracy=0.7189552313631122, Testing Accuracy=0.7316193053676139\n",
      "lambda=3.10000,degree=8, Training Accuracy=0.7190554970672282, Testing Accuracy=0.7343256653134866\n",
      "lambda=3.60000,degree=1, Training Accuracy=0.694440266706773, Testing Accuracy=0.7162832656743346\n",
      "lambda=3.60000,degree=2, Training Accuracy=0.7044668371183637, Testing Accuracy=0.7221470455570591\n",
      "lambda=3.60000,degree=3, Training Accuracy=0.7096305208803328, Testing Accuracy=0.7230491655390167\n",
      "lambda=3.60000,degree=4, Training Accuracy=0.7133904847846794, Testing Accuracy=0.7284618854307623\n",
      "lambda=3.60000,degree=5, Training Accuracy=0.7154459317190555, Testing Accuracy=0.7284618854307623\n",
      "lambda=3.60000,degree=6, Training Accuracy=0.7168997844287361, Testing Accuracy=0.7284618854307623\n",
      "lambda=3.60000,degree=7, Training Accuracy=0.7180528400260691, Testing Accuracy=0.7307171853856563\n",
      "lambda=3.60000,degree=8, Training Accuracy=0.7182533714343009, Testing Accuracy=0.7316193053676139\n",
      "lambda=4.10000,degree=1, Training Accuracy=0.6942397352985411, Testing Accuracy=0.7162832656743346\n",
      "lambda=4.10000,degree=2, Training Accuracy=0.703012984408683, Testing Accuracy=0.7207938655841227\n",
      "lambda=4.10000,degree=3, Training Accuracy=0.7081265353185943, Testing Accuracy=0.7225981055480379\n",
      "lambda=4.10000,degree=4, Training Accuracy=0.7122875620394045, Testing Accuracy=0.7266576454668471\n",
      "lambda=4.10000,degree=5, Training Accuracy=0.7141424775655487, Testing Accuracy=0.7266576454668471\n",
      "lambda=4.10000,degree=6, Training Accuracy=0.7153957988669976, Testing Accuracy=0.7280108254397835\n",
      "lambda=4.10000,degree=7, Training Accuracy=0.7164485887602146, Testing Accuracy=0.7271087054578259\n",
      "lambda=4.10000,degree=8, Training Accuracy=0.7174512458013736, Testing Accuracy=0.7298150654036987\n",
      "lambda=4.60000,degree=1, Training Accuracy=0.6941896024464832, Testing Accuracy=0.7162832656743346\n",
      "lambda=4.60000,degree=2, Training Accuracy=0.7022108587757557, Testing Accuracy=0.7189896256202075\n",
      "lambda=4.60000,degree=3, Training Accuracy=0.707374542537725, Testing Accuracy=0.7225981055480379\n",
      "lambda=4.60000,degree=4, Training Accuracy=0.7113350378503033, Testing Accuracy=0.7253044654939107\n",
      "lambda=4.60000,degree=5, Training Accuracy=0.7125883591517521, Testing Accuracy=0.7275597654488047\n",
      "lambda=4.60000,degree=6, Training Accuracy=0.7145936732340703, Testing Accuracy=0.7266576454668471\n",
      "lambda=4.60000,degree=7, Training Accuracy=0.7155963302752294, Testing Accuracy=0.7271087054578259\n",
      "lambda=4.60000,degree=8, Training Accuracy=0.716047525943751, Testing Accuracy=0.7284618854307623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy per jet nbr: \n",
      "\n",
      "[0.8253602882305845, 0.7450676982591876, 0.7364033346566098, 0.7523680649526387]\n"
     ]
    }
   ],
   "source": [
    "#separate data with respect to column 24 and remove None\n",
    "split_x_test, _, split_ids_test =  separate(y_donotUse, tX_test, ids_test)\n",
    "\n",
    "\n",
    "split_x_cleaned_test = removeNone(split_x_test, dataStatistics(split_x_test))\n",
    "\n",
    "#median instead of None\n",
    "split_x_with_median = putMedianInsteadOfNone(split_x_cleaned_test)\n",
    "\n",
    "\n",
    "\n",
    "#line dropped when None\n",
    "#split_x_drop_lines, split_y_dropped_split_indexes_dropped = dropLineIfNone(split_x_cleaned_test, _, split_ids_test)\n",
    "\n",
    "#degrees for polynomial feature expension\n",
    "degrees = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "y_res = []\n",
    "\n",
    "acc = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(cleaned_with_median)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #training: chose either cross calidation or cross validation for logistic regression with regularization\n",
    "    w_star, d, accuracy = crossValidation(cleaned_with_median[i], split_y[i], 0.9, degrees ,6)\n",
    "    #w_star, d, accuracy = crossValidationForLogistic_reg(cleaned_with_median[i], split_y[i], 0.9, degrees ,6)\n",
    "    \n",
    "    \n",
    "    #polynomial feature expension\n",
    "    extended_and_normalized = normalize(build_poly(split_x_with_median[i], d))\n",
    "    \n",
    "    #adding bias term\n",
    "    bias = np.ones(shape=split_x_with_median[i].shape)          \n",
    "    x_test_ready = np.c_[bias, extended_and_normalized]\n",
    "    \n",
    "    #prediction\n",
    "    y_res.append(predict_labels(w_star, x_test_ready))\n",
    "\n",
    "\n",
    "    acc.append(accuracy)\n",
    "\n",
    "print(\"Accuracy per jet nbr: \\n\")\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "\n",
    "#reassemble the data for the submission\n",
    "y_pred = put_together(y_res, split_ids_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
