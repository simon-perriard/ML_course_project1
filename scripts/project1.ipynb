{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../data/train.csv' \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from implementations import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the data according to the value of column 24 (PRI_jet_num) \n",
    "\n",
    "def separate(y, tX, ids):\n",
    "    \n",
    "    split_x = []\n",
    "    split_y = []\n",
    "    split_ids = []\n",
    "    \n",
    "    jet_column_nbr = 22\n",
    "    \n",
    "    for i in range(4):\n",
    "        \n",
    "        split_x.append(tX[np.where(tX[:,jet_column_nbr] == i)])\n",
    "        split_y.append(y[np.where(tX[:,jet_column_nbr] == i)])\n",
    "        split_ids.append(ids[np.where(tX[:,jet_column_nbr] == i)])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return split_x, split_y, split_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_x, split_y, split_ids = separate(y, tX, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns from each set of data given a boolean array\n",
    "\n",
    "def removeNone(data, selection):\n",
    "   \n",
    "    cleaned=[]\n",
    "    \n",
    "    for i in range(4):\n",
    "        curr_data = data[i]\n",
    "        \n",
    "        cleaned.append(curr_data[:,selection[i]])\n",
    "      \n",
    "    return cleaned\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print statistics about the None values (-999) for each columns\n",
    "#returns a boolean array that can be used to filter the columns that have 100% of undefined values (-999)\n",
    "def dataStatistics(data):\n",
    "    \n",
    "    stats=[]\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        print(\"Statistics \")\n",
    "        print(\"Type :\")\n",
    "        print(i)\n",
    "        \n",
    "        \n",
    "        nones = (data[i] == -999)\n",
    "    \n",
    "        mean = np.sum(nones, axis=0)/nones.shape[0]\n",
    "        print(mean) \n",
    "        stats.append(mean != 1)\n",
    "    \n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.26145747 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         1.\n",
      " 1.         1.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09751883 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05859584 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.0666396 0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.       ]\n"
     ]
    }
   ],
   "source": [
    "selection = dataStatistics(split_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = removeNone(split_x, selection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_momentum_vector(data):\n",
    "#### colums with tau info ####\n",
    "    tau_transverse_momentum_0_1_col = 9\n",
    "    tau_pseudo_rapidity_0_1_col = 10\n",
    "    tau_azimuth_angle_0_1_col = 11\n",
    "    \n",
    "    tau_transverse_momentum_2_3_col = 13\n",
    "    tau_pseudo_rapidity_2_3_col = 14\n",
    "    tau_azimuth_angle_2_3_col = 15\n",
    "    \n",
    "#### colums with lep info ####    \n",
    "    lep_transverse_momentum_0_1_col = 12\n",
    "    lep_pseudo_rapidity_0_1_col = 13\n",
    "    lep_azimuth_angle_0_1_col = 14\n",
    "     \n",
    "    lep_transverse_momentum_2_3_col = 16\n",
    "    lep_pseudo_rapidity_2_3_col = 17\n",
    "    lep_azimuth_angle_2_3_col = 18\n",
    "\n",
    "#### colums with leading_jet info ####\n",
    "    leading_jet_transverse_momentum_1_col = 19\n",
    "    leading_jet_pseudo_rapidity_1_col = 20\n",
    "    leading_jet_azimuth_angle_1_col = 21\n",
    "\n",
    "    leading_jet_transverse_momentum_2_3_col = 23\n",
    "    leading_jet_pseudo_rapidity_2_3_col = 24\n",
    "    leading_jet_azimuth_angle_2_3_col = 25\n",
    "    \n",
    "#### colums with subleading_jet info ####\n",
    "    subleading_jet_transverse_momentum_2_3_col = 26\n",
    "    subleading_jet_pseudo_rapidity_2_3_col = 27\n",
    "    subleading_jet_azimuth_angle_2_3_col = 28\n",
    "\n",
    "#### columns with missing transverse energy info ####\n",
    "    mte_0_1_col = 15\n",
    "    mte_angle_0_1_col = 16\n",
    "    \n",
    "    mte_2_3_col = 19\n",
    "    mte_angle_2_3_col = 20\n",
    "    \n",
    "    data_with_momentum_vector = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        current = data[i]\n",
    "        \n",
    "        if i == 0:\n",
    "            tau_transverse_momentum_0 = current[:,tau_transverse_momentum_0_1_col]\n",
    "            tau_pseudo_rapidity_0 = current[:, tau_pseudo_rapidity_0_1_col]\n",
    "            tau_azimuth_angle_0 = current[:, tau_azimuth_angle_0_1_col]\n",
    "            \n",
    "            lep_transverse_momentum_0 = current[:,lep_transverse_momentum_0_1_col]\n",
    "            lep_pseudo_rapidity_0 = current[:, lep_pseudo_rapidity_0_1_col]\n",
    "            lep_azimuth_angle_0 = current[:, lep_azimuth_angle_0_1_col]\n",
    "            \n",
    "            mte_0 = current[:, mte_0_1_col]\n",
    "            mte_angle_0 = current[:, mte_angle_0_1_col]\n",
    "            \n",
    "            current = np.c_[current, tau_transverse_momentum_0*np.cos(tau_azimuth_angle_0)]\n",
    "            current = np.c_[current, tau_transverse_momentum_0*np.sin(tau_azimuth_angle_0)]\n",
    "            current = np.c_[current, tau_transverse_momentum_0*np.sinh(tau_pseudo_rapidity_0)]\n",
    "            current = np.c_[current, tau_transverse_momentum_0*np.cosh(tau_pseudo_rapidity_0)]\n",
    "            \n",
    "            current = np.c_[current, lep_transverse_momentum_0*np.cos(lep_azimuth_angle_0)]\n",
    "            current = np.c_[current, lep_transverse_momentum_0*np.sin(lep_azimuth_angle_0)]\n",
    "            current = np.c_[current, lep_transverse_momentum_0*np.sinh(lep_pseudo_rapidity_0)]\n",
    "            current = np.c_[current, lep_transverse_momentum_0*np.cosh(lep_pseudo_rapidity_0)]\n",
    "            \n",
    "            current = np.c_[current, mte_0*np.cos(mte_angle_0)]\n",
    "            current = np.c_[current, mte_0*np.sin(mte_angle_0)]\n",
    "            \n",
    "        elif i == 1:\n",
    "            tau_transverse_momentum_1 = current[:,tau_transverse_momentum_0_1_col]\n",
    "            tau_pseudo_rapidity_1 = current[:, tau_pseudo_rapidity_0_1_col]\n",
    "            tau_azimuth_angle_1 = current[:, tau_azimuth_angle_0_1_col]\n",
    "            \n",
    "            lep_transverse_momentum_1 = current[:,lep_transverse_momentum_0_1_col]\n",
    "            lep_pseudo_rapidity_1 = current[:, lep_pseudo_rapidity_0_1_col]\n",
    "            lep_azimuth_angle_1 = current[:, lep_azimuth_angle_0_1_col]\n",
    "            \n",
    "            leading_jet_transverse_momentum_1 = current[:,lep_transverse_momentum_0_1_col]\n",
    "            leading_jet_pseudo_rapidity_1 = current[:, lep_pseudo_rapidity_0_1_col]\n",
    "            leading_jet_azimuth_angle_1 = current[:, lep_azimuth_angle_0_1_col]\n",
    "            \n",
    "            mte_1 = current[:, mte_0_1_col]\n",
    "            mte_angle_1 = current[:, mte_angle_0_1_col]\n",
    "            \n",
    "            current = np.c_[current, tau_transverse_momentum_1*np.cos(tau_azimuth_angle_1)]\n",
    "            current = np.c_[current, tau_transverse_momentum_1*np.sin(tau_azimuth_angle_1)]\n",
    "            current = np.c_[current, tau_transverse_momentum_1*np.sinh(tau_pseudo_rapidity_1)]\n",
    "            current = np.c_[current, tau_transverse_momentum_1*np.cosh(tau_pseudo_rapidity_1)]\n",
    "            \n",
    "            current = np.c_[current, lep_transverse_momentum_1*np.cos(lep_azimuth_angle_1)]\n",
    "            current = np.c_[current, lep_transverse_momentum_1*np.sin(lep_azimuth_angle_1)]\n",
    "            current = np.c_[current, lep_transverse_momentum_1*np.sinh(lep_pseudo_rapidity_1)]\n",
    "            current = np.c_[current, lep_transverse_momentum_1*np.cosh(lep_pseudo_rapidity_1)]\n",
    "            \n",
    "            current = np.c_[current, leading_jet_transverse_momentum_1*np.cos(leading_jet_azimuth_angle_1)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_1*np.sin(leading_jet_azimuth_angle_1)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_1*np.sinh(leading_jet_pseudo_rapidity_1)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_1*np.cosh(leading_jet_pseudo_rapidity_1)]\n",
    "            \n",
    "            current = np.c_[current, mte_1*np.cos(mte_angle_1)]\n",
    "            current = np.c_[current, mte_1*np.sin(mte_angle_1)]\n",
    "            \n",
    "        else:\n",
    "            tau_transverse_momentum_2_3 = current[:,tau_transverse_momentum_2_3_col]\n",
    "            tau_pseudo_rapidity_2_3 = current[:, tau_pseudo_rapidity_2_3_col]\n",
    "            tau_azimuth_angle_2_3 = current[:, tau_azimuth_angle_2_3_col]\n",
    "            \n",
    "            lep_transverse_momentum_2_3 = current[:,lep_transverse_momentum_2_3_col]\n",
    "            lep_pseudo_rapidity_2_3 = current[:, lep_pseudo_rapidity_2_3_col]\n",
    "            lep_azimuth_angle_2_3 = current[:, lep_azimuth_angle_2_3_col]\n",
    "            \n",
    "            leading_jet_transverse_momentum_2_3 = current[:,leading_jet_transverse_momentum_2_3_col]\n",
    "            leading_jet_pseudo_rapidity_2_3 = current[:, leading_jet_pseudo_rapidity_2_3_col]\n",
    "            leading_jet_azimuth_angle_2_3 = current[:, leading_jet_azimuth_angle_2_3_col]\n",
    "            \n",
    "            subleading_jet_transverse_momentum_2_3 = current[:,subleading_jet_transverse_momentum_2_3_col]\n",
    "            subleading_jet_pseudo_rapidity_2_3 = current[:, subleading_jet_pseudo_rapidity_2_3_col]\n",
    "            subleading_jet_azimuth_angle_2_3 = current[:, subleading_jet_azimuth_angle_2_3_col]\n",
    "            \n",
    "            mte_2_3 = current[:, mte_2_3_col]\n",
    "            mte_angle_2_3 = current[:, mte_angle_2_3_col]\n",
    "            \n",
    "            current = np.c_[current, tau_transverse_momentum_2_3*np.cos(tau_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, tau_transverse_momentum_2_3*np.sin(tau_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, tau_transverse_momentum_2_3*np.sinh(tau_pseudo_rapidity_2_3)]\n",
    "            current = np.c_[current, tau_transverse_momentum_2_3*np.cosh(tau_pseudo_rapidity_2_3)]\n",
    "            \n",
    "            current = np.c_[current, lep_transverse_momentum_2_3*np.cos(lep_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, lep_transverse_momentum_2_3*np.sin(lep_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, lep_transverse_momentum_2_3*np.sinh(lep_pseudo_rapidity_2_3)]\n",
    "            current = np.c_[current, lep_transverse_momentum_2_3*np.cosh(lep_pseudo_rapidity_2_3)]\n",
    "            \n",
    "            current = np.c_[current, leading_jet_transverse_momentum_2_3*np.cos(leading_jet_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_2_3*np.sin(leading_jet_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_2_3*np.sinh(leading_jet_pseudo_rapidity_2_3)]\n",
    "            current = np.c_[current, leading_jet_transverse_momentum_2_3*np.cosh(leading_jet_pseudo_rapidity_2_3)]\n",
    "        \n",
    "            current = np.c_[current, subleading_jet_transverse_momentum_2_3*np.cos(subleading_jet_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, subleading_jet_transverse_momentum_2_3*np.sin(subleading_jet_azimuth_angle_2_3)]\n",
    "            current = np.c_[current, subleading_jet_transverse_momentum_2_3*np.sinh(subleading_jet_pseudo_rapidity_2_3)]\n",
    "            current = np.c_[current, subleading_jet_transverse_momentum_2_3*np.cosh(subleading_jet_pseudo_rapidity_2_3)]\n",
    "        \n",
    "            current = np.c_[current, mte_2_3*np.cos(mte_angle_2_3)]\n",
    "            current = np.c_[current, mte_2_3*np.sin(mte_angle_2_3)]\n",
    "                    \n",
    "        data_with_momentum_vector.append(current)\n",
    "    \n",
    "    return data_with_momentum_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can either drop the lines with residual Nones or replace the Nones by the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the value of column 0 (can be None sometimes) by the median value of this column\n",
    "\n",
    "def putMedianInsteadOfNone(cleaned):\n",
    "    \n",
    "    completed_data = []\n",
    "    \n",
    "    for i in range(len(cleaned)):\n",
    "        #current PRI_jet_num\n",
    "        current = cleaned[i]\n",
    "        \n",
    "        median = np.median(current[np.where(current[:,0] != -999)], axis = 0)\n",
    "        \n",
    "        #replace -999 by median value\n",
    "        current[np.where(current[:,0] == -999)] = median\n",
    "        \n",
    "        completed_data.append(current)\n",
    "    \n",
    "    \n",
    "    return completed_data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_with_median = putMedianInsteadOfNone(cleaned)\n",
    "\n",
    "cleaned_with_median_with_momentum = add_momentum_vector(cleaned_with_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of putting the median we can simply drop the data where columns 0 == -999\n",
    "def dropLineIfNone(cleaned, split_y, split_ids):\n",
    "    \n",
    "    res_x=[]\n",
    "    res_y=[]\n",
    "    res_ids=[]\n",
    "    \n",
    "    for i in range(len(cleaned)):\n",
    "        \n",
    "        current = cleaned[i]\n",
    "        \n",
    "        drop_indexes = np.where(current[:,0] != -999)\n",
    "        \n",
    "        res_x.append(current[drop_indexes])\n",
    "        res_y.append(current[drop_indexes])\n",
    "        res_ids.append(current[drop_indexes])\n",
    "        \n",
    "    return res_x, res_y, res_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dropped_x, dropped_y, dropped_ids = dropLineIfNone(cleaned, split_y, split_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point, the first values in each of the split data has a PRI_jet_num = 0, then 1 and so on. The data is clean and we can work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Expension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to perform polynomial feature expension\n",
    "\n",
    "def build_poly(x, degree):\n",
    "   \n",
    "    x_extended = x\n",
    "\n",
    "    for d in range (2, degree +1):\n",
    "        x_extended = np.c_[x_extended, x**d]\n",
    "        \n",
    "\n",
    "    return x_extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to split the training set into a (new) training set and a test set (same as in lab03)\n",
    "\n",
    "def split_data(x, y, ratio, seed=1):\n",
    "    \"\"\"\n",
    "    split the dataset based on the split ratio. If ratio is 0.8 \n",
    "    you will have 80% of your data set dedicated to training \n",
    "    and the rest dedicated to testing\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    " \n",
    "    # split the data based on the given ratio\n",
    "\n",
    "    training_nbr = int(x.shape[0] * ratio)\n",
    "    indexes = np.random.choice(x.shape[0],training_nbr, replace=False)\n",
    "    \n",
    "    x_train = x[indexes]\n",
    "    y_train = y[indexes]\n",
    "    x_test = np.delete(x, indexes, axis = 0)\n",
    "    y_test = np.delete(y, indexes, axis = 0)\n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross-validation \n",
    "\n",
    "def crossValidation(x, y, splitRatio, degrees, seed =1):\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, splitRatio, seed)\n",
    "    \n",
    "    a_training = []\n",
    "    a_testing = []\n",
    "    weights = []\n",
    "    degr = []\n",
    "    \n",
    "    # define parameter (just add more for loops if there are more parameters for the model)\n",
    "    lambdas = np.arange(0.000001,0.00001,0.000001)\n",
    "    lambdas = [0]\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        for ind_d, d in enumerate(degrees):\n",
    "            \n",
    "            \n",
    "            #perform polynomial feature expension\n",
    "            x_test_poly = build_poly(x_test,d)\n",
    "            x_train_poly = build_poly(x_train, d)\n",
    "           \n",
    "            \n",
    "            #normalize data (DANGER: the test set must be normalized with the training set's mean and std)\n",
    "            mean = np.mean(x_train_poly, axis =0)\n",
    "            std = np.std(x_train_poly, axis = 0)\n",
    "            \n",
    "              \n",
    "            #put 1 if std = 0\n",
    "            std = std + (std == 0)\n",
    "\n",
    "            \n",
    "            x_train_ready = (x_train_poly - mean) / std\n",
    "            x_test_ready = (x_test_poly - mean) / std\n",
    "            \n",
    "            \n",
    "            #add bias term\n",
    "            bias_tr = np.ones(shape=x_train.shape)\n",
    "            bias_te = np.ones(shape=x_test.shape)\n",
    "            \n",
    "            x_train_ready = np.c_[bias_tr, x_train_ready]\n",
    "            x_test_ready = np.c_[bias_te, x_test_ready]\n",
    "            \n",
    "            \n",
    "            #Models\n",
    "        \n",
    "            #ideal :  lambdas = np.arange(0,0.000001,0.0000001) => 81.9 %\n",
    "            w_star, e_tr = ridge_regression(y_train,x_train_ready, lambda_)\n",
    "        \n",
    "            #ideal : lambdas = np.arange(0,0.3,0.1)\n",
    "            #w_star, e_tr = logistic_regression(y_train, x_train_ready,np.ones(x_train_ready.shape[1])  ,400, lambda_)\n",
    "        \n",
    "            #don't usel least squares with lambda bigger than 0.35 ideal: lambdas = np.arange(0.001,0.13,0.01)\n",
    "            #w_star, e_tr = least_squares_GD(y_train, x_train_ready,np.ones(x_train_ready.shape[1])  ,400, lambda_)    \n",
    "            #w_star, e_tr = least_squares_SGD(y_train, x_train,np.ones(x_train.shape[1])  ,400, lambda_)\n",
    "        \n",
    "            #DON'T REALLY NEED TO DO CROSS VALIDATION FOR THIS ONE ;) BUT PRACTICAL TO RUN IT HERE\n",
    "            w_star, e_tr = least_squares(y_train, x_train_ready)  \n",
    "        \n",
    "            degr.append(d)\n",
    "        \n",
    "            #compare the prediction with the reality\n",
    "            accuracy_training = np.count_nonzero(predict_labels(w_star, x_train_ready) + y_train)/len(y_train)\n",
    "            accuracy_testing = np.count_nonzero(predict_labels(w_star, x_test_ready) + y_test)/len(y_test)\n",
    "        \n",
    "            a_training.append(accuracy_training)\n",
    "            a_testing.append(accuracy_testing)\n",
    "            weights.append(w_star)\n",
    "            print(\"lambda={l:.5f},degree={deg}, Training Accuracy={tr}, Testing Accuracy={te}\".format(\n",
    "                   l=lambda_, tr=a_training[ind*len(degrees)+ind_d], te=a_testing[ind*len(degrees)+ind_d], deg=d))\n",
    "        \n",
    "            #plt.plot(lambdas, a_training,'r--' , lambdas, a_testing, 'g--')\n",
    "            #plt.show\n",
    "    \n",
    "    return weights[np.argmax(a_testing)], degr[np.argmax(a_testing)], a_testing[np.argmax(a_testing)], x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross-validation \n",
    "\n",
    "def crossValidationForLogistic_reg(x, y, splitRatio, degrees, seed =1):\n",
    "    \n",
    "    x_train, y_train, x_test, y_test = split_data(x, y, splitRatio, seed)\n",
    "    \n",
    "    a_training = []\n",
    "    a_testing = []\n",
    "    weights = []\n",
    "    degr = []\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    # define parameter (just add more for loops if there are more parameters for the model)\n",
    "    lambdas = np.arange(0.0001,0.3,0.1)\n",
    "    gammas = np.arange(0.01,1,0.3)\n",
    "    \n",
    "    for ind, lambda_ in enumerate(lambdas):\n",
    "        \n",
    "        for ind_d, d in enumerate(degrees):\n",
    "            \n",
    "            for ind_g, gamma in enumerate(gammas):\n",
    "            \n",
    "                #perform polynomial feature expension\n",
    "                x_test_poly = build_poly(x_test,d)\n",
    "                x_train_poly = build_poly(x_train, d)\n",
    "            \n",
    "                #normalize data (DANGER: the test set must be normalized with the training set's mean and std)\n",
    "                mean = np.mean(x_train_poly, axis =0)\n",
    "                std = np.std(x_train_poly, axis = 0)\n",
    "            \n",
    "                #put 1 if std = 0\n",
    "                std = std + (std == 0)\n",
    "            \n",
    "                x_train_ready = (x_train_poly - mean) / std\n",
    "                x_test_ready = (x_test_poly - mean) / std\n",
    "                \n",
    "               \n",
    "                #add bias term\n",
    "                \n",
    "                bias_tr = np.ones(shape=x_train.shape)\n",
    "                bias_te = np.ones(shape=x_test.shape)\n",
    "            \n",
    "                x_train_ready = np.c_[bias_tr, x_train_ready]\n",
    "                x_test_ready = np.c_[bias_te, x_test_ready]\n",
    "                \n",
    "           \n",
    "\n",
    "                #Model\n",
    "        \n",
    "                #ideal :lambdas = np.arange(0,0.3,0.01)\n",
    "                #       gammas = np.arange(0,3,0.5)\n",
    "                w_star, e_tr = reg_logistic_regression(y_train, x_train_ready, lambda_, np.ones(x_test_ready.shape[1]), 30, gamma)\n",
    "        \n",
    "           \n",
    "                degr.append(d)\n",
    "        \n",
    "                #compare the prediction with the reality\n",
    "                accuracy_training = np.count_nonzero(predict_labels(w_star, x_train_ready) + y_train)/len(y_train)\n",
    "                accuracy_testing = np.count_nonzero(predict_labels(w_star, x_test_ready) + y_test)/len(y_test)\n",
    "        \n",
    "                a_training.append(accuracy_training)\n",
    "                a_testing.append(accuracy_testing)\n",
    "                weights.append(w_star)\n",
    "                print(\"lambda={l:.5f},degree={deg}, gamma={ga:.5f}, Training Accuracy={tr}, Testing Accuracy={te}\".format(\n",
    "                       l=lambda_, tr=a_training[index], te=a_testing[index], deg=d, ga=gamma))\n",
    "        \n",
    "                #increment index\n",
    "                index = index + 1\n",
    "    \n",
    "    return weights[np.argmax(a_testing)], degr[np.argmax(a_testing)], a_testing[np.argmax(a_testing)], x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we separated the data according to PRI_jet_num\n",
    "# we have to make separate prediction and then put them together for the submission\n",
    "\n",
    "def put_together(labels, indices):\n",
    "    \n",
    "    #First build first chunk\n",
    "    ids_0 = np.matrix(indices[0]).T\n",
    "    lab_0 = np.matrix(labels[0]).T\n",
    "    \n",
    "    unsorted_res = np.concatenate((ids_0, lab_0), axis=1)\n",
    "    \n",
    "    for i in range(1,len(labels)):\n",
    "        ids = np.matrix(indices[i]).T\n",
    "        lab = np.matrix(labels[i]).T\n",
    "        by_jet_num = np.concatenate((ids, lab), axis=1)\n",
    "        unsorted_res = np.concatenate((unsorted_res, by_jet_num), axis=0)\n",
    "    \n",
    "    sorted_res = unsorted_res[np.lexsort(np.fliplr(unsorted_res).T)]\n",
    "    \n",
    "    return sorted_res[0,:,:][:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = '../data/test.csv' \n",
    "y_donotUse, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics \n",
      "Type :\n",
      "0\n",
      "[0.2605448 0.        0.        0.        1.        1.        1.\n",
      " 0.        0.        0.        0.        0.        1.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        1.        1.        1.        1.        1.\n",
      " 1.        0.       ]\n",
      "Statistics \n",
      "Type :\n",
      "1\n",
      "[0.09834149 0.         0.         0.         1.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         1.         1.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "2\n",
      "[0.05881481 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "Statistics \n",
      "Type :\n",
      "3\n",
      "[0.06376737 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282492410004337, Testing Accuracy=0.8346677341873499\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.8327531944707021, Testing Accuracy=0.8415732586068855\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8369235217579876, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8380133672890648, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8389920040924812, Testing Accuracy=0.8454763811048839\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.840037366132494, Testing Accuracy=0.8481785428342674\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8416498926835778, Testing Accuracy=0.850880704563651\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8423727494133739, Testing Accuracy=0.8511809447558046\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8423616285406079, Testing Accuracy=0.8515812650120096\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8426285294869942, Testing Accuracy=0.8507806244995997\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8424617163955027, Testing Accuracy=0.8506805444355484\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.842606287741462, Testing Accuracy=0.850880704563651\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8427619799601873, Testing Accuracy=0.8513811048839072\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.84283982606955, Testing Accuracy=0.8505804643714971\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8426285294869942, Testing Accuracy=0.8511809447558046\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8347678142514011\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832864403198363, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.836856796521391, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8378131915792751, Testing Accuracy=0.8444755804643715\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8382802682354511, Testing Accuracy=0.8450760608486789\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8386917405277966, Testing Accuracy=0.8456765412329864\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.839092091947376, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8393812346392945, Testing Accuracy=0.8459767814251401\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8393589928937623, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8394813224941894, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8394813224941894, Testing Accuracy=0.84677742193755\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8396926190767452, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8396814982039791, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.839548047730786, Testing Accuracy=0.8468775020016013\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8395925312218503, Testing Accuracy=0.8468775020016013\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244459025144293, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8347678142514011\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.8328755240711291, Testing Accuracy=0.8418734987990393\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8368456756486249, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8377575872154447, Testing Accuracy=0.844675740592474\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8381801803805563, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8384804439452408, Testing Accuracy=0.8449759807846277\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8388363118737558, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8391032128201421, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8391810589295048, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8392366632933352, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8393145094026979, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8394590807486572, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8395146851124876, Testing Accuracy=0.84677742193755\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8395814103490842, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8395369268580198, Testing Accuracy=0.8466773418734987\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244347816416633, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8347678142514011\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.8328866449438952, Testing Accuracy=0.8418734987990393\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8368456756486249, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8376686202333159, Testing Accuracy=0.8449759807846277\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380912133984275, Testing Accuracy=0.8454763811048839\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8384359604541765, Testing Accuracy=0.8452762209767815\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8386472570367323, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8391476963112064, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8390809710746099, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8391254545656743, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8392700259116336, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8392922676571657, Testing Accuracy=0.8466773418734987\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8393701137665284, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8394813224941894, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8395035642397215, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244347816416633, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282714827459658, Testing Accuracy=0.8347678142514011\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.8328755240711291, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8368123130303267, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8376574993605498, Testing Accuracy=0.8450760608486789\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380467299073632, Testing Accuracy=0.8451761409127302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=7, Training Accuracy=0.8384025978358781, Testing Accuracy=0.8454763811048839\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8385805318001357, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8389586414741829, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.838847432746522, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8390809710746099, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8391588171839726, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8392366632933352, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8393478720209963, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8393589928937623, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8394145972575928, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282714827459658, Testing Accuracy=0.8347678142514011\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832853282325597, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8368345547758588, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8376463784877837, Testing Accuracy=0.8450760608486789\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380800925256614, Testing Accuracy=0.8451761409127302\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8382913891082172, Testing Accuracy=0.8458767013610888\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.838502685690773, Testing Accuracy=0.8459767814251401\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8389030371103524, Testing Accuracy=0.8454763811048839\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8389141579831185, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8390253667107794, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8391254545656743, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8391143336929082, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8392922676571657, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.839325630275464, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8393367511482301, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282492410004337, Testing Accuracy=0.8346677341873499\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832864403198363, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8368011921575605, Testing Accuracy=0.8452762209767815\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8376686202333159, Testing Accuracy=0.8449759807846277\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380689716528953, Testing Accuracy=0.8451761409127302\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8381913012533223, Testing Accuracy=0.8456765412329864\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8384248395814103, Testing Accuracy=0.8458767013610888\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8387695866371593, Testing Accuracy=0.8455764611689351\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8388363118737558, Testing Accuracy=0.8458767013610888\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8389586414741829, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8391143336929082, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8391032128201421, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8393145094026979, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8392477841661014, Testing Accuracy=0.8464771817453963\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8392700259116336, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282381201276676, Testing Accuracy=0.8346677341873499\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832864403198363, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8367678295392622, Testing Accuracy=0.8452762209767815\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8377909498337429, Testing Accuracy=0.8449759807846277\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380356090345971, Testing Accuracy=0.8452762209767815\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8382246638716206, Testing Accuracy=0.8454763811048839\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8383025099809833, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.838736224018861, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8388363118737558, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8389030371103524, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8391254545656743, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8391476963112064, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8392255424205691, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.839203300675037, Testing Accuracy=0.8466773418734987\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8392144215478031, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244570233871954, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8346677341873499\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832864403198363, Testing Accuracy=0.8418734987990393\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8367678295392622, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8377464663426786, Testing Accuracy=0.8450760608486789\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8380022464162987, Testing Accuracy=0.8452762209767815\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8382469056171529, Testing Accuracy=0.8456765412329864\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8383692352175799, Testing Accuracy=0.8457766212970377\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.838736224018861, Testing Accuracy=0.8458767013610888\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8388029492554576, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.838847432746522, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8390364875835455, Testing Accuracy=0.8463771016813451\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8390698502018439, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8392144215478031, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8391476963112064, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8391476963112064, Testing Accuracy=0.84677742193755\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.8244459025144293, Testing Accuracy=0.8282626100880705\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.8282603618731998, Testing Accuracy=0.8346677341873499\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.832864403198363, Testing Accuracy=0.8419735788630904\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.8367455877937301, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.8378576750703395, Testing Accuracy=0.8447758206565252\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.8379911255435326, Testing Accuracy=0.8453763010408326\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.8382135429988545, Testing Accuracy=0.8455764611689351\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8384470813269426, Testing Accuracy=0.8455764611689351\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8385916526729018, Testing Accuracy=0.8459767814251401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=10, Training Accuracy=0.8387584657643932, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8388251910009897, Testing Accuracy=0.8461769415532426\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.838969762346949, Testing Accuracy=0.8462770216172938\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8390587293290778, Testing Accuracy=0.8460768614891914\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8391365754384404, Testing Accuracy=0.8459767814251401\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8391588171839726, Testing Accuracy=0.8465772618094476\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8391810589295048, Testing Accuracy=0.8466773418734987\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7147258163894024, Testing Accuracy=0.7120567375886525\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7606929458797231, Testing Accuracy=0.7595099935525468\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714396251558269, Testing Accuracy=0.7676337846550613\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7809110318244996, Testing Accuracy=0.7773049645390071\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7838341285875997, Testing Accuracy=0.779110251450677\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.787487999541475, Testing Accuracy=0.7829787234042553\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7947384258264196, Testing Accuracy=0.7922630560928433\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.8020891544512746, Testing Accuracy=0.7987105093488073\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8051555402713895, Testing Accuracy=0.8010315925209542\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8046826863832409, Testing Accuracy=0.8016763378465506\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8050265801200762, Testing Accuracy=0.8014184397163121\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8053991316683144, Testing Accuracy=0.8007736943907157\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.8055567496310306, Testing Accuracy=0.800902643455835\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8054277894797174, Testing Accuracy=0.8006447453255964\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8057716832165528, Testing Accuracy=0.7994842037395229\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.805628394159538, Testing Accuracy=0.8007736943907157\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7146828296722979, Testing Accuracy=0.7120567375886525\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607072747854247, Testing Accuracy=0.7595099935525468\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7715112696843343, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808967029187981, Testing Accuracy=0.7774339136041264\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7836191950020777, Testing Accuracy=0.7788523533204385\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7864993050480735, Testing Accuracy=0.7809155383623468\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7874450128243706, Testing Accuracy=0.7824629271437782\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7882331026379515, Testing Accuracy=0.7829787234042553\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7891071658857413, Testing Accuracy=0.7822050290135396\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7895943486795913, Testing Accuracy=0.7838813668600902\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7898809267936208, Testing Accuracy=0.7847840103159253\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7899669002278296, Testing Accuracy=0.7845261121856867\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7901245181905457, Testing Accuracy=0.7847840103159253\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.790253478341859, Testing Accuracy=0.785170857511283\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7905113986444855, Testing Accuracy=0.7850419084461637\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7906546877015003, Testing Accuracy=0.7850419084461637\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7146828296722979, Testing Accuracy=0.7120567375886525\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607072747854247, Testing Accuracy=0.7595099935525468\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7715255985900357, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808393872959922, Testing Accuracy=0.7774339136041264\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834185903222571, Testing Accuracy=0.778207607994842\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7858974910086117, Testing Accuracy=0.7807865892972276\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7868145409735059, Testing Accuracy=0.7820760799484203\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7872730659559529, Testing Accuracy=0.781431334622824\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7875596440699824, Testing Accuracy=0.7809155383623468\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7879895112410266, Testing Accuracy=0.7823339780786589\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7887059565261001, Testing Accuracy=0.7819471308833011\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7888779033945178, Testing Accuracy=0.782849774339136\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7887059565261001, Testing Accuracy=0.7829787234042553\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7888205877717118, Testing Accuracy=0.782849774339136\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7888492455831149, Testing Accuracy=0.782849774339136\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7887776010546075, Testing Accuracy=0.7825918762088975\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7146828296722979, Testing Accuracy=0.7120567375886525\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607072747854247, Testing Accuracy=0.7595099935525468\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714969407786327, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808537162016936, Testing Accuracy=0.7774339136041264\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834185903222571, Testing Accuracy=0.778207607994842\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7854532949318661, Testing Accuracy=0.7803997421018698\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.786241384745447, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7867715542564014, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7868718565963118, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7873160526730574, Testing Accuracy=0.7806576402321083\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7873876972015648, Testing Accuracy=0.7813023855577047\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7878462221840118, Testing Accuracy=0.7816892327530626\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7883190760721603, Testing Accuracy=0.7818181818181819\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7883190760721603, Testing Accuracy=0.7820760799484203\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7883190760721603, Testing Accuracy=0.7818181818181819\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7883620627892648, Testing Accuracy=0.7815602836879433\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7146828296722979, Testing Accuracy=0.7120567375886525\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607072747854247, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714826118729312, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808393872959922, Testing Accuracy=0.7775628626692457\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834329192279585, Testing Accuracy=0.7785944551901999\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7850234277608219, Testing Accuracy=0.7803997421018698\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7859404777257161, Testing Accuracy=0.7813023855577047\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7862557136511484, Testing Accuracy=0.7807865892972276\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7862987003682529, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7867428964449985, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7869435011248191, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7872587370502515, Testing Accuracy=0.7806576402321083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=13, Training Accuracy=0.787487999541475, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7875023284471765, Testing Accuracy=0.7809155383623468\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7876886042212956, Testing Accuracy=0.7807865892972276\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7877315909384001, Testing Accuracy=0.7806576402321083\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7146685007665965, Testing Accuracy=0.7119277885235332\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607216036911261, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714826118729312, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808393872959922, Testing Accuracy=0.7774339136041264\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834615770393615, Testing Accuracy=0.7783365570599613\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7849517832323145, Testing Accuracy=0.7788523533204385\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7858545042915073, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7860121222542235, Testing Accuracy=0.7802707930367505\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7860551089713279, Testing Accuracy=0.7793681495809155\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.786184069122641, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7865996073879837, Testing Accuracy=0.7792392005157963\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7868431987849088, Testing Accuracy=0.7801418439716312\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7872014214274455, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7872300792388486, Testing Accuracy=0.7802707930367505\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7871584347103412, Testing Accuracy=0.7802707930367505\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7871441058046397, Testing Accuracy=0.7802707930367505\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7147114874837008, Testing Accuracy=0.7119277885235332\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607359325968276, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714396251558269, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808823740130966, Testing Accuracy=0.7774339136041264\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7833039590766453, Testing Accuracy=0.7783365570599613\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7847368496467925, Testing Accuracy=0.7784655061250806\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7858688331972087, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7859548066314176, Testing Accuracy=0.7801418439716312\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7857971886687014, Testing Accuracy=0.7788523533204385\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7859261488200147, Testing Accuracy=0.7796260477111541\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7863273581796558, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7864706472366705, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7868575276906102, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7869148433134161, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7870724612761324, Testing Accuracy=0.7801418439716312\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7869148433134161, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7147114874837008, Testing Accuracy=0.7119277885235332\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7607216036911261, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7714109673444239, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808680451073952, Testing Accuracy=0.777691811734365\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7832896301709439, Testing Accuracy=0.7785944551901999\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7847225207410911, Testing Accuracy=0.778207607994842\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7856825574230896, Testing Accuracy=0.7810444874274661\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7856252418002837, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7856825574230896, Testing Accuracy=0.7785944551901999\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7858258464801043, Testing Accuracy=0.7794970986460348\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.785969135537119, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7862557136511484, Testing Accuracy=0.7801418439716312\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7866282651993868, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7866712519164911, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7866139362936853, Testing Accuracy=0.7796260477111541\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7867142386335956, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7147114874837008, Testing Accuracy=0.7119277885235332\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7606929458797231, Testing Accuracy=0.7596389426176661\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7713679806273195, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808393872959922, Testing Accuracy=0.7775628626692457\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834329192279585, Testing Accuracy=0.7789813023855577\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7845792316840763, Testing Accuracy=0.7778207607994843\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.7853529925919558, Testing Accuracy=0.7809155383623468\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7855249394603734, Testing Accuracy=0.7792392005157963\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7856109128945823, Testing Accuracy=0.7787234042553192\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7856109128945823, Testing Accuracy=0.7793681495809155\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.7858545042915073, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7860694378770293, Testing Accuracy=0.7800128949065119\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7864133316138646, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7864706472366705, Testing Accuracy=0.7797549967762734\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7865566206708794, Testing Accuracy=0.7802707930367505\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.7865422917651779, Testing Accuracy=0.7796260477111541\n",
      "lambda=0.00000,degree=1, Training Accuracy=0.7147258163894024, Testing Accuracy=0.7119277885235332\n",
      "lambda=0.00000,degree=2, Training Accuracy=0.7606929458797231, Testing Accuracy=0.7597678916827852\n",
      "lambda=0.00000,degree=3, Training Accuracy=0.7713536517216181, Testing Accuracy=0.7677627337201806\n",
      "lambda=0.00000,degree=4, Training Accuracy=0.7808823740130966, Testing Accuracy=0.7779497098646034\n",
      "lambda=0.00000,degree=5, Training Accuracy=0.7834185903222571, Testing Accuracy=0.779110251450677\n",
      "lambda=0.00000,degree=6, Training Accuracy=0.7844072848156586, Testing Accuracy=0.7783365570599613\n",
      "lambda=0.00000,degree=7, Training Accuracy=0.785209703534941, Testing Accuracy=0.7807865892972276\n",
      "lambda=0.00000,degree=8, Training Accuracy=0.7854676238375675, Testing Accuracy=0.7788523533204385\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.7856109128945823, Testing Accuracy=0.7787234042553192\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.7854962816489705, Testing Accuracy=0.7789813023855577\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.785725544140194, Testing Accuracy=0.7793681495809155\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.7858115175744028, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.7861124245941338, Testing Accuracy=0.7798839458413926\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.7862843714625514, Testing Accuracy=0.7801418439716312\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.7864419894252676, Testing Accuracy=0.7797549967762734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda=0.00000,degree=8, Training Accuracy=0.830939226519337, Testing Accuracy=0.8423423423423423\n",
      "lambda=0.00000,degree=9, Training Accuracy=0.8361878453038674, Testing Accuracy=0.8558558558558559\n",
      "lambda=0.00000,degree=10, Training Accuracy=0.8378453038674033, Testing Accuracy=0.8468468468468469\n",
      "lambda=0.00000,degree=11, Training Accuracy=0.8378913443830571, Testing Accuracy=0.8536036036036037\n",
      "lambda=0.00000,degree=12, Training Accuracy=0.8375690607734807, Testing Accuracy=0.8490990990990991\n",
      "lambda=0.00000,degree=13, Training Accuracy=0.838121546961326, Testing Accuracy=0.8490990990990991\n",
      "lambda=0.00000,degree=14, Training Accuracy=0.8377992633517496, Testing Accuracy=0.8513513513513513\n",
      "lambda=0.00000,degree=15, Training Accuracy=0.8382596685082873, Testing Accuracy=0.8423423423423423\n",
      "lambda=0.00000,degree=16, Training Accuracy=0.8392725598526704, Testing Accuracy=0.8468468468468469\n",
      "lambda=0.00000,degree=17, Training Accuracy=0.8397790055248618, Testing Accuracy=0.8423423423423423\n",
      "lambda=0.00000,degree=18, Training Accuracy=0.839134438305709, Testing Accuracy=0.8423423423423423\n",
      "lambda=0.00000,degree=19, Training Accuracy=0.8392725598526704, Testing Accuracy=0.8423423423423423\n",
      "lambda=0.00000,degree=20, Training Accuracy=0.8398710865561694, Testing Accuracy=0.8400900900900901\n",
      "lambda=0.00000,degree=21, Training Accuracy=0.8397790055248618, Testing Accuracy=0.8378378378378378\n",
      "Accuracy per jet nbr: \n",
      "\n",
      "[0.8414207103551776, 0.7949709864603481, 0.8303571428571429, 0.8558558558558559]\n"
     ]
    }
   ],
   "source": [
    "#separate data with respect to column 24 and remove None\n",
    "split_x_test, _, split_ids_test =  separate(y_donotUse, tX_test, ids_test)\n",
    "\n",
    "\n",
    "split_x_cleaned_test = removeNone(split_x_test, dataStatistics(split_x_test))\n",
    "\n",
    "#median instead of None\n",
    "split_x_with_median = putMedianInsteadOfNone(split_x_cleaned_test)\n",
    "\n",
    "split_x_with_median_with_momentum = add_momentum_vector(split_x_with_median)\n",
    "\n",
    "#line dropped when None\n",
    "#split_x_drop_lines, split_y_dropped_split_indexes_dropped = dropLineIfNone(split_x_cleaned_test, _, split_ids_test)\n",
    "\n",
    "#degrees for polynomial feature expension\n",
    "degrees = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21]\n",
    "\n",
    "y_res = []\n",
    "\n",
    "acc = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(cleaned_with_median)):\n",
    "    \n",
    "    \n",
    "    \n",
    "    #training: chose either cross calidation or cross validation for logistic regression with regularization\n",
    "    w_star, d, accuracy, training_set = crossValidation(cleaned_with_median_with_momentum[i], split_y[i], 0.98, degrees ,6)\n",
    "    #w_star, d, accuracy, training_set = crossValidationForLogistic_reg(cleaned_with_median[i], split_y[i], 0.9, degrees ,6)\n",
    "    \n",
    "    \n",
    "    #polynomial feature expension and normalization using the training data\n",
    "    mean = np.mean(build_poly(training_set,d), axis = 0)\n",
    "    std = np.std(build_poly(training_set,d), axis = 0)\n",
    "    \n",
    "      \n",
    "    #put 1 if std = 0\n",
    "    std = std + (std == 0)\n",
    "    \n",
    "    extended_and_normalized = (build_poly(split_x_with_median_with_momentum[i], d) - mean) / std\n",
    "    \n",
    "    #adding bias term\n",
    "    bias = np.ones(shape=split_x_with_median_with_momentum[i].shape)          \n",
    "    x_test_ready = np.c_[bias, extended_and_normalized]\n",
    "    \n",
    "    #prediction\n",
    "    y_res.append(predict_labels(w_star, x_test_ready))\n",
    "\n",
    "\n",
    "    acc.append(accuracy)\n",
    "\n",
    "print(\"Accuracy per jet nbr: \\n\")\n",
    "print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '../data/submission.csv'\n",
    "#reassemble the data for the submission\n",
    "y_pred = put_together(y_res, split_ids_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
